07/22 01:29:22 PM | 
07/22 01:29:22 PM | Parameters:
07/22 01:29:22 PM | ALPHA_LR=0.0003
07/22 01:29:22 PM | ALPHA_WEIGHT_DECAY=0.001
07/22 01:29:22 PM | BATCH_SIZE=64
07/22 01:29:22 PM | DATA_PATH=./data/
07/22 01:29:22 PM | DATASET=cifar100
07/22 01:29:22 PM | EPOCHS=50
07/22 01:29:22 PM | GPUS=[0]
07/22 01:29:22 PM | INIT_CHANNELS=16
07/22 01:29:22 PM | LAYERS=8
07/22 01:29:22 PM | NAME=cifar100-ignore
07/22 01:29:22 PM | PATH=searchs/cifar100-ignore
07/22 01:29:22 PM | PLOT_PATH=searchs/cifar100-ignore/plots
07/22 01:29:22 PM | PRINT_FREQ=50
07/22 01:29:22 PM | SEED=2
07/22 01:29:22 PM | W_GRAD_CLIP=5.0
07/22 01:29:22 PM | W_LR=0.025
07/22 01:29:22 PM | W_LR_MIN=0.001
07/22 01:29:22 PM | W_MOMENTUM=0.9
07/22 01:29:22 PM | W_WEIGHT_DECAY=0.0003
07/22 01:29:22 PM | WORKERS=4
07/22 01:29:22 PM | 
07/22 01:29:22 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/22 01:29:29 PM | size of train loader is 391
07/22 01:29:36 PM | Train: [ 1/50] Step 000/390 Loss 0.012 Prec@(1,5) (1.6%, 9.4%)
07/22 01:33:05 PM | Train: [ 1/50] Step 050/390 Loss 0.012 Prec@(1,5) (1.0%, 5.2%)
07/22 01:36:08 PM | Train: [ 1/50] Step 100/390 Loss 0.012 Prec@(1,5) (1.2%, 5.6%)
07/22 01:39:11 PM | Train: [ 1/50] Step 150/390 Loss 0.012 Prec@(1,5) (1.3%, 5.9%)
07/22 01:42:03 PM | Train: [ 1/50] Step 200/390 Loss 0.012 Prec@(1,5) (1.3%, 6.2%)
07/22 01:44:53 PM | Train: [ 1/50] Step 250/390 Loss 0.012 Prec@(1,5) (1.5%, 6.6%)
07/22 01:47:43 PM | Train: [ 1/50] Step 300/390 Loss 0.012 Prec@(1,5) (1.7%, 6.9%)
07/22 01:50:47 PM | Train: [ 1/50] Step 350/390 Loss 0.012 Prec@(1,5) (1.8%, 7.3%)
07/22 01:53:09 PM | Train: [ 1/50] Step 390/390 Loss 0.012 Prec@(1,5) (1.9%, 7.6%)
07/22 01:53:09 PM | Train: [ 1/50] Final Prec@1 1.9280%
07/22 01:53:11 PM | Valid: [ 1/50] Step 000/390 Loss 4.343 Prec@(1,5) (6.2%, 9.4%)
07/22 01:53:31 PM | Valid: [ 1/50] Step 050/390 Loss 4.483 Prec@(1,5) (3.3%, 11.4%)
07/22 01:53:51 PM | Valid: [ 1/50] Step 100/390 Loss 4.490 Prec@(1,5) (2.9%, 11.0%)
07/22 01:54:11 PM | Valid: [ 1/50] Step 150/390 Loss 4.492 Prec@(1,5) (3.0%, 11.0%)
07/22 01:54:31 PM | Valid: [ 1/50] Step 200/390 Loss 4.491 Prec@(1,5) (3.0%, 11.1%)
07/22 01:54:51 PM | Valid: [ 1/50] Step 250/390 Loss 4.495 Prec@(1,5) (2.9%, 10.9%)
07/22 01:55:11 PM | Valid: [ 1/50] Step 300/390 Loss 4.497 Prec@(1,5) (2.8%, 10.7%)
07/22 01:55:31 PM | Valid: [ 1/50] Step 350/390 Loss 4.498 Prec@(1,5) (2.7%, 10.6%)
07/22 01:55:45 PM | Valid: [ 1/50] Step 390/390 Loss 4.499 Prec@(1,5) (2.7%, 10.5%)
07/22 01:55:46 PM | Valid: [ 1/50] Final Prec@1 2.7480%
07/22 01:55:46 PM | genotype = Genotype(normal=[[('dil_conv_3x3', 0), ('dil_conv_3x3', 1)], [('sep_conv_3x3', 0), ('sep_conv_3x3', 1)], [('dil_conv_3x3', 1), ('dil_conv_3x3', 0)], [('sep_conv_5x5', 4), ('dil_conv_5x5', 2)]], normal_concat=range(2, 6), reduce=[[('sep_conv_5x5', 0), ('skip_connect', 1)], [('dil_conv_3x3', 1), ('max_pool_3x3', 0)], [('sep_conv_5x5', 0), ('dil_conv_5x5', 2)], [('max_pool_3x3', 0), ('avg_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1224, 0.1251, 0.1249, 0.1230, 0.1264, 0.1288, 0.1244, 0.1251],
        [0.1240, 0.1246, 0.1256, 0.1243, 0.1229, 0.1275, 0.1263, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1206, 0.1231, 0.1228, 0.1285, 0.1263, 0.1267, 0.1263, 0.1257],
        [0.1205, 0.1215, 0.1226, 0.1278, 0.1269, 0.1262, 0.1271, 0.1274],
        [0.1202, 0.1213, 0.1235, 0.1255, 0.1272, 0.1256, 0.1276, 0.1291]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1233, 0.1226, 0.1232, 0.1231, 0.1242, 0.1286, 0.1271, 0.1277],
        [0.1224, 0.1214, 0.1216, 0.1249, 0.1258, 0.1295, 0.1282, 0.1261],
        [0.1216, 0.1207, 0.1228, 0.1279, 0.1253, 0.1255, 0.1260, 0.1301],
        [0.1215, 0.1205, 0.1210, 0.1259, 0.1281, 0.1289, 0.1283, 0.1259]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1274, 0.1273, 0.1237, 0.1228, 0.1240, 0.1252, 0.1245],
        [0.1216, 0.1217, 0.1228, 0.1252, 0.1282, 0.1252, 0.1251, 0.1303],
        [0.1205, 0.1208, 0.1251, 0.1269, 0.1255, 0.1233, 0.1286, 0.1294],
        [0.1208, 0.1216, 0.1239, 0.1273, 0.1244, 0.1270, 0.1263, 0.1286],
        [0.1216, 0.1218, 0.1226, 0.1225, 0.1301, 0.1270, 0.1244, 0.1299]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1281, 0.1283, 0.1198, 0.1254, 0.1289, 0.1208, 0.1269, 0.1217],
        [0.1227, 0.1222, 0.1274, 0.1264, 0.1261, 0.1261, 0.1255, 0.1238]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1293, 0.1290, 0.1225, 0.1201, 0.1248, 0.1233, 0.1262, 0.1247],
        [0.1190, 0.1191, 0.1286, 0.1280, 0.1260, 0.1299, 0.1208, 0.1285],
        [0.1225, 0.1217, 0.1232, 0.1243, 0.1261, 0.1283, 0.1279, 0.1261]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1235, 0.1241, 0.1228, 0.1253, 0.1300, 0.1238, 0.1228, 0.1276],
        [0.1260, 0.1260, 0.1214, 0.1271, 0.1262, 0.1231, 0.1238, 0.1265],
        [0.1212, 0.1229, 0.1235, 0.1254, 0.1253, 0.1276, 0.1289, 0.1254],
        [0.1221, 0.1227, 0.1251, 0.1269, 0.1245, 0.1237, 0.1280, 0.1270]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1294, 0.1292, 0.1232, 0.1233, 0.1201, 0.1237, 0.1272, 0.1240],
        [0.1234, 0.1223, 0.1275, 0.1255, 0.1229, 0.1262, 0.1258, 0.1264],
        [0.1267, 0.1282, 0.1279, 0.1221, 0.1260, 0.1220, 0.1241, 0.1230],
        [0.1262, 0.1265, 0.1278, 0.1233, 0.1214, 0.1263, 0.1257, 0.1228],
        [0.1269, 0.1276, 0.1283, 0.1208, 0.1251, 0.1246, 0.1238, 0.1231]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/22 01:55:47 PM | size of train loader is 391
07/22 01:55:53 PM | Train: [ 2/50] Step 000/390 Loss 0.011 Prec@(1,5) (3.1%, 14.1%)
07/22 01:58:46 PM | Train: [ 2/50] Step 050/390 Loss 0.011 Prec@(1,5) (2.7%, 11.4%)
07/22 02:01:50 PM | Train: [ 2/50] Step 100/390 Loss 0.011 Prec@(1,5) (3.0%, 11.5%)
07/22 02:04:51 PM | Train: [ 2/50] Step 150/390 Loss 0.011 Prec@(1,5) (3.1%, 11.4%)
07/22 02:07:48 PM | Train: [ 2/50] Step 200/390 Loss 0.011 Prec@(1,5) (3.3%, 11.6%)
07/22 02:10:44 PM | Train: [ 2/50] Step 250/390 Loss 0.011 Prec@(1,5) (3.3%, 11.8%)
07/22 02:13:47 PM | Train: [ 2/50] Step 300/390 Loss 0.011 Prec@(1,5) (3.4%, 11.9%)
07/22 02:16:41 PM | Train: [ 2/50] Step 350/390 Loss 0.011 Prec@(1,5) (3.4%, 12.0%)
07/22 02:18:55 PM | Train: [ 2/50] Step 390/390 Loss 0.011 Prec@(1,5) (3.5%, 12.1%)
07/22 02:18:55 PM | Train: [ 2/50] Final Prec@1 3.4840%
07/22 02:18:56 PM | Valid: [ 2/50] Step 000/390 Loss 4.299 Prec@(1,5) (6.2%, 15.6%)
07/22 02:19:16 PM | Valid: [ 2/50] Step 050/390 Loss 4.417 Prec@(1,5) (3.6%, 13.4%)
07/22 02:19:36 PM | Valid: [ 2/50] Step 100/390 Loss 4.414 Prec@(1,5) (4.0%, 13.8%)
07/22 02:19:56 PM | Valid: [ 2/50] Step 150/390 Loss 4.411 Prec@(1,5) (4.1%, 14.1%)
07/22 02:20:17 PM | Valid: [ 2/50] Step 200/390 Loss 4.413 Prec@(1,5) (4.2%, 14.2%)
07/22 02:20:37 PM | Valid: [ 2/50] Step 250/390 Loss 4.413 Prec@(1,5) (4.0%, 13.9%)
07/22 02:20:57 PM | Valid: [ 2/50] Step 300/390 Loss 4.413 Prec@(1,5) (4.0%, 13.8%)
07/22 02:21:17 PM | Valid: [ 2/50] Step 350/390 Loss 4.412 Prec@(1,5) (4.1%, 14.0%)
07/22 02:21:31 PM | Valid: [ 2/50] Step 390/390 Loss 4.410 Prec@(1,5) (4.1%, 14.2%)
07/22 02:21:31 PM | Valid: [ 2/50] Final Prec@1 4.1080%
07/22 02:21:31 PM | genotype = Genotype(normal=[[('dil_conv_3x3', 0), ('skip_connect', 1)], [('sep_conv_3x3', 0), ('dil_conv_5x5', 2)], [('dil_conv_3x3', 0), ('sep_conv_3x3', 2)], [('avg_pool_3x3', 0), ('sep_conv_5x5', 4)]], normal_concat=range(2, 6), reduce=[[('avg_pool_3x3', 0), ('sep_conv_3x3', 1)], [('dil_conv_3x3', 1), ('max_pool_3x3', 0)], [('dil_conv_5x5', 2), ('sep_conv_5x5', 0)], [('max_pool_3x3', 0), ('avg_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1210, 0.1266, 0.1262, 0.1208, 0.1262, 0.1307, 0.1233, 0.1252],
        [0.1257, 0.1292, 0.1297, 0.1202, 0.1216, 0.1283, 0.1246, 0.1207]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1184, 0.1240, 0.1229, 0.1291, 0.1282, 0.1267, 0.1268, 0.1239],
        [0.1198, 0.1239, 0.1252, 0.1273, 0.1259, 0.1244, 0.1271, 0.1264],
        [0.1194, 0.1225, 0.1250, 0.1254, 0.1268, 0.1242, 0.1283, 0.1286]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1253, 0.1245, 0.1250, 0.1187, 0.1226, 0.1291, 0.1281, 0.1266],
        [0.1236, 0.1230, 0.1228, 0.1226, 0.1243, 0.1296, 0.1293, 0.1247],
        [0.1234, 0.1224, 0.1249, 0.1289, 0.1244, 0.1232, 0.1228, 0.1300],
        [0.1239, 0.1227, 0.1229, 0.1256, 0.1280, 0.1279, 0.1263, 0.1227]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1274, 0.1319, 0.1316, 0.1215, 0.1211, 0.1203, 0.1233, 0.1229],
        [0.1226, 0.1242, 0.1253, 0.1233, 0.1286, 0.1237, 0.1228, 0.1294],
        [0.1213, 0.1221, 0.1277, 0.1264, 0.1249, 0.1208, 0.1288, 0.1281],
        [0.1226, 0.1243, 0.1278, 0.1275, 0.1228, 0.1238, 0.1244, 0.1269],
        [0.1232, 0.1237, 0.1251, 0.1172, 0.1298, 0.1274, 0.1251, 0.1286]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1302, 0.1312, 0.1185, 0.1265, 0.1306, 0.1181, 0.1274, 0.1174],
        [0.1193, 0.1195, 0.1267, 0.1281, 0.1270, 0.1277, 0.1270, 0.1247]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1323, 0.1323, 0.1180, 0.1171, 0.1249, 0.1236, 0.1268, 0.1250],
        [0.1154, 0.1160, 0.1306, 0.1279, 0.1251, 0.1327, 0.1194, 0.1329],
        [0.1217, 0.1211, 0.1231, 0.1230, 0.1256, 0.1303, 0.1292, 0.1260]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1247, 0.1256, 0.1200, 0.1249, 0.1307, 0.1219, 0.1216, 0.1307],
        [0.1239, 0.1240, 0.1213, 0.1280, 0.1295, 0.1216, 0.1237, 0.1280],
        [0.1192, 0.1214, 0.1214, 0.1272, 0.1242, 0.1288, 0.1326, 0.1253],
        [0.1205, 0.1213, 0.1242, 0.1254, 0.1263, 0.1223, 0.1309, 0.1291]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1348, 0.1343, 0.1203, 0.1210, 0.1173, 0.1227, 0.1266, 0.1230],
        [0.1222, 0.1209, 0.1283, 0.1249, 0.1216, 0.1281, 0.1258, 0.1282],
        [0.1310, 0.1337, 0.1311, 0.1192, 0.1253, 0.1183, 0.1218, 0.1196],
        [0.1282, 0.1288, 0.1300, 0.1225, 0.1186, 0.1259, 0.1255, 0.1205],
        [0.1299, 0.1307, 0.1307, 0.1173, 0.1258, 0.1231, 0.1225, 0.1199]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/22 02:21:32 PM | size of train loader is 391
07/22 02:21:38 PM | Train: [ 3/50] Step 000/390 Loss 0.011 Prec@(1,5) (3.1%, 6.2%)
07/22 02:24:44 PM | Train: [ 3/50] Step 050/390 Loss 0.011 Prec@(1,5) (4.3%, 13.5%)
07/22 02:27:42 PM | Train: [ 3/50] Step 100/390 Loss 0.011 Prec@(1,5) (4.4%, 14.0%)
07/22 02:30:45 PM | Train: [ 3/50] Step 150/390 Loss 0.011 Prec@(1,5) (4.4%, 14.3%)
07/22 02:33:58 PM | Train: [ 3/50] Step 200/390 Loss 0.011 Prec@(1,5) (4.4%, 14.5%)
07/22 02:37:20 PM | Train: [ 3/50] Step 250/390 Loss 0.011 Prec@(1,5) (4.4%, 14.8%)
07/22 02:40:19 PM | Train: [ 3/50] Step 300/390 Loss 0.011 Prec@(1,5) (4.5%, 15.1%)
07/22 02:43:16 PM | Train: [ 3/50] Step 350/390 Loss 0.011 Prec@(1,5) (4.5%, 15.3%)
07/22 02:45:37 PM | Train: [ 3/50] Step 390/390 Loss 0.011 Prec@(1,5) (4.6%, 15.4%)
07/22 02:45:37 PM | Train: [ 3/50] Final Prec@1 4.5760%
07/22 02:45:38 PM | Valid: [ 3/50] Step 000/390 Loss 4.294 Prec@(1,5) (1.6%, 17.2%)
07/22 02:45:58 PM | Valid: [ 3/50] Step 050/390 Loss 4.355 Prec@(1,5) (5.0%, 17.3%)
07/22 02:46:18 PM | Valid: [ 3/50] Step 100/390 Loss 4.356 Prec@(1,5) (5.1%, 17.1%)
07/22 02:46:38 PM | Valid: [ 3/50] Step 150/390 Loss 4.355 Prec@(1,5) (5.3%, 17.3%)
07/22 02:46:59 PM | Valid: [ 3/50] Step 200/390 Loss 4.352 Prec@(1,5) (5.4%, 17.2%)
07/22 02:47:18 PM | Valid: [ 3/50] Step 250/390 Loss 4.345 Prec@(1,5) (5.4%, 17.4%)
07/22 02:47:39 PM | Valid: [ 3/50] Step 300/390 Loss 4.342 Prec@(1,5) (5.4%, 17.5%)
07/22 02:47:59 PM | Valid: [ 3/50] Step 350/390 Loss 4.340 Prec@(1,5) (5.4%, 17.6%)
07/22 02:48:13 PM | Valid: [ 3/50] Step 390/390 Loss 4.339 Prec@(1,5) (5.5%, 17.5%)
07/22 02:48:13 PM | Valid: [ 3/50] Final Prec@1 5.5240%
07/22 02:48:13 PM | genotype = Genotype(normal=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('sep_conv_5x5', 0), ('skip_connect', 2)], [('dil_conv_3x3', 1), ('sep_conv_3x3', 2)], [('avg_pool_3x3', 0), ('skip_connect', 3)]], normal_concat=range(2, 6), reduce=[[('avg_pool_3x3', 0), ('dil_conv_3x3', 1)], [('dil_conv_3x3', 1), ('avg_pool_3x3', 0)], [('dil_conv_5x5', 2), ('dil_conv_5x5', 3)], [('max_pool_3x3', 0), ('avg_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1213, 0.1302, 0.1293, 0.1193, 0.1258, 0.1294, 0.1213, 0.1234],
        [0.1279, 0.1338, 0.1337, 0.1156, 0.1203, 0.1285, 0.1231, 0.1171]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1180, 0.1274, 0.1252, 0.1280, 0.1298, 0.1252, 0.1257, 0.1207],
        [0.1201, 0.1278, 0.1288, 0.1269, 0.1250, 0.1223, 0.1257, 0.1234],
        [0.1214, 0.1267, 0.1290, 0.1246, 0.1240, 0.1217, 0.1267, 0.1259]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1284, 0.1280, 0.1283, 0.1149, 0.1215, 0.1277, 0.1277, 0.1234],
        [0.1258, 0.1260, 0.1257, 0.1179, 0.1239, 0.1298, 0.1279, 0.1230],
        [0.1264, 0.1258, 0.1278, 0.1287, 0.1222, 0.1206, 0.1210, 0.1275],
        [0.1281, 0.1273, 0.1276, 0.1238, 0.1257, 0.1253, 0.1235, 0.1186]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1295, 0.1366, 0.1360, 0.1196, 0.1183, 0.1164, 0.1228, 0.1209],
        [0.1259, 0.1301, 0.1307, 0.1209, 0.1261, 0.1217, 0.1190, 0.1255],
        [0.1241, 0.1265, 0.1316, 0.1253, 0.1234, 0.1187, 0.1274, 0.1231],
        [0.1270, 0.1301, 0.1340, 0.1263, 0.1199, 0.1206, 0.1203, 0.1218],
        [0.1265, 0.1282, 0.1302, 0.1128, 0.1264, 0.1273, 0.1245, 0.1241]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1328, 0.1343, 0.1178, 0.1267, 0.1328, 0.1155, 0.1265, 0.1136],
        [0.1173, 0.1178, 0.1243, 0.1289, 0.1262, 0.1303, 0.1282, 0.1269]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1351, 0.1352, 0.1144, 0.1152, 0.1240, 0.1245, 0.1269, 0.1247],
        [0.1133, 0.1143, 0.1309, 0.1276, 0.1237, 0.1347, 0.1194, 0.1361],
        [0.1211, 0.1212, 0.1225, 0.1218, 0.1258, 0.1320, 0.1299, 0.1258]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1257, 0.1262, 0.1179, 0.1253, 0.1295, 0.1211, 0.1210, 0.1334],
        [0.1214, 0.1217, 0.1225, 0.1292, 0.1319, 0.1206, 0.1242, 0.1284],
        [0.1176, 0.1197, 0.1188, 0.1291, 0.1239, 0.1303, 0.1354, 0.1251],
        [0.1195, 0.1197, 0.1226, 0.1245, 0.1274, 0.1209, 0.1330, 0.1323]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1415, 0.1409, 0.1171, 0.1191, 0.1147, 0.1194, 0.1260, 0.1213],
        [0.1206, 0.1196, 0.1289, 0.1240, 0.1209, 0.1294, 0.1260, 0.1307],
        [0.1345, 0.1395, 0.1342, 0.1155, 0.1242, 0.1154, 0.1198, 0.1169],
        [0.1300, 0.1317, 0.1318, 0.1222, 0.1166, 0.1261, 0.1235, 0.1182],
        [0.1316, 0.1333, 0.1320, 0.1152, 0.1271, 0.1227, 0.1212, 0.1169]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/22 02:48:14 PM | size of train loader is 391
07/22 02:48:19 PM | Train: [ 4/50] Step 000/390 Loss 0.011 Prec@(1,5) (1.6%, 14.1%)
07/22 02:51:20 PM | Train: [ 4/50] Step 050/390 Loss 0.011 Prec@(1,5) (5.4%, 17.7%)
07/22 02:54:17 PM | Train: [ 4/50] Step 100/390 Loss 0.011 Prec@(1,5) (5.5%, 17.8%)
07/22 02:57:07 PM | Train: [ 4/50] Step 150/390 Loss 0.011 Prec@(1,5) (5.7%, 18.2%)
07/22 03:00:05 PM | Train: [ 4/50] Step 200/390 Loss 0.011 Prec@(1,5) (5.7%, 18.2%)
07/22 03:02:59 PM | Train: [ 4/50] Step 250/390 Loss 0.011 Prec@(1,5) (5.6%, 18.2%)
07/22 03:05:52 PM | Train: [ 4/50] Step 300/390 Loss 0.011 Prec@(1,5) (5.6%, 18.4%)
07/22 03:08:56 PM | Train: [ 4/50] Step 350/390 Loss 0.011 Prec@(1,5) (5.6%, 18.5%)
07/22 03:11:10 PM | Train: [ 4/50] Step 390/390 Loss 0.011 Prec@(1,5) (5.6%, 18.6%)
07/22 03:11:10 PM | Train: [ 4/50] Final Prec@1 5.6400%
07/22 03:11:12 PM | Valid: [ 4/50] Step 000/390 Loss 4.188 Prec@(1,5) (6.2%, 25.0%)
07/22 03:11:32 PM | Valid: [ 4/50] Step 050/390 Loss 4.260 Prec@(1,5) (6.6%, 20.0%)
07/22 03:11:52 PM | Valid: [ 4/50] Step 100/390 Loss 4.271 Prec@(1,5) (6.5%, 20.3%)
07/22 03:12:13 PM | Valid: [ 4/50] Step 150/390 Loss 4.274 Prec@(1,5) (6.4%, 20.1%)
07/22 03:12:34 PM | Valid: [ 4/50] Step 200/390 Loss 4.274 Prec@(1,5) (6.5%, 20.2%)
07/22 03:12:55 PM | Valid: [ 4/50] Step 250/390 Loss 4.274 Prec@(1,5) (6.5%, 20.2%)
07/22 03:13:16 PM | Valid: [ 4/50] Step 300/390 Loss 4.275 Prec@(1,5) (6.4%, 20.2%)
07/22 03:13:37 PM | Valid: [ 4/50] Step 350/390 Loss 4.273 Prec@(1,5) (6.5%, 20.3%)
07/22 03:13:53 PM | Valid: [ 4/50] Step 390/390 Loss 4.276 Prec@(1,5) (6.4%, 20.2%)
07/22 03:13:53 PM | Valid: [ 4/50] Final Prec@1 6.4120%
07/22 03:13:53 PM | genotype = Genotype(normal=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('max_pool_3x3', 3), ('max_pool_3x3', 0)], [('skip_connect', 3), ('avg_pool_3x3', 0)]], normal_concat=range(2, 6), reduce=[[('avg_pool_3x3', 0), ('dil_conv_3x3', 1)], [('max_pool_3x3', 0), ('dil_conv_3x3', 1)], [('dil_conv_5x5', 2), ('dil_conv_5x5', 3)], [('max_pool_3x3', 0), ('avg_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1223, 0.1333, 0.1312, 0.1175, 0.1245, 0.1281, 0.1203, 0.1227],
        [0.1326, 0.1396, 0.1386, 0.1121, 0.1188, 0.1269, 0.1194, 0.1120]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1184, 0.1308, 0.1272, 0.1266, 0.1309, 0.1236, 0.1253, 0.1173],
        [0.1222, 0.1328, 0.1335, 0.1265, 0.1233, 0.1198, 0.1234, 0.1186],
        [0.1250, 0.1323, 0.1334, 0.1239, 0.1211, 0.1186, 0.1240, 0.1217]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1321, 0.1315, 0.1311, 0.1115, 0.1212, 0.1249, 0.1267, 0.1209],
        [0.1312, 0.1317, 0.1310, 0.1126, 0.1213, 0.1287, 0.1249, 0.1186],
        [0.1318, 0.1315, 0.1315, 0.1285, 0.1191, 0.1174, 0.1181, 0.1223],
        [0.1351, 0.1347, 0.1347, 0.1208, 0.1224, 0.1212, 0.1195, 0.1117]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1322, 0.1412, 0.1402, 0.1188, 0.1146, 0.1133, 0.1212, 0.1185],
        [0.1308, 0.1365, 0.1370, 0.1187, 0.1235, 0.1183, 0.1157, 0.1195],
        [0.1290, 0.1320, 0.1351, 0.1236, 0.1210, 0.1170, 0.1258, 0.1164],
        [0.1337, 0.1376, 0.1414, 0.1243, 0.1158, 0.1164, 0.1158, 0.1150],
        [0.1330, 0.1348, 0.1375, 0.1081, 0.1220, 0.1250, 0.1216, 0.1179]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1347, 0.1360, 0.1174, 0.1275, 0.1349, 0.1128, 0.1270, 0.1097],
        [0.1171, 0.1181, 0.1199, 0.1273, 0.1266, 0.1318, 0.1295, 0.1296]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1376, 0.1374, 0.1124, 0.1143, 0.1220, 0.1242, 0.1272, 0.1250],
        [0.1126, 0.1140, 0.1315, 0.1273, 0.1220, 0.1363, 0.1184, 0.1379],
        [0.1210, 0.1221, 0.1235, 0.1201, 0.1268, 0.1312, 0.1290, 0.1262]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1286, 0.1275, 0.1164, 0.1255, 0.1287, 0.1195, 0.1182, 0.1357],
        [0.1204, 0.1210, 0.1237, 0.1297, 0.1337, 0.1194, 0.1237, 0.1283],
        [0.1164, 0.1180, 0.1164, 0.1304, 0.1240, 0.1320, 0.1381, 0.1246],
        [0.1203, 0.1197, 0.1224, 0.1232, 0.1277, 0.1189, 0.1336, 0.1342]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1464, 0.1450, 0.1154, 0.1180, 0.1128, 0.1168, 0.1249, 0.1207],
        [0.1197, 0.1188, 0.1302, 0.1225, 0.1192, 0.1313, 0.1274, 0.1310],
        [0.1373, 0.1434, 0.1352, 0.1123, 0.1244, 0.1129, 0.1192, 0.1152],
        [0.1317, 0.1336, 0.1324, 0.1214, 0.1155, 0.1256, 0.1223, 0.1174],
        [0.1326, 0.1341, 0.1313, 0.1153, 0.1283, 0.1221, 0.1200, 0.1162]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/22 03:13:55 PM | size of train loader is 391
07/22 03:14:00 PM | Train: [ 5/50] Step 000/390 Loss 0.011 Prec@(1,5) (3.1%, 17.2%)
07/22 03:17:05 PM | Train: [ 5/50] Step 050/390 Loss 0.011 Prec@(1,5) (7.0%, 20.5%)
07/22 03:20:23 PM | Train: [ 5/50] Step 100/390 Loss 0.011 Prec@(1,5) (6.9%, 20.7%)
07/22 03:23:49 PM | Train: [ 5/50] Step 150/390 Loss 0.011 Prec@(1,5) (6.8%, 20.9%)
07/22 03:27:13 PM | Train: [ 5/50] Step 200/390 Loss 0.011 Prec@(1,5) (6.8%, 20.7%)
07/22 03:30:38 PM | Train: [ 5/50] Step 250/390 Loss 0.011 Prec@(1,5) (6.8%, 20.9%)
07/22 03:33:53 PM | Train: [ 5/50] Step 300/390 Loss 0.011 Prec@(1,5) (6.7%, 21.1%)
07/22 03:36:57 PM | Train: [ 5/50] Step 350/390 Loss 0.011 Prec@(1,5) (6.8%, 21.2%)
07/22 03:39:18 PM | Train: [ 5/50] Step 390/390 Loss 0.011 Prec@(1,5) (6.8%, 21.3%)
07/22 03:39:18 PM | Train: [ 5/50] Final Prec@1 6.8360%
07/22 03:39:20 PM | Valid: [ 5/50] Step 000/390 Loss 4.217 Prec@(1,5) (6.2%, 21.9%)
07/22 03:39:41 PM | Valid: [ 5/50] Step 050/390 Loss 4.219 Prec@(1,5) (6.8%, 22.5%)
07/22 03:40:02 PM | Valid: [ 5/50] Step 100/390 Loss 4.217 Prec@(1,5) (6.9%, 22.2%)
07/22 03:40:23 PM | Valid: [ 5/50] Step 150/390 Loss 4.216 Prec@(1,5) (7.1%, 22.5%)
07/22 03:40:43 PM | Valid: [ 5/50] Step 200/390 Loss 4.211 Prec@(1,5) (7.2%, 22.5%)
07/22 03:41:04 PM | Valid: [ 5/50] Step 250/390 Loss 4.210 Prec@(1,5) (7.3%, 22.5%)
07/22 03:41:25 PM | Valid: [ 5/50] Step 300/390 Loss 4.212 Prec@(1,5) (7.3%, 22.5%)
07/22 03:41:46 PM | Valid: [ 5/50] Step 350/390 Loss 4.213 Prec@(1,5) (7.3%, 22.3%)
07/22 03:42:01 PM | Valid: [ 5/50] Step 390/390 Loss 4.212 Prec@(1,5) (7.3%, 22.5%)
07/22 03:42:01 PM | Valid: [ 5/50] Final Prec@1 7.2560%
07/22 03:42:01 PM | genotype = Genotype(normal=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('max_pool_3x3', 3), ('max_pool_3x3', 2)], [('skip_connect', 3), ('avg_pool_3x3', 0)]], normal_concat=range(2, 6), reduce=[[('avg_pool_3x3', 0), ('dil_conv_3x3', 1)], [('max_pool_3x3', 0), ('dil_conv_3x3', 1)], [('dil_conv_5x5', 2), ('sep_conv_5x5', 1)], [('max_pool_3x3', 0), ('avg_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1258, 0.1368, 0.1337, 0.1161, 0.1222, 0.1262, 0.1187, 0.1205],
        [0.1372, 0.1442, 0.1425, 0.1087, 0.1179, 0.1255, 0.1161, 0.1079]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1214, 0.1351, 0.1298, 0.1239, 0.1320, 0.1213, 0.1241, 0.1125],
        [0.1257, 0.1378, 0.1382, 0.1249, 0.1216, 0.1167, 0.1214, 0.1138],
        [0.1302, 0.1387, 0.1382, 0.1232, 0.1166, 0.1161, 0.1206, 0.1165]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1370, 0.1352, 0.1337, 0.1090, 0.1191, 0.1224, 0.1271, 0.1167],
        [0.1362, 0.1364, 0.1356, 0.1086, 0.1199, 0.1269, 0.1219, 0.1145],
        [0.1375, 0.1371, 0.1349, 0.1293, 0.1166, 0.1137, 0.1143, 0.1166],
        [0.1429, 0.1423, 0.1423, 0.1167, 0.1189, 0.1165, 0.1152, 0.1051]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1366, 0.1469, 0.1457, 0.1164, 0.1108, 0.1097, 0.1193, 0.1147],
        [0.1360, 0.1424, 0.1433, 0.1163, 0.1200, 0.1162, 0.1113, 0.1145],
        [0.1345, 0.1379, 0.1391, 0.1214, 0.1184, 0.1154, 0.1236, 0.1098],
        [0.1406, 0.1448, 0.1486, 0.1223, 0.1123, 0.1126, 0.1110, 0.1079],
        [0.1392, 0.1411, 0.1454, 0.1041, 0.1186, 0.1220, 0.1188, 0.1108]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1365, 0.1367, 0.1176, 0.1271, 0.1366, 0.1099, 0.1271, 0.1083],
        [0.1185, 0.1195, 0.1167, 0.1269, 0.1252, 0.1327, 0.1307, 0.1298]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1397, 0.1389, 0.1110, 0.1126, 0.1204, 0.1234, 0.1272, 0.1269],
        [0.1137, 0.1146, 0.1322, 0.1251, 0.1195, 0.1380, 0.1183, 0.1385],
        [0.1222, 0.1227, 0.1235, 0.1195, 0.1257, 0.1317, 0.1295, 0.1253]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1314, 0.1286, 0.1157, 0.1242, 0.1268, 0.1195, 0.1162, 0.1376],
        [0.1189, 0.1194, 0.1256, 0.1321, 0.1361, 0.1179, 0.1227, 0.1272],
        [0.1158, 0.1166, 0.1148, 0.1327, 0.1238, 0.1335, 0.1387, 0.1242],
        [0.1215, 0.1195, 0.1229, 0.1219, 0.1281, 0.1161, 0.1335, 0.1365]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1505, 0.1482, 0.1143, 0.1175, 0.1120, 0.1142, 0.1235, 0.1199],
        [0.1191, 0.1182, 0.1301, 0.1216, 0.1182, 0.1333, 0.1283, 0.1312],
        [0.1400, 0.1466, 0.1362, 0.1095, 0.1241, 0.1111, 0.1186, 0.1139],
        [0.1337, 0.1347, 0.1327, 0.1214, 0.1154, 0.1247, 0.1203, 0.1171],
        [0.1339, 0.1346, 0.1309, 0.1163, 0.1298, 0.1213, 0.1179, 0.1153]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/22 03:42:02 PM | size of train loader is 391
07/22 03:42:08 PM | Train: [ 6/50] Step 000/390 Loss 0.010 Prec@(1,5) (4.7%, 28.1%)
07/22 03:45:32 PM | Train: [ 6/50] Step 050/390 Loss 0.011 Prec@(1,5) (7.4%, 23.1%)
07/22 03:48:47 PM | Train: [ 6/50] Step 100/390 Loss 0.011 Prec@(1,5) (7.4%, 22.9%)
07/22 03:51:59 PM | Train: [ 6/50] Step 150/390 Loss 0.011 Prec@(1,5) (7.6%, 23.1%)
07/22 03:55:17 PM | Train: [ 6/50] Step 200/390 Loss 0.011 Prec@(1,5) (7.7%, 23.4%)
07/22 03:58:35 PM | Train: [ 6/50] Step 250/390 Loss 0.011 Prec@(1,5) (7.8%, 23.5%)
07/22 04:01:58 PM | Train: [ 6/50] Step 300/390 Loss 0.011 Prec@(1,5) (7.7%, 23.5%)
07/22 04:05:26 PM | Train: [ 6/50] Step 350/390 Loss 0.011 Prec@(1,5) (7.6%, 23.5%)
07/22 04:08:06 PM | Train: [ 6/50] Step 390/390 Loss 0.011 Prec@(1,5) (7.7%, 23.6%)
07/22 04:08:06 PM | Train: [ 6/50] Final Prec@1 7.6760%
07/22 04:08:07 PM | Valid: [ 6/50] Step 000/390 Loss 4.273 Prec@(1,5) (9.4%, 23.4%)
07/22 04:08:28 PM | Valid: [ 6/50] Step 050/390 Loss 4.135 Prec@(1,5) (8.3%, 25.3%)
07/22 04:08:50 PM | Valid: [ 6/50] Step 100/390 Loss 4.130 Prec@(1,5) (8.3%, 25.6%)
07/22 04:09:10 PM | Valid: [ 6/50] Step 150/390 Loss 4.137 Prec@(1,5) (8.3%, 24.9%)
07/22 04:09:30 PM | Valid: [ 6/50] Step 200/390 Loss 4.141 Prec@(1,5) (8.3%, 24.8%)
07/22 04:09:50 PM | Valid: [ 6/50] Step 250/390 Loss 4.142 Prec@(1,5) (8.4%, 25.0%)
07/22 04:10:11 PM | Valid: [ 6/50] Step 300/390 Loss 4.141 Prec@(1,5) (8.4%, 25.1%)
07/22 04:10:31 PM | Valid: [ 6/50] Step 350/390 Loss 4.140 Prec@(1,5) (8.3%, 25.1%)
07/22 04:10:45 PM | Valid: [ 6/50] Step 390/390 Loss 4.141 Prec@(1,5) (8.3%, 25.0%)
07/22 04:10:45 PM | Valid: [ 6/50] Final Prec@1 8.3040%
07/22 04:10:45 PM | genotype = Genotype(normal=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('max_pool_3x3', 3), ('max_pool_3x3', 2)], [('skip_connect', 3), ('skip_connect', 4)]], normal_concat=range(2, 6), reduce=[[('sep_conv_5x5', 0), ('dil_conv_5x5', 1)], [('max_pool_3x3', 0), ('dil_conv_3x3', 1)], [('sep_conv_5x5', 1), ('dil_conv_5x5', 2)], [('max_pool_3x3', 0), ('avg_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1296, 0.1419, 0.1374, 0.1145, 0.1195, 0.1225, 0.1172, 0.1175],
        [0.1423, 0.1500, 0.1474, 0.1061, 0.1157, 0.1232, 0.1116, 0.1036]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1239, 0.1401, 0.1331, 0.1220, 0.1328, 0.1180, 0.1229, 0.1073],
        [0.1281, 0.1429, 0.1433, 0.1222, 0.1208, 0.1144, 0.1188, 0.1094],
        [0.1351, 0.1451, 0.1428, 0.1215, 0.1121, 0.1138, 0.1184, 0.1113]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1423, 0.1405, 0.1377, 0.1060, 0.1170, 0.1188, 0.1257, 0.1120],
        [0.1408, 0.1411, 0.1399, 0.1051, 0.1179, 0.1247, 0.1204, 0.1100],
        [0.1441, 0.1440, 0.1397, 0.1275, 0.1139, 0.1099, 0.1103, 0.1106],
        [0.1512, 0.1507, 0.1509, 0.1126, 0.1150, 0.1113, 0.1100, 0.0983]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1418, 0.1538, 0.1516, 0.1134, 0.1065, 0.1060, 0.1172, 0.1097],
        [0.1417, 0.1490, 0.1498, 0.1144, 0.1159, 0.1139, 0.1062, 0.1092],
        [0.1403, 0.1438, 0.1425, 0.1205, 0.1147, 0.1142, 0.1209, 0.1031],
        [0.1472, 0.1523, 0.1563, 0.1193, 0.1091, 0.1085, 0.1063, 0.1009],
        [0.1460, 0.1481, 0.1549, 0.1004, 0.1146, 0.1181, 0.1138, 0.1041]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1388, 0.1386, 0.1178, 0.1260, 0.1390, 0.1067, 0.1264, 0.1067],
        [0.1195, 0.1205, 0.1127, 0.1270, 0.1251, 0.1319, 0.1330, 0.1303]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1420, 0.1410, 0.1103, 0.1118, 0.1182, 0.1223, 0.1271, 0.1273],
        [0.1145, 0.1156, 0.1334, 0.1234, 0.1170, 0.1384, 0.1174, 0.1402],
        [0.1222, 0.1243, 0.1242, 0.1183, 0.1246, 0.1317, 0.1302, 0.1245]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1343, 0.1302, 0.1142, 0.1241, 0.1251, 0.1180, 0.1152, 0.1390],
        [0.1181, 0.1185, 0.1279, 0.1340, 0.1376, 0.1154, 0.1227, 0.1258],
        [0.1156, 0.1161, 0.1137, 0.1341, 0.1245, 0.1345, 0.1388, 0.1227],
        [0.1232, 0.1203, 0.1244, 0.1195, 0.1272, 0.1134, 0.1336, 0.1383]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1540, 0.1512, 0.1145, 0.1174, 0.1106, 0.1113, 0.1212, 0.1199],
        [0.1191, 0.1184, 0.1299, 0.1207, 0.1168, 0.1343, 0.1294, 0.1315],
        [0.1413, 0.1502, 0.1377, 0.1057, 0.1238, 0.1106, 0.1173, 0.1133],
        [0.1342, 0.1351, 0.1326, 0.1211, 0.1154, 0.1241, 0.1203, 0.1171],
        [0.1339, 0.1338, 0.1296, 0.1172, 0.1314, 0.1209, 0.1178, 0.1154]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/22 04:10:46 PM | size of train loader is 391
07/22 04:10:52 PM | Train: [ 7/50] Step 000/390 Loss 0.011 Prec@(1,5) (1.6%, 17.2%)
07/22 04:13:51 PM | Train: [ 7/50] Step 050/390 Loss 0.011 Prec@(1,5) (9.0%, 25.5%)
07/22 04:16:46 PM | Train: [ 7/50] Step 100/390 Loss 0.011 Prec@(1,5) (8.8%, 25.9%)
07/22 04:19:36 PM | Train: [ 7/50] Step 150/390 Loss 0.011 Prec@(1,5) (8.7%, 25.7%)
07/22 04:22:28 PM | Train: [ 7/50] Step 200/390 Loss 0.011 Prec@(1,5) (8.8%, 25.8%)
07/22 04:25:24 PM | Train: [ 7/50] Step 250/390 Loss 0.011 Prec@(1,5) (8.7%, 25.9%)
07/22 04:28:46 PM | Train: [ 7/50] Step 300/390 Loss 0.011 Prec@(1,5) (8.7%, 26.0%)
07/22 04:31:59 PM | Train: [ 7/50] Step 350/390 Loss 0.011 Prec@(1,5) (8.6%, 26.2%)
07/22 04:34:31 PM | Train: [ 7/50] Step 390/390 Loss 0.011 Prec@(1,5) (8.7%, 26.3%)
07/22 04:34:31 PM | Train: [ 7/50] Final Prec@1 8.7080%
07/22 04:34:33 PM | Valid: [ 7/50] Step 000/390 Loss 4.018 Prec@(1,5) (10.9%, 31.2%)
07/22 04:34:53 PM | Valid: [ 7/50] Step 050/390 Loss 4.044 Prec@(1,5) (10.2%, 28.0%)
07/22 04:35:14 PM | Valid: [ 7/50] Step 100/390 Loss 4.054 Prec@(1,5) (9.4%, 27.6%)
07/22 04:35:35 PM | Valid: [ 7/50] Step 150/390 Loss 4.065 Prec@(1,5) (9.2%, 27.1%)
07/22 04:35:56 PM | Valid: [ 7/50] Step 200/390 Loss 4.066 Prec@(1,5) (9.1%, 27.1%)
07/22 04:36:18 PM | Valid: [ 7/50] Step 250/390 Loss 4.066 Prec@(1,5) (9.1%, 27.3%)
07/22 04:36:40 PM | Valid: [ 7/50] Step 300/390 Loss 4.071 Prec@(1,5) (9.0%, 27.1%)
07/22 04:37:00 PM | Valid: [ 7/50] Step 350/390 Loss 4.073 Prec@(1,5) (9.0%, 27.0%)
07/22 04:37:16 PM | Valid: [ 7/50] Step 390/390 Loss 4.075 Prec@(1,5) (8.9%, 27.0%)
07/22 04:37:16 PM | Valid: [ 7/50] Final Prec@1 8.9280%
07/22 04:37:16 PM | genotype = Genotype(normal=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('max_pool_3x3', 3), ('max_pool_3x3', 2)], [('skip_connect', 3), ('skip_connect', 4)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_5x5', 1)], [('max_pool_3x3', 0), ('dil_conv_3x3', 1)], [('sep_conv_5x5', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('avg_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1358, 0.1462, 0.1407, 0.1120, 0.1175, 0.1194, 0.1149, 0.1134],
        [0.1478, 0.1531, 0.1497, 0.1029, 0.1152, 0.1209, 0.1096, 0.1009]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1288, 0.1450, 0.1364, 0.1199, 0.1329, 0.1142, 0.1211, 0.1018],
        [0.1316, 0.1471, 0.1474, 0.1206, 0.1199, 0.1113, 0.1162, 0.1059],
        [0.1413, 0.1508, 0.1462, 0.1207, 0.1082, 0.1114, 0.1161, 0.1052]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1491, 0.1453, 0.1416, 0.1033, 0.1142, 0.1146, 0.1247, 0.1072],
        [0.1460, 0.1444, 0.1433, 0.1020, 0.1176, 0.1222, 0.1184, 0.1061],
        [0.1517, 0.1503, 0.1438, 0.1251, 0.1117, 0.1067, 0.1066, 0.1042],
        [0.1599, 0.1583, 0.1591, 0.1084, 0.1109, 0.1062, 0.1052, 0.0920]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1482, 0.1613, 0.1582, 0.1102, 0.1026, 0.1019, 0.1140, 0.1038],
        [0.1472, 0.1536, 0.1550, 0.1129, 0.1123, 0.1117, 0.1026, 0.1047],
        [0.1475, 0.1503, 0.1464, 0.1176, 0.1111, 0.1125, 0.1177, 0.0968],
        [0.1551, 0.1599, 0.1647, 0.1157, 0.1040, 0.1050, 0.1020, 0.0937],
        [0.1535, 0.1550, 0.1657, 0.0954, 0.1101, 0.1143, 0.1090, 0.0970]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1407, 0.1390, 0.1183, 0.1265, 0.1405, 0.1042, 0.1246, 0.1062],
        [0.1210, 0.1211, 0.1097, 0.1265, 0.1256, 0.1314, 0.1346, 0.1301]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1442, 0.1420, 0.1109, 0.1115, 0.1174, 0.1204, 0.1265, 0.1272],
        [0.1151, 0.1154, 0.1335, 0.1215, 0.1157, 0.1407, 0.1166, 0.1414],
        [0.1232, 0.1248, 0.1241, 0.1174, 0.1239, 0.1323, 0.1298, 0.1244]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1372, 0.1309, 0.1147, 0.1239, 0.1231, 0.1170, 0.1135, 0.1397],
        [0.1174, 0.1174, 0.1288, 0.1347, 0.1401, 0.1140, 0.1227, 0.1250],
        [0.1151, 0.1145, 0.1114, 0.1364, 0.1257, 0.1352, 0.1400, 0.1218],
        [0.1256, 0.1210, 0.1253, 0.1180, 0.1257, 0.1113, 0.1335, 0.1396]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1576, 0.1537, 0.1146, 0.1166, 0.1088, 0.1105, 0.1192, 0.1191],
        [0.1187, 0.1178, 0.1302, 0.1194, 0.1167, 0.1353, 0.1305, 0.1314],
        [0.1435, 0.1521, 0.1384, 0.1029, 0.1225, 0.1103, 0.1167, 0.1136],
        [0.1363, 0.1361, 0.1331, 0.1202, 0.1144, 0.1238, 0.1191, 0.1170],
        [0.1351, 0.1334, 0.1285, 0.1176, 0.1323, 0.1200, 0.1174, 0.1157]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/22 04:37:18 PM | size of train loader is 391
07/22 04:37:23 PM | Train: [ 8/50] Step 000/390 Loss 0.011 Prec@(1,5) (6.2%, 29.7%)
07/22 04:40:38 PM | Train: [ 8/50] Step 050/390 Loss 0.010 Prec@(1,5) (9.9%, 27.3%)
07/22 04:44:02 PM | Train: [ 8/50] Step 100/390 Loss 0.010 Prec@(1,5) (9.7%, 27.1%)
07/22 04:47:30 PM | Train: [ 8/50] Step 150/390 Loss 0.010 Prec@(1,5) (9.8%, 27.1%)
07/22 04:50:57 PM | Train: [ 8/50] Step 200/390 Loss 0.010 Prec@(1,5) (9.6%, 27.1%)
07/22 04:54:19 PM | Train: [ 8/50] Step 250/390 Loss 0.010 Prec@(1,5) (9.7%, 27.4%)
07/22 04:57:41 PM | Train: [ 8/50] Step 300/390 Loss 0.010 Prec@(1,5) (9.6%, 27.5%)
07/22 05:01:07 PM | Train: [ 8/50] Step 350/390 Loss 0.010 Prec@(1,5) (9.8%, 27.8%)
07/22 05:03:51 PM | Train: [ 8/50] Step 390/390 Loss 0.010 Prec@(1,5) (9.7%, 27.8%)
07/22 05:03:52 PM | Train: [ 8/50] Final Prec@1 9.7320%
07/22 05:03:53 PM | Valid: [ 8/50] Step 000/390 Loss 4.098 Prec@(1,5) (7.8%, 17.2%)
07/22 05:04:14 PM | Valid: [ 8/50] Step 050/390 Loss 4.020 Prec@(1,5) (10.0%, 28.1%)
07/22 05:04:34 PM | Valid: [ 8/50] Step 100/390 Loss 4.008 Prec@(1,5) (10.5%, 28.8%)
07/22 05:04:54 PM | Valid: [ 8/50] Step 150/390 Loss 4.012 Prec@(1,5) (10.4%, 29.0%)
07/22 05:05:14 PM | Valid: [ 8/50] Step 200/390 Loss 4.015 Prec@(1,5) (10.2%, 28.8%)
07/22 05:05:34 PM | Valid: [ 8/50] Step 250/390 Loss 4.011 Prec@(1,5) (10.1%, 29.0%)
07/22 05:05:54 PM | Valid: [ 8/50] Step 300/390 Loss 4.005 Prec@(1,5) (10.2%, 29.4%)
07/22 05:06:14 PM | Valid: [ 8/50] Step 350/390 Loss 4.007 Prec@(1,5) (10.2%, 29.1%)
07/22 05:06:28 PM | Valid: [ 8/50] Step 390/390 Loss 4.009 Prec@(1,5) (10.2%, 29.1%)
07/22 05:06:28 PM | Valid: [ 8/50] Final Prec@1 10.1680%
07/22 05:06:28 PM | genotype = Genotype(normal=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('skip_connect', 4), ('skip_connect', 3)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_5x5', 1)], [('max_pool_3x3', 0), ('dil_conv_3x3', 1)], [('sep_conv_5x5', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('avg_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1422, 0.1510, 0.1441, 0.1097, 0.1156, 0.1160, 0.1123, 0.1091],
        [0.1533, 0.1560, 0.1526, 0.0992, 0.1146, 0.1184, 0.1076, 0.0982]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1343, 0.1504, 0.1394, 0.1179, 0.1319, 0.1107, 0.1192, 0.0963],
        [0.1357, 0.1510, 0.1514, 0.1180, 0.1185, 0.1085, 0.1143, 0.1025],
        [0.1484, 0.1570, 0.1497, 0.1193, 0.1048, 0.1086, 0.1126, 0.0996]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1561, 0.1514, 0.1464, 0.0999, 0.1112, 0.1100, 0.1231, 0.1019],
        [0.1505, 0.1477, 0.1463, 0.0998, 0.1170, 0.1195, 0.1169, 0.1024],
        [0.1587, 0.1562, 0.1475, 0.1222, 0.1103, 0.1032, 0.1035, 0.0983],
        [0.1673, 0.1651, 0.1674, 0.1047, 0.1068, 0.1016, 0.1011, 0.0860]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1542, 0.1674, 0.1632, 0.1077, 0.0990, 0.0985, 0.1118, 0.0982],
        [0.1525, 0.1571, 0.1596, 0.1108, 0.1091, 0.1098, 0.1000, 0.1012],
        [0.1534, 0.1551, 0.1498, 0.1152, 0.1080, 0.1109, 0.1155, 0.0920],
        [0.1623, 0.1656, 0.1723, 0.1115, 0.1012, 0.1016, 0.0976, 0.0879],
        [0.1598, 0.1599, 0.1762, 0.0911, 0.1061, 0.1111, 0.1048, 0.0910]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1420, 0.1382, 0.1192, 0.1274, 0.1420, 0.1022, 0.1230, 0.1060],
        [0.1220, 0.1210, 0.1079, 0.1275, 0.1252, 0.1299, 0.1368, 0.1296]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1456, 0.1414, 0.1116, 0.1114, 0.1170, 0.1189, 0.1258, 0.1282],
        [0.1156, 0.1149, 0.1347, 0.1195, 0.1148, 0.1417, 0.1159, 0.1430],
        [0.1248, 0.1248, 0.1228, 0.1171, 0.1232, 0.1343, 0.1301, 0.1229]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1396, 0.1311, 0.1151, 0.1234, 0.1221, 0.1152, 0.1129, 0.1406],
        [0.1168, 0.1162, 0.1293, 0.1354, 0.1425, 0.1127, 0.1227, 0.1243],
        [0.1152, 0.1131, 0.1100, 0.1374, 0.1260, 0.1352, 0.1414, 0.1216],
        [0.1279, 0.1215, 0.1264, 0.1148, 0.1248, 0.1096, 0.1339, 0.1411]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1608, 0.1553, 0.1143, 0.1161, 0.1073, 0.1096, 0.1183, 0.1183],
        [0.1189, 0.1177, 0.1312, 0.1186, 0.1165, 0.1343, 0.1311, 0.1316],
        [0.1462, 0.1535, 0.1380, 0.0999, 0.1221, 0.1115, 0.1157, 0.1132],
        [0.1391, 0.1371, 0.1331, 0.1204, 0.1126, 0.1229, 0.1188, 0.1160],
        [0.1365, 0.1328, 0.1276, 0.1177, 0.1338, 0.1203, 0.1155, 0.1159]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/22 05:06:30 PM | size of train loader is 391
07/22 05:06:35 PM | Train: [ 9/50] Step 000/390 Loss 0.011 Prec@(1,5) (10.9%, 25.0%)
07/22 05:09:31 PM | Train: [ 9/50] Step 050/390 Loss 0.010 Prec@(1,5) (9.9%, 29.5%)
07/22 05:12:50 PM | Train: [ 9/50] Step 100/390 Loss 0.010 Prec@(1,5) (9.9%, 29.7%)
07/22 05:16:05 PM | Train: [ 9/50] Step 150/390 Loss 0.010 Prec@(1,5) (10.0%, 29.7%)
07/22 05:19:31 PM | Train: [ 9/50] Step 200/390 Loss 0.010 Prec@(1,5) (10.2%, 29.7%)
07/22 05:22:57 PM | Train: [ 9/50] Step 250/390 Loss 0.010 Prec@(1,5) (10.1%, 29.8%)
07/22 05:26:05 PM | Train: [ 9/50] Step 300/390 Loss 0.010 Prec@(1,5) (10.2%, 29.9%)
07/22 05:29:17 PM | Train: [ 9/50] Step 350/390 Loss 0.010 Prec@(1,5) (10.3%, 29.9%)
07/22 05:31:57 PM | Train: [ 9/50] Step 390/390 Loss 0.010 Prec@(1,5) (10.3%, 29.8%)
07/22 05:31:57 PM | Train: [ 9/50] Final Prec@1 10.2800%
07/22 05:31:59 PM | Valid: [ 9/50] Step 000/390 Loss 4.068 Prec@(1,5) (14.1%, 32.8%)
07/22 05:32:20 PM | Valid: [ 9/50] Step 050/390 Loss 3.929 Prec@(1,5) (11.1%, 30.6%)
07/22 05:32:41 PM | Valid: [ 9/50] Step 100/390 Loss 3.936 Prec@(1,5) (10.8%, 30.5%)
07/22 05:33:01 PM | Valid: [ 9/50] Step 150/390 Loss 3.942 Prec@(1,5) (11.0%, 30.6%)
07/22 05:33:21 PM | Valid: [ 9/50] Step 200/390 Loss 3.949 Prec@(1,5) (10.6%, 30.6%)
07/22 05:33:41 PM | Valid: [ 9/50] Step 250/390 Loss 3.947 Prec@(1,5) (10.9%, 30.9%)
07/22 05:34:01 PM | Valid: [ 9/50] Step 300/390 Loss 3.948 Prec@(1,5) (10.8%, 30.9%)
07/22 05:34:21 PM | Valid: [ 9/50] Step 350/390 Loss 3.947 Prec@(1,5) (10.8%, 31.0%)
07/22 05:34:35 PM | Valid: [ 9/50] Step 390/390 Loss 3.948 Prec@(1,5) (10.9%, 30.9%)
07/22 05:34:36 PM | Valid: [ 9/50] Final Prec@1 10.8520%
07/22 05:34:36 PM | genotype = Genotype(normal=[[('max_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('skip_connect', 4), ('skip_connect', 3)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_5x5', 1)], [('max_pool_3x3', 0), ('dil_conv_3x3', 1)], [('sep_conv_5x5', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('avg_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1500, 0.1546, 0.1468, 0.1066, 0.1148, 0.1121, 0.1100, 0.1051],
        [0.1596, 0.1579, 0.1540, 0.0963, 0.1137, 0.1167, 0.1059, 0.0958]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1407, 0.1551, 0.1427, 0.1160, 0.1306, 0.1078, 0.1165, 0.0907],
        [0.1391, 0.1533, 0.1541, 0.1167, 0.1176, 0.1062, 0.1131, 0.0999],
        [0.1551, 0.1619, 0.1532, 0.1178, 0.1014, 0.1062, 0.1098, 0.0946]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1631, 0.1558, 0.1500, 0.0980, 0.1092, 0.1059, 0.1210, 0.0970],
        [0.1545, 0.1491, 0.1479, 0.0984, 0.1178, 0.1171, 0.1154, 0.0998],
        [0.1645, 0.1599, 0.1495, 0.1204, 0.1099, 0.1011, 0.1014, 0.0934],
        [0.1745, 0.1698, 0.1747, 0.1016, 0.1031, 0.0976, 0.0979, 0.0808]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1602, 0.1732, 0.1683, 0.1041, 0.0952, 0.0955, 0.1101, 0.0934],
        [0.1573, 0.1593, 0.1624, 0.1109, 0.1067, 0.1076, 0.0980, 0.0977],
        [0.1592, 0.1591, 0.1527, 0.1124, 0.1057, 0.1097, 0.1135, 0.0878],
        [0.1689, 0.1708, 0.1804, 0.1077, 0.0980, 0.0977, 0.0934, 0.0831],
        [0.1652, 0.1637, 0.1868, 0.0870, 0.1024, 0.1079, 0.1015, 0.0855]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1426, 0.1370, 0.1213, 0.1281, 0.1422, 0.1010, 0.1220, 0.1058],
        [0.1227, 0.1205, 0.1078, 0.1277, 0.1238, 0.1310, 0.1369, 0.1295]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1471, 0.1412, 0.1115, 0.1115, 0.1160, 0.1179, 0.1257, 0.1290],
        [0.1152, 0.1135, 0.1348, 0.1199, 0.1148, 0.1428, 0.1151, 0.1439],
        [0.1253, 0.1238, 0.1209, 0.1167, 0.1229, 0.1358, 0.1318, 0.1228]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1421, 0.1310, 0.1154, 0.1236, 0.1208, 0.1148, 0.1122, 0.1401],
        [0.1165, 0.1150, 0.1290, 0.1351, 0.1443, 0.1119, 0.1234, 0.1248],
        [0.1151, 0.1117, 0.1079, 0.1377, 0.1272, 0.1370, 0.1433, 0.1200],
        [0.1306, 0.1220, 0.1274, 0.1123, 0.1239, 0.1079, 0.1343, 0.1415]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1627, 0.1554, 0.1152, 0.1166, 0.1058, 0.1086, 0.1185, 0.1171],
        [0.1171, 0.1157, 0.1329, 0.1174, 0.1173, 0.1355, 0.1325, 0.1316],
        [0.1477, 0.1530, 0.1367, 0.0982, 0.1216, 0.1123, 0.1161, 0.1144],
        [0.1414, 0.1370, 0.1320, 0.1204, 0.1120, 0.1225, 0.1191, 0.1156],
        [0.1380, 0.1319, 0.1264, 0.1186, 0.1327, 0.1205, 0.1147, 0.1173]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/22 05:34:37 PM | size of train loader is 391
07/22 05:34:43 PM | Train: [10/50] Step 000/390 Loss 0.010 Prec@(1,5) (20.3%, 39.1%)
07/22 05:37:38 PM | Train: [10/50] Step 050/390 Loss 0.010 Prec@(1,5) (10.3%, 31.3%)
07/22 05:40:30 PM | Train: [10/50] Step 100/390 Loss 0.010 Prec@(1,5) (10.7%, 30.8%)
07/22 05:43:31 PM | Train: [10/50] Step 150/390 Loss 0.010 Prec@(1,5) (10.3%, 30.6%)
07/22 05:46:26 PM | Train: [10/50] Step 200/390 Loss 0.010 Prec@(1,5) (10.6%, 31.1%)
07/22 05:49:14 PM | Train: [10/50] Step 250/390 Loss 0.010 Prec@(1,5) (10.8%, 31.3%)
07/22 05:52:01 PM | Train: [10/50] Step 300/390 Loss 0.010 Prec@(1,5) (10.9%, 31.4%)
07/22 05:54:47 PM | Train: [10/50] Step 350/390 Loss 0.010 Prec@(1,5) (11.0%, 31.4%)
07/22 05:57:00 PM | Train: [10/50] Step 390/390 Loss 0.010 Prec@(1,5) (11.0%, 31.4%)
07/22 05:57:00 PM | Train: [10/50] Final Prec@1 10.9520%
07/22 05:57:01 PM | Valid: [10/50] Step 000/390 Loss 4.014 Prec@(1,5) (3.1%, 23.4%)
07/22 05:57:21 PM | Valid: [10/50] Step 050/390 Loss 3.896 Prec@(1,5) (11.5%, 32.0%)
07/22 05:57:41 PM | Valid: [10/50] Step 100/390 Loss 3.909 Prec@(1,5) (11.5%, 31.7%)
07/22 05:58:02 PM | Valid: [10/50] Step 150/390 Loss 3.900 Prec@(1,5) (11.3%, 32.2%)
07/22 05:58:22 PM | Valid: [10/50] Step 200/390 Loss 3.896 Prec@(1,5) (11.5%, 32.4%)
07/22 05:58:42 PM | Valid: [10/50] Step 250/390 Loss 3.895 Prec@(1,5) (11.3%, 32.3%)
07/22 05:59:02 PM | Valid: [10/50] Step 300/390 Loss 3.898 Prec@(1,5) (11.3%, 32.3%)
07/22 05:59:22 PM | Valid: [10/50] Step 350/390 Loss 3.899 Prec@(1,5) (11.3%, 32.3%)
07/22 05:59:37 PM | Valid: [10/50] Step 390/390 Loss 3.898 Prec@(1,5) (11.4%, 32.3%)
07/22 05:59:37 PM | Valid: [10/50] Final Prec@1 11.3920%
07/22 05:59:37 PM | genotype = Genotype(normal=[[('max_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('max_pool_3x3', 3), ('max_pool_3x3', 2)], [('skip_connect', 4), ('skip_connect', 3)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_5x5', 1)], [('max_pool_3x3', 0), ('dil_conv_3x3', 1)], [('max_pool_3x3', 0), ('sep_conv_5x5', 1)], [('max_pool_3x3', 0), ('avg_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1577, 0.1577, 0.1492, 0.1038, 0.1136, 0.1093, 0.1074, 0.1013],
        [0.1655, 0.1581, 0.1544, 0.0941, 0.1129, 0.1157, 0.1053, 0.0941]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1468, 0.1593, 0.1451, 0.1149, 0.1284, 0.1050, 0.1147, 0.0857],
        [0.1423, 0.1546, 0.1559, 0.1159, 0.1170, 0.1041, 0.1124, 0.0979],
        [0.1614, 0.1657, 0.1558, 0.1163, 0.0979, 0.1055, 0.1071, 0.0903]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1694, 0.1588, 0.1519, 0.0959, 0.1074, 0.1027, 0.1203, 0.0938],
        [0.1590, 0.1502, 0.1495, 0.0974, 0.1177, 0.1144, 0.1136, 0.0982],
        [0.1699, 0.1627, 0.1508, 0.1182, 0.1100, 0.0992, 0.0992, 0.0900],
        [0.1816, 0.1736, 0.1808, 0.0992, 0.0994, 0.0938, 0.0950, 0.0767]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1659, 0.1791, 0.1733, 0.1008, 0.0917, 0.0926, 0.1081, 0.0885],
        [0.1618, 0.1605, 0.1643, 0.1109, 0.1048, 0.1070, 0.0959, 0.0948],
        [0.1653, 0.1633, 0.1555, 0.1089, 0.1031, 0.1081, 0.1120, 0.0837],
        [0.1752, 0.1750, 0.1881, 0.1041, 0.0951, 0.0946, 0.0893, 0.0785],
        [0.1708, 0.1669, 0.1975, 0.0830, 0.0989, 0.1047, 0.0976, 0.0805]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1444, 0.1363, 0.1231, 0.1292, 0.1419, 0.0997, 0.1205, 0.1051],
        [0.1222, 0.1187, 0.1076, 0.1297, 0.1222, 0.1317, 0.1381, 0.1299]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1486, 0.1409, 0.1108, 0.1115, 0.1175, 0.1173, 0.1247, 0.1287],
        [0.1142, 0.1114, 0.1349, 0.1198, 0.1153, 0.1454, 0.1136, 0.1456],
        [0.1268, 0.1230, 0.1198, 0.1163, 0.1219, 0.1364, 0.1326, 0.1232]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1452, 0.1320, 0.1146, 0.1222, 0.1200, 0.1149, 0.1116, 0.1395],
        [0.1151, 0.1128, 0.1282, 0.1358, 0.1466, 0.1123, 0.1243, 0.1248],
        [0.1156, 0.1106, 0.1072, 0.1383, 0.1273, 0.1374, 0.1436, 0.1199],
        [0.1340, 0.1236, 0.1294, 0.1092, 0.1211, 0.1061, 0.1352, 0.1414]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1653, 0.1556, 0.1159, 0.1166, 0.1044, 0.1081, 0.1185, 0.1155],
        [0.1161, 0.1141, 0.1343, 0.1167, 0.1179, 0.1361, 0.1324, 0.1322],
        [0.1498, 0.1531, 0.1362, 0.0962, 0.1206, 0.1132, 0.1161, 0.1148],
        [0.1433, 0.1363, 0.1306, 0.1216, 0.1116, 0.1220, 0.1198, 0.1148],
        [0.1395, 0.1309, 0.1255, 0.1180, 0.1330, 0.1216, 0.1129, 0.1185]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/22 05:59:38 PM | size of train loader is 391
07/22 05:59:43 PM | Train: [11/50] Step 000/390 Loss 0.010 Prec@(1,5) (14.1%, 26.6%)
07/22 06:02:37 PM | Train: [11/50] Step 050/390 Loss 0.010 Prec@(1,5) (11.1%, 31.3%)
07/22 06:05:31 PM | Train: [11/50] Step 100/390 Loss 0.010 Prec@(1,5) (11.1%, 31.2%)
07/22 06:08:21 PM | Train: [11/50] Step 150/390 Loss 0.010 Prec@(1,5) (11.4%, 31.5%)
07/22 06:11:11 PM | Train: [11/50] Step 200/390 Loss 0.010 Prec@(1,5) (11.5%, 31.7%)
07/22 06:14:01 PM | Train: [11/50] Step 250/390 Loss 0.010 Prec@(1,5) (11.3%, 32.3%)
07/22 06:16:52 PM | Train: [11/50] Step 300/390 Loss 0.010 Prec@(1,5) (11.3%, 32.4%)
07/22 06:19:46 PM | Train: [11/50] Step 350/390 Loss 0.010 Prec@(1,5) (11.4%, 32.7%)
07/22 06:21:59 PM | Train: [11/50] Step 390/390 Loss 0.010 Prec@(1,5) (11.3%, 32.6%)
07/22 06:22:00 PM | Train: [11/50] Final Prec@1 11.3040%
07/22 06:22:01 PM | Valid: [11/50] Step 000/390 Loss 3.878 Prec@(1,5) (9.4%, 35.9%)
07/22 06:22:21 PM | Valid: [11/50] Step 050/390 Loss 3.862 Prec@(1,5) (13.3%, 34.5%)
07/22 06:22:41 PM | Valid: [11/50] Step 100/390 Loss 3.863 Prec@(1,5) (12.6%, 33.6%)
07/22 06:23:01 PM | Valid: [11/50] Step 150/390 Loss 3.863 Prec@(1,5) (12.2%, 33.4%)
07/22 06:23:21 PM | Valid: [11/50] Step 200/390 Loss 3.874 Prec@(1,5) (12.1%, 33.0%)
07/22 06:23:41 PM | Valid: [11/50] Step 250/390 Loss 3.870 Prec@(1,5) (12.1%, 33.1%)
07/22 06:24:01 PM | Valid: [11/50] Step 300/390 Loss 3.872 Prec@(1,5) (12.0%, 32.9%)
07/22 06:24:21 PM | Valid: [11/50] Step 350/390 Loss 3.867 Prec@(1,5) (12.1%, 33.2%)
07/22 06:24:35 PM | Valid: [11/50] Step 390/390 Loss 3.866 Prec@(1,5) (12.2%, 33.3%)
07/22 06:24:36 PM | Valid: [11/50] Final Prec@1 12.1800%
07/22 06:24:36 PM | genotype = Genotype(normal=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('max_pool_3x3', 3), ('max_pool_3x3', 2)], [('skip_connect', 4), ('skip_connect', 3)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_5x5', 1)], [('max_pool_3x3', 0), ('dil_conv_3x3', 1)], [('max_pool_3x3', 0), ('sep_conv_5x5', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1650, 0.1596, 0.1504, 0.1021, 0.1124, 0.1073, 0.1057, 0.0975],
        [0.1722, 0.1574, 0.1540, 0.0917, 0.1128, 0.1144, 0.1051, 0.0924]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1528, 0.1616, 0.1464, 0.1145, 0.1269, 0.1035, 0.1123, 0.0820],
        [0.1460, 0.1546, 0.1568, 0.1154, 0.1167, 0.1022, 0.1115, 0.0967],
        [0.1675, 0.1679, 0.1571, 0.1161, 0.0952, 0.1033, 0.1063, 0.0865]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1752, 0.1609, 0.1535, 0.0939, 0.1062, 0.0987, 0.1207, 0.0909],
        [0.1630, 0.1502, 0.1505, 0.0974, 0.1184, 0.1113, 0.1122, 0.0969],
        [0.1749, 0.1646, 0.1527, 0.1152, 0.1098, 0.0982, 0.0973, 0.0874],
        [0.1861, 0.1749, 0.1860, 0.0983, 0.0967, 0.0911, 0.0935, 0.0734]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1720, 0.1840, 0.1776, 0.0978, 0.0886, 0.0895, 0.1065, 0.0840],
        [0.1659, 0.1602, 0.1650, 0.1109, 0.1031, 0.1065, 0.0955, 0.0929],
        [0.1704, 0.1655, 0.1569, 0.1075, 0.1018, 0.1074, 0.1102, 0.0804],
        [0.1816, 0.1786, 0.1959, 0.1005, 0.0923, 0.0913, 0.0849, 0.0749],
        [0.1756, 0.1684, 0.2079, 0.0797, 0.0955, 0.1019, 0.0944, 0.0765]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1456, 0.1347, 0.1254, 0.1294, 0.1412, 0.0984, 0.1194, 0.1060],
        [0.1225, 0.1170, 0.1074, 0.1304, 0.1214, 0.1308, 0.1411, 0.1295]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1498, 0.1396, 0.1104, 0.1119, 0.1185, 0.1181, 0.1242, 0.1275],
        [0.1147, 0.1103, 0.1341, 0.1205, 0.1146, 0.1473, 0.1119, 0.1468],
        [0.1278, 0.1218, 0.1186, 0.1151, 0.1207, 0.1393, 0.1332, 0.1235]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1475, 0.1315, 0.1152, 0.1212, 0.1180, 0.1143, 0.1123, 0.1400],
        [0.1152, 0.1115, 0.1265, 0.1352, 0.1484, 0.1129, 0.1257, 0.1245],
        [0.1161, 0.1088, 0.1054, 0.1383, 0.1277, 0.1384, 0.1459, 0.1196],
        [0.1371, 0.1243, 0.1308, 0.1065, 0.1192, 0.1045, 0.1355, 0.1421]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1674, 0.1549, 0.1168, 0.1158, 0.1045, 0.1082, 0.1181, 0.1144],
        [0.1155, 0.1127, 0.1356, 0.1151, 0.1190, 0.1386, 0.1314, 0.1321],
        [0.1523, 0.1522, 0.1347, 0.0946, 0.1218, 0.1142, 0.1148, 0.1153],
        [0.1464, 0.1361, 0.1299, 0.1221, 0.1104, 0.1214, 0.1203, 0.1133],
        [0.1415, 0.1305, 0.1253, 0.1182, 0.1318, 0.1216, 0.1119, 0.1192]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/22 06:24:37 PM | size of train loader is 391
07/22 06:24:42 PM | Train: [12/50] Step 000/390 Loss 0.010 Prec@(1,5) (10.9%, 26.6%)
07/22 06:27:33 PM | Train: [12/50] Step 050/390 Loss 0.010 Prec@(1,5) (11.9%, 33.8%)
07/22 06:30:23 PM | Train: [12/50] Step 100/390 Loss 0.010 Prec@(1,5) (12.1%, 33.6%)
07/22 06:33:11 PM | Train: [12/50] Step 150/390 Loss 0.010 Prec@(1,5) (11.8%, 33.0%)
07/22 06:36:01 PM | Train: [12/50] Step 200/390 Loss 0.010 Prec@(1,5) (11.9%, 33.1%)
07/22 06:38:47 PM | Train: [12/50] Step 250/390 Loss 0.010 Prec@(1,5) (12.0%, 33.4%)
07/22 06:41:38 PM | Train: [12/50] Step 300/390 Loss 0.010 Prec@(1,5) (12.1%, 33.5%)
07/22 06:44:27 PM | Train: [12/50] Step 350/390 Loss 0.010 Prec@(1,5) (12.2%, 33.7%)
07/22 06:46:39 PM | Train: [12/50] Step 390/390 Loss 0.010 Prec@(1,5) (12.2%, 33.9%)
07/22 06:46:40 PM | Train: [12/50] Final Prec@1 12.2440%
07/22 06:46:41 PM | Valid: [12/50] Step 000/390 Loss 3.662 Prec@(1,5) (9.4%, 31.2%)
07/22 06:47:01 PM | Valid: [12/50] Step 050/390 Loss 3.848 Prec@(1,5) (13.0%, 34.7%)
07/22 06:47:21 PM | Valid: [12/50] Step 100/390 Loss 3.828 Prec@(1,5) (13.1%, 35.1%)
07/22 06:47:41 PM | Valid: [12/50] Step 150/390 Loss 3.822 Prec@(1,5) (13.1%, 35.0%)
07/22 06:48:02 PM | Valid: [12/50] Step 200/390 Loss 3.817 Prec@(1,5) (13.0%, 34.8%)
07/22 06:48:22 PM | Valid: [12/50] Step 250/390 Loss 3.821 Prec@(1,5) (12.9%, 34.6%)
07/22 06:48:42 PM | Valid: [12/50] Step 300/390 Loss 3.819 Prec@(1,5) (13.0%, 34.7%)
07/22 06:49:02 PM | Valid: [12/50] Step 350/390 Loss 3.818 Prec@(1,5) (12.8%, 34.7%)
07/22 06:49:16 PM | Valid: [12/50] Step 390/390 Loss 3.818 Prec@(1,5) (12.7%, 34.5%)
07/22 06:49:16 PM | Valid: [12/50] Final Prec@1 12.7040%
07/22 06:49:16 PM | genotype = Genotype(normal=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 2), ('avg_pool_3x3', 0)], [('max_pool_3x3', 3), ('max_pool_3x3', 2)], [('skip_connect', 4), ('skip_connect', 3)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_5x5', 1)], [('dil_conv_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('sep_conv_5x5', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1726, 0.1601, 0.1502, 0.1006, 0.1122, 0.1057, 0.1043, 0.0943],
        [0.1805, 0.1572, 0.1544, 0.0893, 0.1124, 0.1116, 0.1040, 0.0907]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1594, 0.1635, 0.1476, 0.1141, 0.1248, 0.1021, 0.1100, 0.0785],
        [0.1512, 0.1557, 0.1591, 0.1140, 0.1151, 0.0996, 0.1105, 0.0947],
        [0.1749, 0.1703, 0.1590, 0.1151, 0.0926, 0.1014, 0.1041, 0.0826]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1811, 0.1615, 0.1536, 0.0926, 0.1058, 0.0954, 0.1210, 0.0889],
        [0.1674, 0.1509, 0.1518, 0.0964, 0.1200, 0.1085, 0.1097, 0.0952],
        [0.1799, 0.1657, 0.1542, 0.1127, 0.1096, 0.0970, 0.0959, 0.0850],
        [0.1908, 0.1756, 0.1906, 0.0980, 0.0938, 0.0881, 0.0925, 0.0706]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1773, 0.1868, 0.1805, 0.0955, 0.0852, 0.0875, 0.1061, 0.0809],
        [0.1710, 0.1595, 0.1652, 0.1108, 0.1015, 0.1057, 0.0952, 0.0912],
        [0.1749, 0.1659, 0.1567, 0.1063, 0.1013, 0.1077, 0.1093, 0.0778],
        [0.1872, 0.1799, 0.2013, 0.0980, 0.0908, 0.0884, 0.0819, 0.0723],
        [0.1797, 0.1680, 0.2166, 0.0775, 0.0932, 0.0987, 0.0926, 0.0736]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1476, 0.1337, 0.1264, 0.1299, 0.1404, 0.0966, 0.1186, 0.1068],
        [0.1219, 0.1149, 0.1086, 0.1315, 0.1212, 0.1311, 0.1417, 0.1291]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1513, 0.1381, 0.1099, 0.1122, 0.1191, 0.1186, 0.1250, 0.1259],
        [0.1133, 0.1075, 0.1340, 0.1209, 0.1152, 0.1503, 0.1100, 0.1487],
        [0.1282, 0.1201, 0.1168, 0.1145, 0.1201, 0.1416, 0.1344, 0.1243]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1494, 0.1305, 0.1150, 0.1197, 0.1172, 0.1136, 0.1137, 0.1410],
        [0.1153, 0.1103, 0.1254, 0.1333, 0.1501, 0.1153, 0.1255, 0.1247],
        [0.1164, 0.1075, 0.1051, 0.1390, 0.1269, 0.1372, 0.1471, 0.1208],
        [0.1405, 0.1251, 0.1321, 0.1034, 0.1169, 0.1028, 0.1371, 0.1421]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1710, 0.1551, 0.1177, 0.1135, 0.1037, 0.1077, 0.1190, 0.1123],
        [0.1139, 0.1105, 0.1376, 0.1138, 0.1208, 0.1390, 0.1319, 0.1325],
        [0.1547, 0.1523, 0.1331, 0.0927, 0.1219, 0.1162, 0.1143, 0.1149],
        [0.1490, 0.1355, 0.1280, 0.1253, 0.1102, 0.1214, 0.1193, 0.1112],
        [0.1438, 0.1298, 0.1243, 0.1174, 0.1331, 0.1211, 0.1102, 0.1203]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/22 06:49:17 PM | size of train loader is 391
07/22 06:49:23 PM | Train: [13/50] Step 000/390 Loss 0.010 Prec@(1,5) (9.4%, 32.8%)
07/22 06:52:14 PM | Train: [13/50] Step 050/390 Loss 0.010 Prec@(1,5) (11.4%, 33.4%)
07/22 06:55:04 PM | Train: [13/50] Step 100/390 Loss 0.010 Prec@(1,5) (12.0%, 34.0%)
07/22 06:58:10 PM | Train: [13/50] Step 150/390 Loss 0.010 Prec@(1,5) (12.1%, 34.3%)
07/22 07:01:38 PM | Train: [13/50] Step 200/390 Loss 0.010 Prec@(1,5) (12.1%, 34.3%)
07/22 07:05:02 PM | Train: [13/50] Step 250/390 Loss 0.010 Prec@(1,5) (12.3%, 34.4%)
