07/22 01:29:22 PM | 
07/22 01:29:22 PM | Parameters:
07/22 01:29:22 PM | ALPHA_LR=0.0003
07/22 01:29:22 PM | ALPHA_WEIGHT_DECAY=0.001
07/22 01:29:22 PM | BATCH_SIZE=64
07/22 01:29:22 PM | DATA_PATH=./data/
07/22 01:29:22 PM | DATASET=cifar100
07/22 01:29:22 PM | EPOCHS=50
07/22 01:29:22 PM | GPUS=[0]
07/22 01:29:22 PM | INIT_CHANNELS=16
07/22 01:29:22 PM | LAYERS=8
07/22 01:29:22 PM | NAME=cifar100-ignore
07/22 01:29:22 PM | PATH=searchs/cifar100-ignore
07/22 01:29:22 PM | PLOT_PATH=searchs/cifar100-ignore/plots
07/22 01:29:22 PM | PRINT_FREQ=50
07/22 01:29:22 PM | SEED=2
07/22 01:29:22 PM | W_GRAD_CLIP=5.0
07/22 01:29:22 PM | W_LR=0.025
07/22 01:29:22 PM | W_LR_MIN=0.001
07/22 01:29:22 PM | W_MOMENTUM=0.9
07/22 01:29:22 PM | W_WEIGHT_DECAY=0.0003
07/22 01:29:22 PM | WORKERS=4
07/22 01:29:22 PM | 
07/22 01:29:22 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/22 01:29:29 PM | size of train loader is 391
07/22 01:29:36 PM | Train: [ 1/50] Step 000/390 Loss 0.012 Prec@(1,5) (1.6%, 9.4%)
07/22 01:33:05 PM | Train: [ 1/50] Step 050/390 Loss 0.012 Prec@(1,5) (1.0%, 5.2%)
07/22 01:36:08 PM | Train: [ 1/50] Step 100/390 Loss 0.012 Prec@(1,5) (1.2%, 5.6%)
07/22 01:39:11 PM | Train: [ 1/50] Step 150/390 Loss 0.012 Prec@(1,5) (1.3%, 5.9%)
07/22 01:42:03 PM | Train: [ 1/50] Step 200/390 Loss 0.012 Prec@(1,5) (1.3%, 6.2%)
07/22 01:44:53 PM | Train: [ 1/50] Step 250/390 Loss 0.012 Prec@(1,5) (1.5%, 6.6%)
07/22 01:47:43 PM | Train: [ 1/50] Step 300/390 Loss 0.012 Prec@(1,5) (1.7%, 6.9%)
07/22 01:50:47 PM | Train: [ 1/50] Step 350/390 Loss 0.012 Prec@(1,5) (1.8%, 7.3%)
07/22 01:53:09 PM | Train: [ 1/50] Step 390/390 Loss 0.012 Prec@(1,5) (1.9%, 7.6%)
07/22 01:53:09 PM | Train: [ 1/50] Final Prec@1 1.9280%
07/22 01:53:11 PM | Valid: [ 1/50] Step 000/390 Loss 4.343 Prec@(1,5) (6.2%, 9.4%)
07/22 01:53:31 PM | Valid: [ 1/50] Step 050/390 Loss 4.483 Prec@(1,5) (3.3%, 11.4%)
07/22 01:53:51 PM | Valid: [ 1/50] Step 100/390 Loss 4.490 Prec@(1,5) (2.9%, 11.0%)
07/22 01:54:11 PM | Valid: [ 1/50] Step 150/390 Loss 4.492 Prec@(1,5) (3.0%, 11.0%)
07/22 01:54:31 PM | Valid: [ 1/50] Step 200/390 Loss 4.491 Prec@(1,5) (3.0%, 11.1%)
07/22 01:54:51 PM | Valid: [ 1/50] Step 250/390 Loss 4.495 Prec@(1,5) (2.9%, 10.9%)
07/22 01:55:11 PM | Valid: [ 1/50] Step 300/390 Loss 4.497 Prec@(1,5) (2.8%, 10.7%)
07/22 01:55:31 PM | Valid: [ 1/50] Step 350/390 Loss 4.498 Prec@(1,5) (2.7%, 10.6%)
07/22 01:55:45 PM | Valid: [ 1/50] Step 390/390 Loss 4.499 Prec@(1,5) (2.7%, 10.5%)
07/22 01:55:46 PM | Valid: [ 1/50] Final Prec@1 2.7480%
07/22 01:55:46 PM | genotype = Genotype(normal=[[('dil_conv_3x3', 0), ('dil_conv_3x3', 1)], [('sep_conv_3x3', 0), ('sep_conv_3x3', 1)], [('dil_conv_3x3', 1), ('dil_conv_3x3', 0)], [('sep_conv_5x5', 4), ('dil_conv_5x5', 2)]], normal_concat=range(2, 6), reduce=[[('sep_conv_5x5', 0), ('skip_connect', 1)], [('dil_conv_3x3', 1), ('max_pool_3x3', 0)], [('sep_conv_5x5', 0), ('dil_conv_5x5', 2)], [('max_pool_3x3', 0), ('avg_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1224, 0.1251, 0.1249, 0.1230, 0.1264, 0.1288, 0.1244, 0.1251],
        [0.1240, 0.1246, 0.1256, 0.1243, 0.1229, 0.1275, 0.1263, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1206, 0.1231, 0.1228, 0.1285, 0.1263, 0.1267, 0.1263, 0.1257],
        [0.1205, 0.1215, 0.1226, 0.1278, 0.1269, 0.1262, 0.1271, 0.1274],
        [0.1202, 0.1213, 0.1235, 0.1255, 0.1272, 0.1256, 0.1276, 0.1291]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1233, 0.1226, 0.1232, 0.1231, 0.1242, 0.1286, 0.1271, 0.1277],
        [0.1224, 0.1214, 0.1216, 0.1249, 0.1258, 0.1295, 0.1282, 0.1261],
        [0.1216, 0.1207, 0.1228, 0.1279, 0.1253, 0.1255, 0.1260, 0.1301],
        [0.1215, 0.1205, 0.1210, 0.1259, 0.1281, 0.1289, 0.1283, 0.1259]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1274, 0.1273, 0.1237, 0.1228, 0.1240, 0.1252, 0.1245],
        [0.1216, 0.1217, 0.1228, 0.1252, 0.1282, 0.1252, 0.1251, 0.1303],
        [0.1205, 0.1208, 0.1251, 0.1269, 0.1255, 0.1233, 0.1286, 0.1294],
        [0.1208, 0.1216, 0.1239, 0.1273, 0.1244, 0.1270, 0.1263, 0.1286],
        [0.1216, 0.1218, 0.1226, 0.1225, 0.1301, 0.1270, 0.1244, 0.1299]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1281, 0.1283, 0.1198, 0.1254, 0.1289, 0.1208, 0.1269, 0.1217],
        [0.1227, 0.1222, 0.1274, 0.1264, 0.1261, 0.1261, 0.1255, 0.1238]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1293, 0.1290, 0.1225, 0.1201, 0.1248, 0.1233, 0.1262, 0.1247],
        [0.1190, 0.1191, 0.1286, 0.1280, 0.1260, 0.1299, 0.1208, 0.1285],
        [0.1225, 0.1217, 0.1232, 0.1243, 0.1261, 0.1283, 0.1279, 0.1261]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1235, 0.1241, 0.1228, 0.1253, 0.1300, 0.1238, 0.1228, 0.1276],
        [0.1260, 0.1260, 0.1214, 0.1271, 0.1262, 0.1231, 0.1238, 0.1265],
        [0.1212, 0.1229, 0.1235, 0.1254, 0.1253, 0.1276, 0.1289, 0.1254],
        [0.1221, 0.1227, 0.1251, 0.1269, 0.1245, 0.1237, 0.1280, 0.1270]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1294, 0.1292, 0.1232, 0.1233, 0.1201, 0.1237, 0.1272, 0.1240],
        [0.1234, 0.1223, 0.1275, 0.1255, 0.1229, 0.1262, 0.1258, 0.1264],
        [0.1267, 0.1282, 0.1279, 0.1221, 0.1260, 0.1220, 0.1241, 0.1230],
        [0.1262, 0.1265, 0.1278, 0.1233, 0.1214, 0.1263, 0.1257, 0.1228],
        [0.1269, 0.1276, 0.1283, 0.1208, 0.1251, 0.1246, 0.1238, 0.1231]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/22 01:55:47 PM | size of train loader is 391
07/22 01:55:53 PM | Train: [ 2/50] Step 000/390 Loss 0.011 Prec@(1,5) (3.1%, 14.1%)
07/22 01:58:46 PM | Train: [ 2/50] Step 050/390 Loss 0.011 Prec@(1,5) (2.7%, 11.4%)
07/22 02:01:50 PM | Train: [ 2/50] Step 100/390 Loss 0.011 Prec@(1,5) (3.0%, 11.5%)
07/22 02:04:51 PM | Train: [ 2/50] Step 150/390 Loss 0.011 Prec@(1,5) (3.1%, 11.4%)
07/22 02:07:48 PM | Train: [ 2/50] Step 200/390 Loss 0.011 Prec@(1,5) (3.3%, 11.6%)
07/22 02:10:44 PM | Train: [ 2/50] Step 250/390 Loss 0.011 Prec@(1,5) (3.3%, 11.8%)
07/22 02:13:47 PM | Train: [ 2/50] Step 300/390 Loss 0.011 Prec@(1,5) (3.4%, 11.9%)
07/22 02:16:41 PM | Train: [ 2/50] Step 350/390 Loss 0.011 Prec@(1,5) (3.4%, 12.0%)
07/22 02:18:55 PM | Train: [ 2/50] Step 390/390 Loss 0.011 Prec@(1,5) (3.5%, 12.1%)
07/22 02:18:55 PM | Train: [ 2/50] Final Prec@1 3.4840%
07/22 02:18:56 PM | Valid: [ 2/50] Step 000/390 Loss 4.299 Prec@(1,5) (6.2%, 15.6%)
07/22 02:19:16 PM | Valid: [ 2/50] Step 050/390 Loss 4.417 Prec@(1,5) (3.6%, 13.4%)
07/22 02:19:36 PM | Valid: [ 2/50] Step 100/390 Loss 4.414 Prec@(1,5) (4.0%, 13.8%)
07/22 02:19:56 PM | Valid: [ 2/50] Step 150/390 Loss 4.411 Prec@(1,5) (4.1%, 14.1%)
07/22 02:20:17 PM | Valid: [ 2/50] Step 200/390 Loss 4.413 Prec@(1,5) (4.2%, 14.2%)
07/22 02:20:37 PM | Valid: [ 2/50] Step 250/390 Loss 4.413 Prec@(1,5) (4.0%, 13.9%)
07/22 02:20:57 PM | Valid: [ 2/50] Step 300/390 Loss 4.413 Prec@(1,5) (4.0%, 13.8%)
07/22 02:21:17 PM | Valid: [ 2/50] Step 350/390 Loss 4.412 Prec@(1,5) (4.1%, 14.0%)
07/22 02:21:31 PM | Valid: [ 2/50] Step 390/390 Loss 4.410 Prec@(1,5) (4.1%, 14.2%)
07/22 02:21:31 PM | Valid: [ 2/50] Final Prec@1 4.1080%
07/22 02:21:31 PM | genotype = Genotype(normal=[[('dil_conv_3x3', 0), ('skip_connect', 1)], [('sep_conv_3x3', 0), ('dil_conv_5x5', 2)], [('dil_conv_3x3', 0), ('sep_conv_3x3', 2)], [('avg_pool_3x3', 0), ('sep_conv_5x5', 4)]], normal_concat=range(2, 6), reduce=[[('avg_pool_3x3', 0), ('sep_conv_3x3', 1)], [('dil_conv_3x3', 1), ('max_pool_3x3', 0)], [('dil_conv_5x5', 2), ('sep_conv_5x5', 0)], [('max_pool_3x3', 0), ('avg_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1210, 0.1266, 0.1262, 0.1208, 0.1262, 0.1307, 0.1233, 0.1252],
        [0.1257, 0.1292, 0.1297, 0.1202, 0.1216, 0.1283, 0.1246, 0.1207]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1184, 0.1240, 0.1229, 0.1291, 0.1282, 0.1267, 0.1268, 0.1239],
        [0.1198, 0.1239, 0.1252, 0.1273, 0.1259, 0.1244, 0.1271, 0.1264],
        [0.1194, 0.1225, 0.1250, 0.1254, 0.1268, 0.1242, 0.1283, 0.1286]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1253, 0.1245, 0.1250, 0.1187, 0.1226, 0.1291, 0.1281, 0.1266],
        [0.1236, 0.1230, 0.1228, 0.1226, 0.1243, 0.1296, 0.1293, 0.1247],
        [0.1234, 0.1224, 0.1249, 0.1289, 0.1244, 0.1232, 0.1228, 0.1300],
        [0.1239, 0.1227, 0.1229, 0.1256, 0.1280, 0.1279, 0.1263, 0.1227]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1274, 0.1319, 0.1316, 0.1215, 0.1211, 0.1203, 0.1233, 0.1229],
        [0.1226, 0.1242, 0.1253, 0.1233, 0.1286, 0.1237, 0.1228, 0.1294],
        [0.1213, 0.1221, 0.1277, 0.1264, 0.1249, 0.1208, 0.1288, 0.1281],
        [0.1226, 0.1243, 0.1278, 0.1275, 0.1228, 0.1238, 0.1244, 0.1269],
        [0.1232, 0.1237, 0.1251, 0.1172, 0.1298, 0.1274, 0.1251, 0.1286]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1302, 0.1312, 0.1185, 0.1265, 0.1306, 0.1181, 0.1274, 0.1174],
        [0.1193, 0.1195, 0.1267, 0.1281, 0.1270, 0.1277, 0.1270, 0.1247]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1323, 0.1323, 0.1180, 0.1171, 0.1249, 0.1236, 0.1268, 0.1250],
        [0.1154, 0.1160, 0.1306, 0.1279, 0.1251, 0.1327, 0.1194, 0.1329],
        [0.1217, 0.1211, 0.1231, 0.1230, 0.1256, 0.1303, 0.1292, 0.1260]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1247, 0.1256, 0.1200, 0.1249, 0.1307, 0.1219, 0.1216, 0.1307],
        [0.1239, 0.1240, 0.1213, 0.1280, 0.1295, 0.1216, 0.1237, 0.1280],
        [0.1192, 0.1214, 0.1214, 0.1272, 0.1242, 0.1288, 0.1326, 0.1253],
        [0.1205, 0.1213, 0.1242, 0.1254, 0.1263, 0.1223, 0.1309, 0.1291]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1348, 0.1343, 0.1203, 0.1210, 0.1173, 0.1227, 0.1266, 0.1230],
        [0.1222, 0.1209, 0.1283, 0.1249, 0.1216, 0.1281, 0.1258, 0.1282],
        [0.1310, 0.1337, 0.1311, 0.1192, 0.1253, 0.1183, 0.1218, 0.1196],
        [0.1282, 0.1288, 0.1300, 0.1225, 0.1186, 0.1259, 0.1255, 0.1205],
        [0.1299, 0.1307, 0.1307, 0.1173, 0.1258, 0.1231, 0.1225, 0.1199]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/22 02:21:32 PM | size of train loader is 391
07/22 02:21:38 PM | Train: [ 3/50] Step 000/390 Loss 0.011 Prec@(1,5) (3.1%, 6.2%)
07/22 02:24:44 PM | Train: [ 3/50] Step 050/390 Loss 0.011 Prec@(1,5) (4.3%, 13.5%)
07/22 02:27:42 PM | Train: [ 3/50] Step 100/390 Loss 0.011 Prec@(1,5) (4.4%, 14.0%)
07/22 02:30:45 PM | Train: [ 3/50] Step 150/390 Loss 0.011 Prec@(1,5) (4.4%, 14.3%)
07/22 02:33:58 PM | Train: [ 3/50] Step 200/390 Loss 0.011 Prec@(1,5) (4.4%, 14.5%)
07/22 02:37:20 PM | Train: [ 3/50] Step 250/390 Loss 0.011 Prec@(1,5) (4.4%, 14.8%)
07/22 02:40:19 PM | Train: [ 3/50] Step 300/390 Loss 0.011 Prec@(1,5) (4.5%, 15.1%)
07/22 02:43:16 PM | Train: [ 3/50] Step 350/390 Loss 0.011 Prec@(1,5) (4.5%, 15.3%)
07/22 02:45:37 PM | Train: [ 3/50] Step 390/390 Loss 0.011 Prec@(1,5) (4.6%, 15.4%)
07/22 02:45:37 PM | Train: [ 3/50] Final Prec@1 4.5760%
07/22 02:45:38 PM | Valid: [ 3/50] Step 000/390 Loss 4.294 Prec@(1,5) (1.6%, 17.2%)
07/22 02:45:58 PM | Valid: [ 3/50] Step 050/390 Loss 4.355 Prec@(1,5) (5.0%, 17.3%)
07/22 02:46:18 PM | Valid: [ 3/50] Step 100/390 Loss 4.356 Prec@(1,5) (5.1%, 17.1%)
07/22 02:46:38 PM | Valid: [ 3/50] Step 150/390 Loss 4.355 Prec@(1,5) (5.3%, 17.3%)
07/22 02:46:59 PM | Valid: [ 3/50] Step 200/390 Loss 4.352 Prec@(1,5) (5.4%, 17.2%)
07/22 02:47:18 PM | Valid: [ 3/50] Step 250/390 Loss 4.345 Prec@(1,5) (5.4%, 17.4%)
07/22 02:47:39 PM | Valid: [ 3/50] Step 300/390 Loss 4.342 Prec@(1,5) (5.4%, 17.5%)
07/22 02:47:59 PM | Valid: [ 3/50] Step 350/390 Loss 4.340 Prec@(1,5) (5.4%, 17.6%)
07/22 02:48:13 PM | Valid: [ 3/50] Step 390/390 Loss 4.339 Prec@(1,5) (5.5%, 17.5%)
07/22 02:48:13 PM | Valid: [ 3/50] Final Prec@1 5.5240%
07/22 02:48:13 PM | genotype = Genotype(normal=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('sep_conv_5x5', 0), ('skip_connect', 2)], [('dil_conv_3x3', 1), ('sep_conv_3x3', 2)], [('avg_pool_3x3', 0), ('skip_connect', 3)]], normal_concat=range(2, 6), reduce=[[('avg_pool_3x3', 0), ('dil_conv_3x3', 1)], [('dil_conv_3x3', 1), ('avg_pool_3x3', 0)], [('dil_conv_5x5', 2), ('dil_conv_5x5', 3)], [('max_pool_3x3', 0), ('avg_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1213, 0.1302, 0.1293, 0.1193, 0.1258, 0.1294, 0.1213, 0.1234],
        [0.1279, 0.1338, 0.1337, 0.1156, 0.1203, 0.1285, 0.1231, 0.1171]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1180, 0.1274, 0.1252, 0.1280, 0.1298, 0.1252, 0.1257, 0.1207],
        [0.1201, 0.1278, 0.1288, 0.1269, 0.1250, 0.1223, 0.1257, 0.1234],
        [0.1214, 0.1267, 0.1290, 0.1246, 0.1240, 0.1217, 0.1267, 0.1259]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1284, 0.1280, 0.1283, 0.1149, 0.1215, 0.1277, 0.1277, 0.1234],
        [0.1258, 0.1260, 0.1257, 0.1179, 0.1239, 0.1298, 0.1279, 0.1230],
        [0.1264, 0.1258, 0.1278, 0.1287, 0.1222, 0.1206, 0.1210, 0.1275],
        [0.1281, 0.1273, 0.1276, 0.1238, 0.1257, 0.1253, 0.1235, 0.1186]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1295, 0.1366, 0.1360, 0.1196, 0.1183, 0.1164, 0.1228, 0.1209],
        [0.1259, 0.1301, 0.1307, 0.1209, 0.1261, 0.1217, 0.1190, 0.1255],
        [0.1241, 0.1265, 0.1316, 0.1253, 0.1234, 0.1187, 0.1274, 0.1231],
        [0.1270, 0.1301, 0.1340, 0.1263, 0.1199, 0.1206, 0.1203, 0.1218],
        [0.1265, 0.1282, 0.1302, 0.1128, 0.1264, 0.1273, 0.1245, 0.1241]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1328, 0.1343, 0.1178, 0.1267, 0.1328, 0.1155, 0.1265, 0.1136],
        [0.1173, 0.1178, 0.1243, 0.1289, 0.1262, 0.1303, 0.1282, 0.1269]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1351, 0.1352, 0.1144, 0.1152, 0.1240, 0.1245, 0.1269, 0.1247],
        [0.1133, 0.1143, 0.1309, 0.1276, 0.1237, 0.1347, 0.1194, 0.1361],
        [0.1211, 0.1212, 0.1225, 0.1218, 0.1258, 0.1320, 0.1299, 0.1258]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1257, 0.1262, 0.1179, 0.1253, 0.1295, 0.1211, 0.1210, 0.1334],
        [0.1214, 0.1217, 0.1225, 0.1292, 0.1319, 0.1206, 0.1242, 0.1284],
        [0.1176, 0.1197, 0.1188, 0.1291, 0.1239, 0.1303, 0.1354, 0.1251],
        [0.1195, 0.1197, 0.1226, 0.1245, 0.1274, 0.1209, 0.1330, 0.1323]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1415, 0.1409, 0.1171, 0.1191, 0.1147, 0.1194, 0.1260, 0.1213],
        [0.1206, 0.1196, 0.1289, 0.1240, 0.1209, 0.1294, 0.1260, 0.1307],
        [0.1345, 0.1395, 0.1342, 0.1155, 0.1242, 0.1154, 0.1198, 0.1169],
        [0.1300, 0.1317, 0.1318, 0.1222, 0.1166, 0.1261, 0.1235, 0.1182],
        [0.1316, 0.1333, 0.1320, 0.1152, 0.1271, 0.1227, 0.1212, 0.1169]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/22 02:48:14 PM | size of train loader is 391
07/22 02:48:19 PM | Train: [ 4/50] Step 000/390 Loss 0.011 Prec@(1,5) (1.6%, 14.1%)
07/22 02:51:20 PM | Train: [ 4/50] Step 050/390 Loss 0.011 Prec@(1,5) (5.4%, 17.7%)
07/22 02:54:17 PM | Train: [ 4/50] Step 100/390 Loss 0.011 Prec@(1,5) (5.5%, 17.8%)
07/22 02:57:07 PM | Train: [ 4/50] Step 150/390 Loss 0.011 Prec@(1,5) (5.7%, 18.2%)
07/22 03:00:05 PM | Train: [ 4/50] Step 200/390 Loss 0.011 Prec@(1,5) (5.7%, 18.2%)
07/22 03:02:59 PM | Train: [ 4/50] Step 250/390 Loss 0.011 Prec@(1,5) (5.6%, 18.2%)
07/22 03:05:52 PM | Train: [ 4/50] Step 300/390 Loss 0.011 Prec@(1,5) (5.6%, 18.4%)
07/22 03:08:56 PM | Train: [ 4/50] Step 350/390 Loss 0.011 Prec@(1,5) (5.6%, 18.5%)
07/22 03:11:10 PM | Train: [ 4/50] Step 390/390 Loss 0.011 Prec@(1,5) (5.6%, 18.6%)
07/22 03:11:10 PM | Train: [ 4/50] Final Prec@1 5.6400%
07/22 03:11:12 PM | Valid: [ 4/50] Step 000/390 Loss 4.188 Prec@(1,5) (6.2%, 25.0%)
07/22 03:11:32 PM | Valid: [ 4/50] Step 050/390 Loss 4.260 Prec@(1,5) (6.6%, 20.0%)
07/22 03:11:52 PM | Valid: [ 4/50] Step 100/390 Loss 4.271 Prec@(1,5) (6.5%, 20.3%)
07/22 03:12:13 PM | Valid: [ 4/50] Step 150/390 Loss 4.274 Prec@(1,5) (6.4%, 20.1%)
07/22 03:12:34 PM | Valid: [ 4/50] Step 200/390 Loss 4.274 Prec@(1,5) (6.5%, 20.2%)
07/22 03:12:55 PM | Valid: [ 4/50] Step 250/390 Loss 4.274 Prec@(1,5) (6.5%, 20.2%)
07/22 03:13:16 PM | Valid: [ 4/50] Step 300/390 Loss 4.275 Prec@(1,5) (6.4%, 20.2%)
07/22 03:13:37 PM | Valid: [ 4/50] Step 350/390 Loss 4.273 Prec@(1,5) (6.5%, 20.3%)
07/22 03:13:53 PM | Valid: [ 4/50] Step 390/390 Loss 4.276 Prec@(1,5) (6.4%, 20.2%)
07/22 03:13:53 PM | Valid: [ 4/50] Final Prec@1 6.4120%
07/22 03:13:53 PM | genotype = Genotype(normal=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('max_pool_3x3', 3), ('max_pool_3x3', 0)], [('skip_connect', 3), ('avg_pool_3x3', 0)]], normal_concat=range(2, 6), reduce=[[('avg_pool_3x3', 0), ('dil_conv_3x3', 1)], [('max_pool_3x3', 0), ('dil_conv_3x3', 1)], [('dil_conv_5x5', 2), ('dil_conv_5x5', 3)], [('max_pool_3x3', 0), ('avg_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1223, 0.1333, 0.1312, 0.1175, 0.1245, 0.1281, 0.1203, 0.1227],
        [0.1326, 0.1396, 0.1386, 0.1121, 0.1188, 0.1269, 0.1194, 0.1120]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1184, 0.1308, 0.1272, 0.1266, 0.1309, 0.1236, 0.1253, 0.1173],
        [0.1222, 0.1328, 0.1335, 0.1265, 0.1233, 0.1198, 0.1234, 0.1186],
        [0.1250, 0.1323, 0.1334, 0.1239, 0.1211, 0.1186, 0.1240, 0.1217]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1321, 0.1315, 0.1311, 0.1115, 0.1212, 0.1249, 0.1267, 0.1209],
        [0.1312, 0.1317, 0.1310, 0.1126, 0.1213, 0.1287, 0.1249, 0.1186],
        [0.1318, 0.1315, 0.1315, 0.1285, 0.1191, 0.1174, 0.1181, 0.1223],
        [0.1351, 0.1347, 0.1347, 0.1208, 0.1224, 0.1212, 0.1195, 0.1117]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1322, 0.1412, 0.1402, 0.1188, 0.1146, 0.1133, 0.1212, 0.1185],
        [0.1308, 0.1365, 0.1370, 0.1187, 0.1235, 0.1183, 0.1157, 0.1195],
        [0.1290, 0.1320, 0.1351, 0.1236, 0.1210, 0.1170, 0.1258, 0.1164],
        [0.1337, 0.1376, 0.1414, 0.1243, 0.1158, 0.1164, 0.1158, 0.1150],
        [0.1330, 0.1348, 0.1375, 0.1081, 0.1220, 0.1250, 0.1216, 0.1179]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1347, 0.1360, 0.1174, 0.1275, 0.1349, 0.1128, 0.1270, 0.1097],
        [0.1171, 0.1181, 0.1199, 0.1273, 0.1266, 0.1318, 0.1295, 0.1296]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1376, 0.1374, 0.1124, 0.1143, 0.1220, 0.1242, 0.1272, 0.1250],
        [0.1126, 0.1140, 0.1315, 0.1273, 0.1220, 0.1363, 0.1184, 0.1379],
        [0.1210, 0.1221, 0.1235, 0.1201, 0.1268, 0.1312, 0.1290, 0.1262]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1286, 0.1275, 0.1164, 0.1255, 0.1287, 0.1195, 0.1182, 0.1357],
        [0.1204, 0.1210, 0.1237, 0.1297, 0.1337, 0.1194, 0.1237, 0.1283],
        [0.1164, 0.1180, 0.1164, 0.1304, 0.1240, 0.1320, 0.1381, 0.1246],
        [0.1203, 0.1197, 0.1224, 0.1232, 0.1277, 0.1189, 0.1336, 0.1342]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1464, 0.1450, 0.1154, 0.1180, 0.1128, 0.1168, 0.1249, 0.1207],
        [0.1197, 0.1188, 0.1302, 0.1225, 0.1192, 0.1313, 0.1274, 0.1310],
        [0.1373, 0.1434, 0.1352, 0.1123, 0.1244, 0.1129, 0.1192, 0.1152],
        [0.1317, 0.1336, 0.1324, 0.1214, 0.1155, 0.1256, 0.1223, 0.1174],
        [0.1326, 0.1341, 0.1313, 0.1153, 0.1283, 0.1221, 0.1200, 0.1162]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/22 03:13:55 PM | size of train loader is 391
07/22 03:14:00 PM | Train: [ 5/50] Step 000/390 Loss 0.011 Prec@(1,5) (3.1%, 17.2%)
07/22 03:17:05 PM | Train: [ 5/50] Step 050/390 Loss 0.011 Prec@(1,5) (7.0%, 20.5%)
07/22 03:20:23 PM | Train: [ 5/50] Step 100/390 Loss 0.011 Prec@(1,5) (6.9%, 20.7%)
07/22 03:23:49 PM | Train: [ 5/50] Step 150/390 Loss 0.011 Prec@(1,5) (6.8%, 20.9%)
07/22 03:27:13 PM | Train: [ 5/50] Step 200/390 Loss 0.011 Prec@(1,5) (6.8%, 20.7%)
07/22 03:30:38 PM | Train: [ 5/50] Step 250/390 Loss 0.011 Prec@(1,5) (6.8%, 20.9%)
07/22 03:33:53 PM | Train: [ 5/50] Step 300/390 Loss 0.011 Prec@(1,5) (6.7%, 21.1%)
07/22 03:36:57 PM | Train: [ 5/50] Step 350/390 Loss 0.011 Prec@(1,5) (6.8%, 21.2%)
07/22 03:39:18 PM | Train: [ 5/50] Step 390/390 Loss 0.011 Prec@(1,5) (6.8%, 21.3%)
07/22 03:39:18 PM | Train: [ 5/50] Final Prec@1 6.8360%
07/22 03:39:20 PM | Valid: [ 5/50] Step 000/390 Loss 4.217 Prec@(1,5) (6.2%, 21.9%)
07/22 03:39:41 PM | Valid: [ 5/50] Step 050/390 Loss 4.219 Prec@(1,5) (6.8%, 22.5%)
07/22 03:40:02 PM | Valid: [ 5/50] Step 100/390 Loss 4.217 Prec@(1,5) (6.9%, 22.2%)
07/22 03:40:23 PM | Valid: [ 5/50] Step 150/390 Loss 4.216 Prec@(1,5) (7.1%, 22.5%)
07/22 03:40:43 PM | Valid: [ 5/50] Step 200/390 Loss 4.211 Prec@(1,5) (7.2%, 22.5%)
07/22 03:41:04 PM | Valid: [ 5/50] Step 250/390 Loss 4.210 Prec@(1,5) (7.3%, 22.5%)
07/22 03:41:25 PM | Valid: [ 5/50] Step 300/390 Loss 4.212 Prec@(1,5) (7.3%, 22.5%)
07/22 03:41:46 PM | Valid: [ 5/50] Step 350/390 Loss 4.213 Prec@(1,5) (7.3%, 22.3%)
07/22 03:42:01 PM | Valid: [ 5/50] Step 390/390 Loss 4.212 Prec@(1,5) (7.3%, 22.5%)
07/22 03:42:01 PM | Valid: [ 5/50] Final Prec@1 7.2560%
07/22 03:42:01 PM | genotype = Genotype(normal=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('max_pool_3x3', 3), ('max_pool_3x3', 2)], [('skip_connect', 3), ('avg_pool_3x3', 0)]], normal_concat=range(2, 6), reduce=[[('avg_pool_3x3', 0), ('dil_conv_3x3', 1)], [('max_pool_3x3', 0), ('dil_conv_3x3', 1)], [('dil_conv_5x5', 2), ('sep_conv_5x5', 1)], [('max_pool_3x3', 0), ('avg_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1258, 0.1368, 0.1337, 0.1161, 0.1222, 0.1262, 0.1187, 0.1205],
        [0.1372, 0.1442, 0.1425, 0.1087, 0.1179, 0.1255, 0.1161, 0.1079]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1214, 0.1351, 0.1298, 0.1239, 0.1320, 0.1213, 0.1241, 0.1125],
        [0.1257, 0.1378, 0.1382, 0.1249, 0.1216, 0.1167, 0.1214, 0.1138],
        [0.1302, 0.1387, 0.1382, 0.1232, 0.1166, 0.1161, 0.1206, 0.1165]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1370, 0.1352, 0.1337, 0.1090, 0.1191, 0.1224, 0.1271, 0.1167],
        [0.1362, 0.1364, 0.1356, 0.1086, 0.1199, 0.1269, 0.1219, 0.1145],
        [0.1375, 0.1371, 0.1349, 0.1293, 0.1166, 0.1137, 0.1143, 0.1166],
        [0.1429, 0.1423, 0.1423, 0.1167, 0.1189, 0.1165, 0.1152, 0.1051]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1366, 0.1469, 0.1457, 0.1164, 0.1108, 0.1097, 0.1193, 0.1147],
        [0.1360, 0.1424, 0.1433, 0.1163, 0.1200, 0.1162, 0.1113, 0.1145],
        [0.1345, 0.1379, 0.1391, 0.1214, 0.1184, 0.1154, 0.1236, 0.1098],
        [0.1406, 0.1448, 0.1486, 0.1223, 0.1123, 0.1126, 0.1110, 0.1079],
        [0.1392, 0.1411, 0.1454, 0.1041, 0.1186, 0.1220, 0.1188, 0.1108]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1365, 0.1367, 0.1176, 0.1271, 0.1366, 0.1099, 0.1271, 0.1083],
        [0.1185, 0.1195, 0.1167, 0.1269, 0.1252, 0.1327, 0.1307, 0.1298]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1397, 0.1389, 0.1110, 0.1126, 0.1204, 0.1234, 0.1272, 0.1269],
        [0.1137, 0.1146, 0.1322, 0.1251, 0.1195, 0.1380, 0.1183, 0.1385],
        [0.1222, 0.1227, 0.1235, 0.1195, 0.1257, 0.1317, 0.1295, 0.1253]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1314, 0.1286, 0.1157, 0.1242, 0.1268, 0.1195, 0.1162, 0.1376],
        [0.1189, 0.1194, 0.1256, 0.1321, 0.1361, 0.1179, 0.1227, 0.1272],
        [0.1158, 0.1166, 0.1148, 0.1327, 0.1238, 0.1335, 0.1387, 0.1242],
        [0.1215, 0.1195, 0.1229, 0.1219, 0.1281, 0.1161, 0.1335, 0.1365]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1505, 0.1482, 0.1143, 0.1175, 0.1120, 0.1142, 0.1235, 0.1199],
        [0.1191, 0.1182, 0.1301, 0.1216, 0.1182, 0.1333, 0.1283, 0.1312],
        [0.1400, 0.1466, 0.1362, 0.1095, 0.1241, 0.1111, 0.1186, 0.1139],
        [0.1337, 0.1347, 0.1327, 0.1214, 0.1154, 0.1247, 0.1203, 0.1171],
        [0.1339, 0.1346, 0.1309, 0.1163, 0.1298, 0.1213, 0.1179, 0.1153]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/22 03:42:02 PM | size of train loader is 391
07/22 03:42:08 PM | Train: [ 6/50] Step 000/390 Loss 0.010 Prec@(1,5) (4.7%, 28.1%)
07/22 03:45:32 PM | Train: [ 6/50] Step 050/390 Loss 0.011 Prec@(1,5) (7.4%, 23.1%)
07/22 03:48:47 PM | Train: [ 6/50] Step 100/390 Loss 0.011 Prec@(1,5) (7.4%, 22.9%)
07/22 03:51:59 PM | Train: [ 6/50] Step 150/390 Loss 0.011 Prec@(1,5) (7.6%, 23.1%)
07/22 03:55:17 PM | Train: [ 6/50] Step 200/390 Loss 0.011 Prec@(1,5) (7.7%, 23.4%)
07/22 03:58:35 PM | Train: [ 6/50] Step 250/390 Loss 0.011 Prec@(1,5) (7.8%, 23.5%)
07/22 04:01:58 PM | Train: [ 6/50] Step 300/390 Loss 0.011 Prec@(1,5) (7.7%, 23.5%)
07/22 04:05:26 PM | Train: [ 6/50] Step 350/390 Loss 0.011 Prec@(1,5) (7.6%, 23.5%)
07/22 04:08:06 PM | Train: [ 6/50] Step 390/390 Loss 0.011 Prec@(1,5) (7.7%, 23.6%)
07/22 04:08:06 PM | Train: [ 6/50] Final Prec@1 7.6760%
07/22 04:08:07 PM | Valid: [ 6/50] Step 000/390 Loss 4.273 Prec@(1,5) (9.4%, 23.4%)
07/22 04:08:28 PM | Valid: [ 6/50] Step 050/390 Loss 4.135 Prec@(1,5) (8.3%, 25.3%)
07/22 04:08:50 PM | Valid: [ 6/50] Step 100/390 Loss 4.130 Prec@(1,5) (8.3%, 25.6%)
07/22 04:09:10 PM | Valid: [ 6/50] Step 150/390 Loss 4.137 Prec@(1,5) (8.3%, 24.9%)
07/22 04:09:30 PM | Valid: [ 6/50] Step 200/390 Loss 4.141 Prec@(1,5) (8.3%, 24.8%)
07/22 04:09:50 PM | Valid: [ 6/50] Step 250/390 Loss 4.142 Prec@(1,5) (8.4%, 25.0%)
07/22 04:10:11 PM | Valid: [ 6/50] Step 300/390 Loss 4.141 Prec@(1,5) (8.4%, 25.1%)
07/22 04:10:31 PM | Valid: [ 6/50] Step 350/390 Loss 4.140 Prec@(1,5) (8.3%, 25.1%)
07/22 04:10:45 PM | Valid: [ 6/50] Step 390/390 Loss 4.141 Prec@(1,5) (8.3%, 25.0%)
07/22 04:10:45 PM | Valid: [ 6/50] Final Prec@1 8.3040%
07/22 04:10:45 PM | genotype = Genotype(normal=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('max_pool_3x3', 3), ('max_pool_3x3', 2)], [('skip_connect', 3), ('skip_connect', 4)]], normal_concat=range(2, 6), reduce=[[('sep_conv_5x5', 0), ('dil_conv_5x5', 1)], [('max_pool_3x3', 0), ('dil_conv_3x3', 1)], [('sep_conv_5x5', 1), ('dil_conv_5x5', 2)], [('max_pool_3x3', 0), ('avg_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1296, 0.1419, 0.1374, 0.1145, 0.1195, 0.1225, 0.1172, 0.1175],
        [0.1423, 0.1500, 0.1474, 0.1061, 0.1157, 0.1232, 0.1116, 0.1036]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1239, 0.1401, 0.1331, 0.1220, 0.1328, 0.1180, 0.1229, 0.1073],
        [0.1281, 0.1429, 0.1433, 0.1222, 0.1208, 0.1144, 0.1188, 0.1094],
        [0.1351, 0.1451, 0.1428, 0.1215, 0.1121, 0.1138, 0.1184, 0.1113]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1423, 0.1405, 0.1377, 0.1060, 0.1170, 0.1188, 0.1257, 0.1120],
        [0.1408, 0.1411, 0.1399, 0.1051, 0.1179, 0.1247, 0.1204, 0.1100],
        [0.1441, 0.1440, 0.1397, 0.1275, 0.1139, 0.1099, 0.1103, 0.1106],
        [0.1512, 0.1507, 0.1509, 0.1126, 0.1150, 0.1113, 0.1100, 0.0983]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1418, 0.1538, 0.1516, 0.1134, 0.1065, 0.1060, 0.1172, 0.1097],
        [0.1417, 0.1490, 0.1498, 0.1144, 0.1159, 0.1139, 0.1062, 0.1092],
        [0.1403, 0.1438, 0.1425, 0.1205, 0.1147, 0.1142, 0.1209, 0.1031],
        [0.1472, 0.1523, 0.1563, 0.1193, 0.1091, 0.1085, 0.1063, 0.1009],
        [0.1460, 0.1481, 0.1549, 0.1004, 0.1146, 0.1181, 0.1138, 0.1041]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1388, 0.1386, 0.1178, 0.1260, 0.1390, 0.1067, 0.1264, 0.1067],
        [0.1195, 0.1205, 0.1127, 0.1270, 0.1251, 0.1319, 0.1330, 0.1303]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1420, 0.1410, 0.1103, 0.1118, 0.1182, 0.1223, 0.1271, 0.1273],
        [0.1145, 0.1156, 0.1334, 0.1234, 0.1170, 0.1384, 0.1174, 0.1402],
        [0.1222, 0.1243, 0.1242, 0.1183, 0.1246, 0.1317, 0.1302, 0.1245]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1343, 0.1302, 0.1142, 0.1241, 0.1251, 0.1180, 0.1152, 0.1390],
        [0.1181, 0.1185, 0.1279, 0.1340, 0.1376, 0.1154, 0.1227, 0.1258],
        [0.1156, 0.1161, 0.1137, 0.1341, 0.1245, 0.1345, 0.1388, 0.1227],
        [0.1232, 0.1203, 0.1244, 0.1195, 0.1272, 0.1134, 0.1336, 0.1383]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1540, 0.1512, 0.1145, 0.1174, 0.1106, 0.1113, 0.1212, 0.1199],
        [0.1191, 0.1184, 0.1299, 0.1207, 0.1168, 0.1343, 0.1294, 0.1315],
        [0.1413, 0.1502, 0.1377, 0.1057, 0.1238, 0.1106, 0.1173, 0.1133],
        [0.1342, 0.1351, 0.1326, 0.1211, 0.1154, 0.1241, 0.1203, 0.1171],
        [0.1339, 0.1338, 0.1296, 0.1172, 0.1314, 0.1209, 0.1178, 0.1154]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/22 04:10:46 PM | size of train loader is 391
07/22 04:10:52 PM | Train: [ 7/50] Step 000/390 Loss 0.011 Prec@(1,5) (1.6%, 17.2%)
07/22 04:13:51 PM | Train: [ 7/50] Step 050/390 Loss 0.011 Prec@(1,5) (9.0%, 25.5%)
07/22 04:16:46 PM | Train: [ 7/50] Step 100/390 Loss 0.011 Prec@(1,5) (8.8%, 25.9%)
07/22 04:19:36 PM | Train: [ 7/50] Step 150/390 Loss 0.011 Prec@(1,5) (8.7%, 25.7%)
07/22 04:22:28 PM | Train: [ 7/50] Step 200/390 Loss 0.011 Prec@(1,5) (8.8%, 25.8%)
07/22 04:25:24 PM | Train: [ 7/50] Step 250/390 Loss 0.011 Prec@(1,5) (8.7%, 25.9%)
07/22 04:28:46 PM | Train: [ 7/50] Step 300/390 Loss 0.011 Prec@(1,5) (8.7%, 26.0%)
07/22 04:31:59 PM | Train: [ 7/50] Step 350/390 Loss 0.011 Prec@(1,5) (8.6%, 26.2%)
07/22 04:34:31 PM | Train: [ 7/50] Step 390/390 Loss 0.011 Prec@(1,5) (8.7%, 26.3%)
07/22 04:34:31 PM | Train: [ 7/50] Final Prec@1 8.7080%
07/22 04:34:33 PM | Valid: [ 7/50] Step 000/390 Loss 4.018 Prec@(1,5) (10.9%, 31.2%)
07/22 04:34:53 PM | Valid: [ 7/50] Step 050/390 Loss 4.044 Prec@(1,5) (10.2%, 28.0%)
07/22 04:35:14 PM | Valid: [ 7/50] Step 100/390 Loss 4.054 Prec@(1,5) (9.4%, 27.6%)
07/22 04:35:35 PM | Valid: [ 7/50] Step 150/390 Loss 4.065 Prec@(1,5) (9.2%, 27.1%)
07/22 04:35:56 PM | Valid: [ 7/50] Step 200/390 Loss 4.066 Prec@(1,5) (9.1%, 27.1%)
07/22 04:36:18 PM | Valid: [ 7/50] Step 250/390 Loss 4.066 Prec@(1,5) (9.1%, 27.3%)
07/22 04:36:40 PM | Valid: [ 7/50] Step 300/390 Loss 4.071 Prec@(1,5) (9.0%, 27.1%)
07/22 04:37:00 PM | Valid: [ 7/50] Step 350/390 Loss 4.073 Prec@(1,5) (9.0%, 27.0%)
07/22 04:37:16 PM | Valid: [ 7/50] Step 390/390 Loss 4.075 Prec@(1,5) (8.9%, 27.0%)
07/22 04:37:16 PM | Valid: [ 7/50] Final Prec@1 8.9280%
07/22 04:37:16 PM | genotype = Genotype(normal=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('max_pool_3x3', 3), ('max_pool_3x3', 2)], [('skip_connect', 3), ('skip_connect', 4)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_5x5', 1)], [('max_pool_3x3', 0), ('dil_conv_3x3', 1)], [('sep_conv_5x5', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('avg_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1358, 0.1462, 0.1407, 0.1120, 0.1175, 0.1194, 0.1149, 0.1134],
        [0.1478, 0.1531, 0.1497, 0.1029, 0.1152, 0.1209, 0.1096, 0.1009]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1288, 0.1450, 0.1364, 0.1199, 0.1329, 0.1142, 0.1211, 0.1018],
        [0.1316, 0.1471, 0.1474, 0.1206, 0.1199, 0.1113, 0.1162, 0.1059],
        [0.1413, 0.1508, 0.1462, 0.1207, 0.1082, 0.1114, 0.1161, 0.1052]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1491, 0.1453, 0.1416, 0.1033, 0.1142, 0.1146, 0.1247, 0.1072],
        [0.1460, 0.1444, 0.1433, 0.1020, 0.1176, 0.1222, 0.1184, 0.1061],
        [0.1517, 0.1503, 0.1438, 0.1251, 0.1117, 0.1067, 0.1066, 0.1042],
        [0.1599, 0.1583, 0.1591, 0.1084, 0.1109, 0.1062, 0.1052, 0.0920]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1482, 0.1613, 0.1582, 0.1102, 0.1026, 0.1019, 0.1140, 0.1038],
        [0.1472, 0.1536, 0.1550, 0.1129, 0.1123, 0.1117, 0.1026, 0.1047],
        [0.1475, 0.1503, 0.1464, 0.1176, 0.1111, 0.1125, 0.1177, 0.0968],
        [0.1551, 0.1599, 0.1647, 0.1157, 0.1040, 0.1050, 0.1020, 0.0937],
        [0.1535, 0.1550, 0.1657, 0.0954, 0.1101, 0.1143, 0.1090, 0.0970]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1407, 0.1390, 0.1183, 0.1265, 0.1405, 0.1042, 0.1246, 0.1062],
        [0.1210, 0.1211, 0.1097, 0.1265, 0.1256, 0.1314, 0.1346, 0.1301]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1442, 0.1420, 0.1109, 0.1115, 0.1174, 0.1204, 0.1265, 0.1272],
        [0.1151, 0.1154, 0.1335, 0.1215, 0.1157, 0.1407, 0.1166, 0.1414],
        [0.1232, 0.1248, 0.1241, 0.1174, 0.1239, 0.1323, 0.1298, 0.1244]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1372, 0.1309, 0.1147, 0.1239, 0.1231, 0.1170, 0.1135, 0.1397],
        [0.1174, 0.1174, 0.1288, 0.1347, 0.1401, 0.1140, 0.1227, 0.1250],
        [0.1151, 0.1145, 0.1114, 0.1364, 0.1257, 0.1352, 0.1400, 0.1218],
        [0.1256, 0.1210, 0.1253, 0.1180, 0.1257, 0.1113, 0.1335, 0.1396]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1576, 0.1537, 0.1146, 0.1166, 0.1088, 0.1105, 0.1192, 0.1191],
        [0.1187, 0.1178, 0.1302, 0.1194, 0.1167, 0.1353, 0.1305, 0.1314],
        [0.1435, 0.1521, 0.1384, 0.1029, 0.1225, 0.1103, 0.1167, 0.1136],
        [0.1363, 0.1361, 0.1331, 0.1202, 0.1144, 0.1238, 0.1191, 0.1170],
        [0.1351, 0.1334, 0.1285, 0.1176, 0.1323, 0.1200, 0.1174, 0.1157]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/22 04:37:18 PM | size of train loader is 391
07/22 04:37:23 PM | Train: [ 8/50] Step 000/390 Loss 0.011 Prec@(1,5) (6.2%, 29.7%)
07/22 04:40:38 PM | Train: [ 8/50] Step 050/390 Loss 0.010 Prec@(1,5) (9.9%, 27.3%)
07/22 04:44:02 PM | Train: [ 8/50] Step 100/390 Loss 0.010 Prec@(1,5) (9.7%, 27.1%)
07/22 04:47:30 PM | Train: [ 8/50] Step 150/390 Loss 0.010 Prec@(1,5) (9.8%, 27.1%)
07/22 04:50:57 PM | Train: [ 8/50] Step 200/390 Loss 0.010 Prec@(1,5) (9.6%, 27.1%)
07/22 04:54:19 PM | Train: [ 8/50] Step 250/390 Loss 0.010 Prec@(1,5) (9.7%, 27.4%)
07/22 04:57:41 PM | Train: [ 8/50] Step 300/390 Loss 0.010 Prec@(1,5) (9.6%, 27.5%)
07/22 05:01:07 PM | Train: [ 8/50] Step 350/390 Loss 0.010 Prec@(1,5) (9.8%, 27.8%)
07/22 05:03:51 PM | Train: [ 8/50] Step 390/390 Loss 0.010 Prec@(1,5) (9.7%, 27.8%)
07/22 05:03:52 PM | Train: [ 8/50] Final Prec@1 9.7320%
07/22 05:03:53 PM | Valid: [ 8/50] Step 000/390 Loss 4.098 Prec@(1,5) (7.8%, 17.2%)
07/22 05:04:14 PM | Valid: [ 8/50] Step 050/390 Loss 4.020 Prec@(1,5) (10.0%, 28.1%)
07/22 05:04:34 PM | Valid: [ 8/50] Step 100/390 Loss 4.008 Prec@(1,5) (10.5%, 28.8%)
07/22 05:04:54 PM | Valid: [ 8/50] Step 150/390 Loss 4.012 Prec@(1,5) (10.4%, 29.0%)
07/22 05:05:14 PM | Valid: [ 8/50] Step 200/390 Loss 4.015 Prec@(1,5) (10.2%, 28.8%)
07/22 05:05:34 PM | Valid: [ 8/50] Step 250/390 Loss 4.011 Prec@(1,5) (10.1%, 29.0%)
07/22 05:05:54 PM | Valid: [ 8/50] Step 300/390 Loss 4.005 Prec@(1,5) (10.2%, 29.4%)
07/22 05:06:14 PM | Valid: [ 8/50] Step 350/390 Loss 4.007 Prec@(1,5) (10.2%, 29.1%)
07/22 05:06:28 PM | Valid: [ 8/50] Step 390/390 Loss 4.009 Prec@(1,5) (10.2%, 29.1%)
07/22 05:06:28 PM | Valid: [ 8/50] Final Prec@1 10.1680%
07/22 05:06:28 PM | genotype = Genotype(normal=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('skip_connect', 4), ('skip_connect', 3)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_5x5', 1)], [('max_pool_3x3', 0), ('dil_conv_3x3', 1)], [('sep_conv_5x5', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('avg_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1422, 0.1510, 0.1441, 0.1097, 0.1156, 0.1160, 0.1123, 0.1091],
        [0.1533, 0.1560, 0.1526, 0.0992, 0.1146, 0.1184, 0.1076, 0.0982]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1343, 0.1504, 0.1394, 0.1179, 0.1319, 0.1107, 0.1192, 0.0963],
        [0.1357, 0.1510, 0.1514, 0.1180, 0.1185, 0.1085, 0.1143, 0.1025],
        [0.1484, 0.1570, 0.1497, 0.1193, 0.1048, 0.1086, 0.1126, 0.0996]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1561, 0.1514, 0.1464, 0.0999, 0.1112, 0.1100, 0.1231, 0.1019],
        [0.1505, 0.1477, 0.1463, 0.0998, 0.1170, 0.1195, 0.1169, 0.1024],
        [0.1587, 0.1562, 0.1475, 0.1222, 0.1103, 0.1032, 0.1035, 0.0983],
        [0.1673, 0.1651, 0.1674, 0.1047, 0.1068, 0.1016, 0.1011, 0.0860]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1542, 0.1674, 0.1632, 0.1077, 0.0990, 0.0985, 0.1118, 0.0982],
        [0.1525, 0.1571, 0.1596, 0.1108, 0.1091, 0.1098, 0.1000, 0.1012],
        [0.1534, 0.1551, 0.1498, 0.1152, 0.1080, 0.1109, 0.1155, 0.0920],
        [0.1623, 0.1656, 0.1723, 0.1115, 0.1012, 0.1016, 0.0976, 0.0879],
        [0.1598, 0.1599, 0.1762, 0.0911, 0.1061, 0.1111, 0.1048, 0.0910]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1420, 0.1382, 0.1192, 0.1274, 0.1420, 0.1022, 0.1230, 0.1060],
        [0.1220, 0.1210, 0.1079, 0.1275, 0.1252, 0.1299, 0.1368, 0.1296]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1456, 0.1414, 0.1116, 0.1114, 0.1170, 0.1189, 0.1258, 0.1282],
        [0.1156, 0.1149, 0.1347, 0.1195, 0.1148, 0.1417, 0.1159, 0.1430],
        [0.1248, 0.1248, 0.1228, 0.1171, 0.1232, 0.1343, 0.1301, 0.1229]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1396, 0.1311, 0.1151, 0.1234, 0.1221, 0.1152, 0.1129, 0.1406],
        [0.1168, 0.1162, 0.1293, 0.1354, 0.1425, 0.1127, 0.1227, 0.1243],
        [0.1152, 0.1131, 0.1100, 0.1374, 0.1260, 0.1352, 0.1414, 0.1216],
        [0.1279, 0.1215, 0.1264, 0.1148, 0.1248, 0.1096, 0.1339, 0.1411]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1608, 0.1553, 0.1143, 0.1161, 0.1073, 0.1096, 0.1183, 0.1183],
        [0.1189, 0.1177, 0.1312, 0.1186, 0.1165, 0.1343, 0.1311, 0.1316],
        [0.1462, 0.1535, 0.1380, 0.0999, 0.1221, 0.1115, 0.1157, 0.1132],
        [0.1391, 0.1371, 0.1331, 0.1204, 0.1126, 0.1229, 0.1188, 0.1160],
        [0.1365, 0.1328, 0.1276, 0.1177, 0.1338, 0.1203, 0.1155, 0.1159]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/22 05:06:30 PM | size of train loader is 391
07/22 05:06:35 PM | Train: [ 9/50] Step 000/390 Loss 0.011 Prec@(1,5) (10.9%, 25.0%)
07/22 05:09:31 PM | Train: [ 9/50] Step 050/390 Loss 0.010 Prec@(1,5) (9.9%, 29.5%)
07/22 05:12:50 PM | Train: [ 9/50] Step 100/390 Loss 0.010 Prec@(1,5) (9.9%, 29.7%)
07/22 05:16:05 PM | Train: [ 9/50] Step 150/390 Loss 0.010 Prec@(1,5) (10.0%, 29.7%)
07/22 05:19:31 PM | Train: [ 9/50] Step 200/390 Loss 0.010 Prec@(1,5) (10.2%, 29.7%)
07/22 05:22:57 PM | Train: [ 9/50] Step 250/390 Loss 0.010 Prec@(1,5) (10.1%, 29.8%)
07/22 05:26:05 PM | Train: [ 9/50] Step 300/390 Loss 0.010 Prec@(1,5) (10.2%, 29.9%)
07/22 05:29:17 PM | Train: [ 9/50] Step 350/390 Loss 0.010 Prec@(1,5) (10.3%, 29.9%)
07/22 05:31:57 PM | Train: [ 9/50] Step 390/390 Loss 0.010 Prec@(1,5) (10.3%, 29.8%)
07/22 05:31:57 PM | Train: [ 9/50] Final Prec@1 10.2800%
07/22 05:31:59 PM | Valid: [ 9/50] Step 000/390 Loss 4.068 Prec@(1,5) (14.1%, 32.8%)
07/22 05:32:20 PM | Valid: [ 9/50] Step 050/390 Loss 3.929 Prec@(1,5) (11.1%, 30.6%)
07/22 05:32:41 PM | Valid: [ 9/50] Step 100/390 Loss 3.936 Prec@(1,5) (10.8%, 30.5%)
07/22 05:33:01 PM | Valid: [ 9/50] Step 150/390 Loss 3.942 Prec@(1,5) (11.0%, 30.6%)
07/22 05:33:21 PM | Valid: [ 9/50] Step 200/390 Loss 3.949 Prec@(1,5) (10.6%, 30.6%)
07/22 05:33:41 PM | Valid: [ 9/50] Step 250/390 Loss 3.947 Prec@(1,5) (10.9%, 30.9%)
07/22 05:34:01 PM | Valid: [ 9/50] Step 300/390 Loss 3.948 Prec@(1,5) (10.8%, 30.9%)
07/22 05:34:21 PM | Valid: [ 9/50] Step 350/390 Loss 3.947 Prec@(1,5) (10.8%, 31.0%)
07/22 05:34:35 PM | Valid: [ 9/50] Step 390/390 Loss 3.948 Prec@(1,5) (10.9%, 30.9%)
07/22 05:34:36 PM | Valid: [ 9/50] Final Prec@1 10.8520%
07/22 05:34:36 PM | genotype = Genotype(normal=[[('max_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('skip_connect', 4), ('skip_connect', 3)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_5x5', 1)], [('max_pool_3x3', 0), ('dil_conv_3x3', 1)], [('sep_conv_5x5', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('avg_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1500, 0.1546, 0.1468, 0.1066, 0.1148, 0.1121, 0.1100, 0.1051],
        [0.1596, 0.1579, 0.1540, 0.0963, 0.1137, 0.1167, 0.1059, 0.0958]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1407, 0.1551, 0.1427, 0.1160, 0.1306, 0.1078, 0.1165, 0.0907],
        [0.1391, 0.1533, 0.1541, 0.1167, 0.1176, 0.1062, 0.1131, 0.0999],
        [0.1551, 0.1619, 0.1532, 0.1178, 0.1014, 0.1062, 0.1098, 0.0946]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1631, 0.1558, 0.1500, 0.0980, 0.1092, 0.1059, 0.1210, 0.0970],
        [0.1545, 0.1491, 0.1479, 0.0984, 0.1178, 0.1171, 0.1154, 0.0998],
        [0.1645, 0.1599, 0.1495, 0.1204, 0.1099, 0.1011, 0.1014, 0.0934],
        [0.1745, 0.1698, 0.1747, 0.1016, 0.1031, 0.0976, 0.0979, 0.0808]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1602, 0.1732, 0.1683, 0.1041, 0.0952, 0.0955, 0.1101, 0.0934],
        [0.1573, 0.1593, 0.1624, 0.1109, 0.1067, 0.1076, 0.0980, 0.0977],
        [0.1592, 0.1591, 0.1527, 0.1124, 0.1057, 0.1097, 0.1135, 0.0878],
        [0.1689, 0.1708, 0.1804, 0.1077, 0.0980, 0.0977, 0.0934, 0.0831],
        [0.1652, 0.1637, 0.1868, 0.0870, 0.1024, 0.1079, 0.1015, 0.0855]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1426, 0.1370, 0.1213, 0.1281, 0.1422, 0.1010, 0.1220, 0.1058],
        [0.1227, 0.1205, 0.1078, 0.1277, 0.1238, 0.1310, 0.1369, 0.1295]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1471, 0.1412, 0.1115, 0.1115, 0.1160, 0.1179, 0.1257, 0.1290],
        [0.1152, 0.1135, 0.1348, 0.1199, 0.1148, 0.1428, 0.1151, 0.1439],
        [0.1253, 0.1238, 0.1209, 0.1167, 0.1229, 0.1358, 0.1318, 0.1228]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1421, 0.1310, 0.1154, 0.1236, 0.1208, 0.1148, 0.1122, 0.1401],
        [0.1165, 0.1150, 0.1290, 0.1351, 0.1443, 0.1119, 0.1234, 0.1248],
        [0.1151, 0.1117, 0.1079, 0.1377, 0.1272, 0.1370, 0.1433, 0.1200],
        [0.1306, 0.1220, 0.1274, 0.1123, 0.1239, 0.1079, 0.1343, 0.1415]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1627, 0.1554, 0.1152, 0.1166, 0.1058, 0.1086, 0.1185, 0.1171],
        [0.1171, 0.1157, 0.1329, 0.1174, 0.1173, 0.1355, 0.1325, 0.1316],
        [0.1477, 0.1530, 0.1367, 0.0982, 0.1216, 0.1123, 0.1161, 0.1144],
        [0.1414, 0.1370, 0.1320, 0.1204, 0.1120, 0.1225, 0.1191, 0.1156],
        [0.1380, 0.1319, 0.1264, 0.1186, 0.1327, 0.1205, 0.1147, 0.1173]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/22 05:34:37 PM | size of train loader is 391
07/22 05:34:43 PM | Train: [10/50] Step 000/390 Loss 0.010 Prec@(1,5) (20.3%, 39.1%)
07/22 05:37:38 PM | Train: [10/50] Step 050/390 Loss 0.010 Prec@(1,5) (10.3%, 31.3%)
07/22 05:40:30 PM | Train: [10/50] Step 100/390 Loss 0.010 Prec@(1,5) (10.7%, 30.8%)
07/22 05:43:31 PM | Train: [10/50] Step 150/390 Loss 0.010 Prec@(1,5) (10.3%, 30.6%)
07/22 05:46:26 PM | Train: [10/50] Step 200/390 Loss 0.010 Prec@(1,5) (10.6%, 31.1%)
07/22 05:49:14 PM | Train: [10/50] Step 250/390 Loss 0.010 Prec@(1,5) (10.8%, 31.3%)
07/22 05:52:01 PM | Train: [10/50] Step 300/390 Loss 0.010 Prec@(1,5) (10.9%, 31.4%)
07/22 05:54:47 PM | Train: [10/50] Step 350/390 Loss 0.010 Prec@(1,5) (11.0%, 31.4%)
07/22 05:57:00 PM | Train: [10/50] Step 390/390 Loss 0.010 Prec@(1,5) (11.0%, 31.4%)
07/22 05:57:00 PM | Train: [10/50] Final Prec@1 10.9520%
07/22 05:57:01 PM | Valid: [10/50] Step 000/390 Loss 4.014 Prec@(1,5) (3.1%, 23.4%)
07/22 05:57:21 PM | Valid: [10/50] Step 050/390 Loss 3.896 Prec@(1,5) (11.5%, 32.0%)
07/22 05:57:41 PM | Valid: [10/50] Step 100/390 Loss 3.909 Prec@(1,5) (11.5%, 31.7%)
07/22 05:58:02 PM | Valid: [10/50] Step 150/390 Loss 3.900 Prec@(1,5) (11.3%, 32.2%)
07/22 05:58:22 PM | Valid: [10/50] Step 200/390 Loss 3.896 Prec@(1,5) (11.5%, 32.4%)
07/22 05:58:42 PM | Valid: [10/50] Step 250/390 Loss 3.895 Prec@(1,5) (11.3%, 32.3%)
07/22 05:59:02 PM | Valid: [10/50] Step 300/390 Loss 3.898 Prec@(1,5) (11.3%, 32.3%)
07/22 05:59:22 PM | Valid: [10/50] Step 350/390 Loss 3.899 Prec@(1,5) (11.3%, 32.3%)
07/22 05:59:37 PM | Valid: [10/50] Step 390/390 Loss 3.898 Prec@(1,5) (11.4%, 32.3%)
07/22 05:59:37 PM | Valid: [10/50] Final Prec@1 11.3920%
07/22 05:59:37 PM | genotype = Genotype(normal=[[('max_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('max_pool_3x3', 3), ('max_pool_3x3', 2)], [('skip_connect', 4), ('skip_connect', 3)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_5x5', 1)], [('max_pool_3x3', 0), ('dil_conv_3x3', 1)], [('max_pool_3x3', 0), ('sep_conv_5x5', 1)], [('max_pool_3x3', 0), ('avg_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1577, 0.1577, 0.1492, 0.1038, 0.1136, 0.1093, 0.1074, 0.1013],
        [0.1655, 0.1581, 0.1544, 0.0941, 0.1129, 0.1157, 0.1053, 0.0941]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1468, 0.1593, 0.1451, 0.1149, 0.1284, 0.1050, 0.1147, 0.0857],
        [0.1423, 0.1546, 0.1559, 0.1159, 0.1170, 0.1041, 0.1124, 0.0979],
        [0.1614, 0.1657, 0.1558, 0.1163, 0.0979, 0.1055, 0.1071, 0.0903]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1694, 0.1588, 0.1519, 0.0959, 0.1074, 0.1027, 0.1203, 0.0938],
        [0.1590, 0.1502, 0.1495, 0.0974, 0.1177, 0.1144, 0.1136, 0.0982],
        [0.1699, 0.1627, 0.1508, 0.1182, 0.1100, 0.0992, 0.0992, 0.0900],
        [0.1816, 0.1736, 0.1808, 0.0992, 0.0994, 0.0938, 0.0950, 0.0767]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1659, 0.1791, 0.1733, 0.1008, 0.0917, 0.0926, 0.1081, 0.0885],
        [0.1618, 0.1605, 0.1643, 0.1109, 0.1048, 0.1070, 0.0959, 0.0948],
        [0.1653, 0.1633, 0.1555, 0.1089, 0.1031, 0.1081, 0.1120, 0.0837],
        [0.1752, 0.1750, 0.1881, 0.1041, 0.0951, 0.0946, 0.0893, 0.0785],
        [0.1708, 0.1669, 0.1975, 0.0830, 0.0989, 0.1047, 0.0976, 0.0805]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1444, 0.1363, 0.1231, 0.1292, 0.1419, 0.0997, 0.1205, 0.1051],
        [0.1222, 0.1187, 0.1076, 0.1297, 0.1222, 0.1317, 0.1381, 0.1299]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1486, 0.1409, 0.1108, 0.1115, 0.1175, 0.1173, 0.1247, 0.1287],
        [0.1142, 0.1114, 0.1349, 0.1198, 0.1153, 0.1454, 0.1136, 0.1456],
        [0.1268, 0.1230, 0.1198, 0.1163, 0.1219, 0.1364, 0.1326, 0.1232]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1452, 0.1320, 0.1146, 0.1222, 0.1200, 0.1149, 0.1116, 0.1395],
        [0.1151, 0.1128, 0.1282, 0.1358, 0.1466, 0.1123, 0.1243, 0.1248],
        [0.1156, 0.1106, 0.1072, 0.1383, 0.1273, 0.1374, 0.1436, 0.1199],
        [0.1340, 0.1236, 0.1294, 0.1092, 0.1211, 0.1061, 0.1352, 0.1414]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1653, 0.1556, 0.1159, 0.1166, 0.1044, 0.1081, 0.1185, 0.1155],
        [0.1161, 0.1141, 0.1343, 0.1167, 0.1179, 0.1361, 0.1324, 0.1322],
        [0.1498, 0.1531, 0.1362, 0.0962, 0.1206, 0.1132, 0.1161, 0.1148],
        [0.1433, 0.1363, 0.1306, 0.1216, 0.1116, 0.1220, 0.1198, 0.1148],
        [0.1395, 0.1309, 0.1255, 0.1180, 0.1330, 0.1216, 0.1129, 0.1185]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/22 05:59:38 PM | size of train loader is 391
07/22 05:59:43 PM | Train: [11/50] Step 000/390 Loss 0.010 Prec@(1,5) (14.1%, 26.6%)
07/22 06:02:37 PM | Train: [11/50] Step 050/390 Loss 0.010 Prec@(1,5) (11.1%, 31.3%)
07/22 06:05:31 PM | Train: [11/50] Step 100/390 Loss 0.010 Prec@(1,5) (11.1%, 31.2%)
07/22 06:08:21 PM | Train: [11/50] Step 150/390 Loss 0.010 Prec@(1,5) (11.4%, 31.5%)
07/22 06:11:11 PM | Train: [11/50] Step 200/390 Loss 0.010 Prec@(1,5) (11.5%, 31.7%)
07/22 06:14:01 PM | Train: [11/50] Step 250/390 Loss 0.010 Prec@(1,5) (11.3%, 32.3%)
07/22 06:16:52 PM | Train: [11/50] Step 300/390 Loss 0.010 Prec@(1,5) (11.3%, 32.4%)
07/22 06:19:46 PM | Train: [11/50] Step 350/390 Loss 0.010 Prec@(1,5) (11.4%, 32.7%)
07/22 06:21:59 PM | Train: [11/50] Step 390/390 Loss 0.010 Prec@(1,5) (11.3%, 32.6%)
07/22 06:22:00 PM | Train: [11/50] Final Prec@1 11.3040%
07/22 06:22:01 PM | Valid: [11/50] Step 000/390 Loss 3.878 Prec@(1,5) (9.4%, 35.9%)
07/22 06:22:21 PM | Valid: [11/50] Step 050/390 Loss 3.862 Prec@(1,5) (13.3%, 34.5%)
07/22 06:22:41 PM | Valid: [11/50] Step 100/390 Loss 3.863 Prec@(1,5) (12.6%, 33.6%)
07/22 06:23:01 PM | Valid: [11/50] Step 150/390 Loss 3.863 Prec@(1,5) (12.2%, 33.4%)
07/22 06:23:21 PM | Valid: [11/50] Step 200/390 Loss 3.874 Prec@(1,5) (12.1%, 33.0%)
07/22 06:23:41 PM | Valid: [11/50] Step 250/390 Loss 3.870 Prec@(1,5) (12.1%, 33.1%)
07/22 06:24:01 PM | Valid: [11/50] Step 300/390 Loss 3.872 Prec@(1,5) (12.0%, 32.9%)
07/22 06:24:21 PM | Valid: [11/50] Step 350/390 Loss 3.867 Prec@(1,5) (12.1%, 33.2%)
07/22 06:24:35 PM | Valid: [11/50] Step 390/390 Loss 3.866 Prec@(1,5) (12.2%, 33.3%)
07/22 06:24:36 PM | Valid: [11/50] Final Prec@1 12.1800%
07/22 06:24:36 PM | genotype = Genotype(normal=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('max_pool_3x3', 3), ('max_pool_3x3', 2)], [('skip_connect', 4), ('skip_connect', 3)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_5x5', 1)], [('max_pool_3x3', 0), ('dil_conv_3x3', 1)], [('max_pool_3x3', 0), ('sep_conv_5x5', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1650, 0.1596, 0.1504, 0.1021, 0.1124, 0.1073, 0.1057, 0.0975],
        [0.1722, 0.1574, 0.1540, 0.0917, 0.1128, 0.1144, 0.1051, 0.0924]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1528, 0.1616, 0.1464, 0.1145, 0.1269, 0.1035, 0.1123, 0.0820],
        [0.1460, 0.1546, 0.1568, 0.1154, 0.1167, 0.1022, 0.1115, 0.0967],
        [0.1675, 0.1679, 0.1571, 0.1161, 0.0952, 0.1033, 0.1063, 0.0865]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1752, 0.1609, 0.1535, 0.0939, 0.1062, 0.0987, 0.1207, 0.0909],
        [0.1630, 0.1502, 0.1505, 0.0974, 0.1184, 0.1113, 0.1122, 0.0969],
        [0.1749, 0.1646, 0.1527, 0.1152, 0.1098, 0.0982, 0.0973, 0.0874],
        [0.1861, 0.1749, 0.1860, 0.0983, 0.0967, 0.0911, 0.0935, 0.0734]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1720, 0.1840, 0.1776, 0.0978, 0.0886, 0.0895, 0.1065, 0.0840],
        [0.1659, 0.1602, 0.1650, 0.1109, 0.1031, 0.1065, 0.0955, 0.0929],
        [0.1704, 0.1655, 0.1569, 0.1075, 0.1018, 0.1074, 0.1102, 0.0804],
        [0.1816, 0.1786, 0.1959, 0.1005, 0.0923, 0.0913, 0.0849, 0.0749],
        [0.1756, 0.1684, 0.2079, 0.0797, 0.0955, 0.1019, 0.0944, 0.0765]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1456, 0.1347, 0.1254, 0.1294, 0.1412, 0.0984, 0.1194, 0.1060],
        [0.1225, 0.1170, 0.1074, 0.1304, 0.1214, 0.1308, 0.1411, 0.1295]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1498, 0.1396, 0.1104, 0.1119, 0.1185, 0.1181, 0.1242, 0.1275],
        [0.1147, 0.1103, 0.1341, 0.1205, 0.1146, 0.1473, 0.1119, 0.1468],
        [0.1278, 0.1218, 0.1186, 0.1151, 0.1207, 0.1393, 0.1332, 0.1235]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1475, 0.1315, 0.1152, 0.1212, 0.1180, 0.1143, 0.1123, 0.1400],
        [0.1152, 0.1115, 0.1265, 0.1352, 0.1484, 0.1129, 0.1257, 0.1245],
        [0.1161, 0.1088, 0.1054, 0.1383, 0.1277, 0.1384, 0.1459, 0.1196],
        [0.1371, 0.1243, 0.1308, 0.1065, 0.1192, 0.1045, 0.1355, 0.1421]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1674, 0.1549, 0.1168, 0.1158, 0.1045, 0.1082, 0.1181, 0.1144],
        [0.1155, 0.1127, 0.1356, 0.1151, 0.1190, 0.1386, 0.1314, 0.1321],
        [0.1523, 0.1522, 0.1347, 0.0946, 0.1218, 0.1142, 0.1148, 0.1153],
        [0.1464, 0.1361, 0.1299, 0.1221, 0.1104, 0.1214, 0.1203, 0.1133],
        [0.1415, 0.1305, 0.1253, 0.1182, 0.1318, 0.1216, 0.1119, 0.1192]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/22 06:24:37 PM | size of train loader is 391
07/22 06:24:42 PM | Train: [12/50] Step 000/390 Loss 0.010 Prec@(1,5) (10.9%, 26.6%)
07/22 06:27:33 PM | Train: [12/50] Step 050/390 Loss 0.010 Prec@(1,5) (11.9%, 33.8%)
07/22 06:30:23 PM | Train: [12/50] Step 100/390 Loss 0.010 Prec@(1,5) (12.1%, 33.6%)
07/22 06:33:11 PM | Train: [12/50] Step 150/390 Loss 0.010 Prec@(1,5) (11.8%, 33.0%)
07/22 06:36:01 PM | Train: [12/50] Step 200/390 Loss 0.010 Prec@(1,5) (11.9%, 33.1%)
07/22 06:38:47 PM | Train: [12/50] Step 250/390 Loss 0.010 Prec@(1,5) (12.0%, 33.4%)
07/22 06:41:38 PM | Train: [12/50] Step 300/390 Loss 0.010 Prec@(1,5) (12.1%, 33.5%)
07/22 06:44:27 PM | Train: [12/50] Step 350/390 Loss 0.010 Prec@(1,5) (12.2%, 33.7%)
07/22 06:46:39 PM | Train: [12/50] Step 390/390 Loss 0.010 Prec@(1,5) (12.2%, 33.9%)
07/22 06:46:40 PM | Train: [12/50] Final Prec@1 12.2440%
07/22 06:46:41 PM | Valid: [12/50] Step 000/390 Loss 3.662 Prec@(1,5) (9.4%, 31.2%)
07/22 06:47:01 PM | Valid: [12/50] Step 050/390 Loss 3.848 Prec@(1,5) (13.0%, 34.7%)
07/22 06:47:21 PM | Valid: [12/50] Step 100/390 Loss 3.828 Prec@(1,5) (13.1%, 35.1%)
07/22 06:47:41 PM | Valid: [12/50] Step 150/390 Loss 3.822 Prec@(1,5) (13.1%, 35.0%)
07/22 06:48:02 PM | Valid: [12/50] Step 200/390 Loss 3.817 Prec@(1,5) (13.0%, 34.8%)
07/22 06:48:22 PM | Valid: [12/50] Step 250/390 Loss 3.821 Prec@(1,5) (12.9%, 34.6%)
07/22 06:48:42 PM | Valid: [12/50] Step 300/390 Loss 3.819 Prec@(1,5) (13.0%, 34.7%)
07/22 06:49:02 PM | Valid: [12/50] Step 350/390 Loss 3.818 Prec@(1,5) (12.8%, 34.7%)
07/22 06:49:16 PM | Valid: [12/50] Step 390/390 Loss 3.818 Prec@(1,5) (12.7%, 34.5%)
07/22 06:49:16 PM | Valid: [12/50] Final Prec@1 12.7040%
07/22 06:49:16 PM | genotype = Genotype(normal=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 2), ('avg_pool_3x3', 0)], [('max_pool_3x3', 3), ('max_pool_3x3', 2)], [('skip_connect', 4), ('skip_connect', 3)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_5x5', 1)], [('dil_conv_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('sep_conv_5x5', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1726, 0.1601, 0.1502, 0.1006, 0.1122, 0.1057, 0.1043, 0.0943],
        [0.1805, 0.1572, 0.1544, 0.0893, 0.1124, 0.1116, 0.1040, 0.0907]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1594, 0.1635, 0.1476, 0.1141, 0.1248, 0.1021, 0.1100, 0.0785],
        [0.1512, 0.1557, 0.1591, 0.1140, 0.1151, 0.0996, 0.1105, 0.0947],
        [0.1749, 0.1703, 0.1590, 0.1151, 0.0926, 0.1014, 0.1041, 0.0826]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1811, 0.1615, 0.1536, 0.0926, 0.1058, 0.0954, 0.1210, 0.0889],
        [0.1674, 0.1509, 0.1518, 0.0964, 0.1200, 0.1085, 0.1097, 0.0952],
        [0.1799, 0.1657, 0.1542, 0.1127, 0.1096, 0.0970, 0.0959, 0.0850],
        [0.1908, 0.1756, 0.1906, 0.0980, 0.0938, 0.0881, 0.0925, 0.0706]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1773, 0.1868, 0.1805, 0.0955, 0.0852, 0.0875, 0.1061, 0.0809],
        [0.1710, 0.1595, 0.1652, 0.1108, 0.1015, 0.1057, 0.0952, 0.0912],
        [0.1749, 0.1659, 0.1567, 0.1063, 0.1013, 0.1077, 0.1093, 0.0778],
        [0.1872, 0.1799, 0.2013, 0.0980, 0.0908, 0.0884, 0.0819, 0.0723],
        [0.1797, 0.1680, 0.2166, 0.0775, 0.0932, 0.0987, 0.0926, 0.0736]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1476, 0.1337, 0.1264, 0.1299, 0.1404, 0.0966, 0.1186, 0.1068],
        [0.1219, 0.1149, 0.1086, 0.1315, 0.1212, 0.1311, 0.1417, 0.1291]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1513, 0.1381, 0.1099, 0.1122, 0.1191, 0.1186, 0.1250, 0.1259],
        [0.1133, 0.1075, 0.1340, 0.1209, 0.1152, 0.1503, 0.1100, 0.1487],
        [0.1282, 0.1201, 0.1168, 0.1145, 0.1201, 0.1416, 0.1344, 0.1243]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1494, 0.1305, 0.1150, 0.1197, 0.1172, 0.1136, 0.1137, 0.1410],
        [0.1153, 0.1103, 0.1254, 0.1333, 0.1501, 0.1153, 0.1255, 0.1247],
        [0.1164, 0.1075, 0.1051, 0.1390, 0.1269, 0.1372, 0.1471, 0.1208],
        [0.1405, 0.1251, 0.1321, 0.1034, 0.1169, 0.1028, 0.1371, 0.1421]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1710, 0.1551, 0.1177, 0.1135, 0.1037, 0.1077, 0.1190, 0.1123],
        [0.1139, 0.1105, 0.1376, 0.1138, 0.1208, 0.1390, 0.1319, 0.1325],
        [0.1547, 0.1523, 0.1331, 0.0927, 0.1219, 0.1162, 0.1143, 0.1149],
        [0.1490, 0.1355, 0.1280, 0.1253, 0.1102, 0.1214, 0.1193, 0.1112],
        [0.1438, 0.1298, 0.1243, 0.1174, 0.1331, 0.1211, 0.1102, 0.1203]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/22 06:49:17 PM | size of train loader is 391
07/22 06:49:23 PM | Train: [13/50] Step 000/390 Loss 0.010 Prec@(1,5) (9.4%, 32.8%)
07/22 06:52:14 PM | Train: [13/50] Step 050/390 Loss 0.010 Prec@(1,5) (11.4%, 33.4%)
07/22 06:55:04 PM | Train: [13/50] Step 100/390 Loss 0.010 Prec@(1,5) (12.0%, 34.0%)
07/22 06:58:10 PM | Train: [13/50] Step 150/390 Loss 0.010 Prec@(1,5) (12.1%, 34.3%)
07/22 07:01:38 PM | Train: [13/50] Step 200/390 Loss 0.010 Prec@(1,5) (12.1%, 34.3%)
07/22 07:05:02 PM | Train: [13/50] Step 250/390 Loss 0.010 Prec@(1,5) (12.3%, 34.4%)
07/22 07:08:30 PM | Train: [13/50] Step 300/390 Loss 0.010 Prec@(1,5) (12.6%, 34.8%)
07/22 07:11:56 PM | Train: [13/50] Step 350/390 Loss 0.010 Prec@(1,5) (12.7%, 35.0%)
07/22 07:14:39 PM | Train: [13/50] Step 390/390 Loss 0.010 Prec@(1,5) (12.7%, 35.2%)
07/22 07:14:39 PM | Train: [13/50] Final Prec@1 12.7040%
07/22 07:14:40 PM | Valid: [13/50] Step 000/390 Loss 3.616 Prec@(1,5) (18.8%, 46.9%)
07/22 07:15:01 PM | Valid: [13/50] Step 050/390 Loss 3.767 Prec@(1,5) (13.3%, 35.7%)
07/22 07:15:22 PM | Valid: [13/50] Step 100/390 Loss 3.775 Prec@(1,5) (13.5%, 35.8%)
07/22 07:15:44 PM | Valid: [13/50] Step 150/390 Loss 3.784 Prec@(1,5) (13.3%, 35.9%)
07/22 07:16:04 PM | Valid: [13/50] Step 200/390 Loss 3.784 Prec@(1,5) (13.2%, 35.8%)
07/22 07:16:24 PM | Valid: [13/50] Step 250/390 Loss 3.788 Prec@(1,5) (13.1%, 35.6%)
07/22 07:16:44 PM | Valid: [13/50] Step 300/390 Loss 3.788 Prec@(1,5) (13.1%, 35.7%)
07/22 07:17:04 PM | Valid: [13/50] Step 350/390 Loss 3.786 Prec@(1,5) (13.2%, 35.7%)
07/22 07:17:18 PM | Valid: [13/50] Step 390/390 Loss 3.784 Prec@(1,5) (13.2%, 35.8%)
07/22 07:17:18 PM | Valid: [13/50] Final Prec@1 13.1680%
07/22 07:17:18 PM | genotype = Genotype(normal=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 0)], [('max_pool_3x3', 3), ('max_pool_3x3', 0)], [('skip_connect', 4), ('skip_connect', 3)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_5x5', 1)], [('dil_conv_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('sep_conv_5x5', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1820, 0.1599, 0.1501, 0.0995, 0.1111, 0.1040, 0.1029, 0.0905],
        [0.1872, 0.1537, 0.1519, 0.0878, 0.1129, 0.1121, 0.1035, 0.0909]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1668, 0.1641, 0.1486, 0.1141, 0.1238, 0.1003, 0.1071, 0.0752],
        [0.1555, 0.1541, 0.1592, 0.1139, 0.1153, 0.0979, 0.1097, 0.0943],
        [0.1820, 0.1699, 0.1592, 0.1149, 0.0903, 0.1002, 0.1037, 0.0798]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1872, 0.1621, 0.1548, 0.0901, 0.1052, 0.0927, 0.1210, 0.0869],
        [0.1712, 0.1504, 0.1522, 0.0958, 0.1217, 0.1062, 0.1079, 0.0945],
        [0.1848, 0.1655, 0.1554, 0.1089, 0.1100, 0.0968, 0.0951, 0.0834],
        [0.1948, 0.1748, 0.1947, 0.0971, 0.0921, 0.0864, 0.0920, 0.0681]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1843, 0.1891, 0.1832, 0.0931, 0.0822, 0.0854, 0.1051, 0.0775],
        [0.1743, 0.1558, 0.1626, 0.1117, 0.1020, 0.1069, 0.0959, 0.0908],
        [0.1800, 0.1656, 0.1575, 0.1039, 0.1008, 0.1078, 0.1081, 0.0762],
        [0.1932, 0.1800, 0.2062, 0.0959, 0.0897, 0.0860, 0.0789, 0.0702],
        [0.1842, 0.1668, 0.2245, 0.0750, 0.0909, 0.0960, 0.0912, 0.0714]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1481, 0.1306, 0.1300, 0.1298, 0.1389, 0.0965, 0.1179, 0.1082],
        [0.1222, 0.1132, 0.1081, 0.1331, 0.1209, 0.1315, 0.1427, 0.1282]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1513, 0.1348, 0.1104, 0.1122, 0.1204, 0.1200, 0.1261, 0.1248],
        [0.1134, 0.1058, 0.1330, 0.1226, 0.1171, 0.1529, 0.1071, 0.1481],
        [0.1298, 0.1178, 0.1150, 0.1150, 0.1187, 0.1435, 0.1353, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1505, 0.1281, 0.1149, 0.1194, 0.1154, 0.1131, 0.1155, 0.1431],
        [0.1160, 0.1092, 0.1230, 0.1318, 0.1517, 0.1177, 0.1261, 0.1246],
        [0.1167, 0.1049, 0.1034, 0.1396, 0.1284, 0.1371, 0.1486, 0.1211],
        [0.1433, 0.1243, 0.1329, 0.1003, 0.1146, 0.1022, 0.1384, 0.1439]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1730, 0.1533, 0.1189, 0.1116, 0.1028, 0.1080, 0.1205, 0.1119],
        [0.1129, 0.1085, 0.1393, 0.1133, 0.1213, 0.1403, 0.1326, 0.1319],
        [0.1577, 0.1505, 0.1318, 0.0914, 0.1220, 0.1184, 0.1131, 0.1151],
        [0.1515, 0.1341, 0.1263, 0.1280, 0.1088, 0.1225, 0.1197, 0.1092],
        [0.1466, 0.1285, 0.1232, 0.1169, 0.1333, 0.1221, 0.1080, 0.1215]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/22 07:17:20 PM | size of train loader is 391
07/22 07:17:26 PM | Train: [14/50] Step 000/390 Loss 0.009 Prec@(1,5) (20.3%, 45.3%)
07/22 07:20:34 PM | Train: [14/50] Step 050/390 Loss 0.010 Prec@(1,5) (13.5%, 36.7%)
07/22 07:23:50 PM | Train: [14/50] Step 100/390 Loss 0.010 Prec@(1,5) (13.4%, 36.7%)
07/22 07:27:08 PM | Train: [14/50] Step 150/390 Loss 0.010 Prec@(1,5) (13.4%, 36.7%)
07/22 07:30:37 PM | Train: [14/50] Step 200/390 Loss 0.010 Prec@(1,5) (13.5%, 36.3%)
07/22 07:33:56 PM | Train: [14/50] Step 250/390 Loss 0.010 Prec@(1,5) (13.4%, 36.5%)
07/22 07:37:17 PM | Train: [14/50] Step 300/390 Loss 0.010 Prec@(1,5) (13.3%, 36.5%)
07/22 07:40:44 PM | Train: [14/50] Step 350/390 Loss 0.010 Prec@(1,5) (13.2%, 36.3%)
07/22 07:43:25 PM | Train: [14/50] Step 390/390 Loss 0.010 Prec@(1,5) (13.3%, 36.3%)
07/22 07:43:25 PM | Train: [14/50] Final Prec@1 13.3360%
07/22 07:43:27 PM | Valid: [14/50] Step 000/390 Loss 3.655 Prec@(1,5) (18.8%, 39.1%)
07/22 07:43:48 PM | Valid: [14/50] Step 050/390 Loss 3.727 Prec@(1,5) (14.6%, 37.4%)
07/22 07:44:09 PM | Valid: [14/50] Step 100/390 Loss 3.746 Prec@(1,5) (13.9%, 37.2%)
07/22 07:44:29 PM | Valid: [14/50] Step 150/390 Loss 3.742 Prec@(1,5) (14.1%, 37.2%)
07/22 07:44:49 PM | Valid: [14/50] Step 200/390 Loss 3.746 Prec@(1,5) (14.0%, 37.2%)
07/22 07:45:09 PM | Valid: [14/50] Step 250/390 Loss 3.745 Prec@(1,5) (14.0%, 37.1%)
07/22 07:45:29 PM | Valid: [14/50] Step 300/390 Loss 3.744 Prec@(1,5) (14.1%, 37.0%)
07/22 07:45:50 PM | Valid: [14/50] Step 350/390 Loss 3.748 Prec@(1,5) (14.0%, 36.8%)
07/22 07:46:04 PM | Valid: [14/50] Step 390/390 Loss 3.749 Prec@(1,5) (13.9%, 36.7%)
07/22 07:46:04 PM | Valid: [14/50] Final Prec@1 13.8840%
07/22 07:46:04 PM | genotype = Genotype(normal=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 0)], [('skip_connect', 3), ('max_pool_3x3', 0)], [('skip_connect', 4), ('skip_connect', 3)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_5x5', 1)], [('dil_conv_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('sep_conv_5x5', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1901, 0.1578, 0.1487, 0.0986, 0.1114, 0.1033, 0.1023, 0.0878],
        [0.1958, 0.1507, 0.1501, 0.0857, 0.1130, 0.1113, 0.1027, 0.0907]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1733, 0.1630, 0.1483, 0.1156, 0.1226, 0.0986, 0.1056, 0.0730],
        [0.1610, 0.1525, 0.1594, 0.1135, 0.1155, 0.0957, 0.1091, 0.0934],
        [0.1885, 0.1685, 0.1597, 0.1140, 0.0889, 0.0996, 0.1031, 0.0777]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1929, 0.1614, 0.1559, 0.0889, 0.1047, 0.0899, 0.1206, 0.0857],
        [0.1769, 0.1500, 0.1529, 0.0951, 0.1226, 0.1032, 0.1062, 0.0930],
        [0.1902, 0.1657, 0.1577, 0.1058, 0.1086, 0.0963, 0.0937, 0.0821],
        [0.1986, 0.1737, 0.1992, 0.0968, 0.0897, 0.0846, 0.0917, 0.0656]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1904, 0.1907, 0.1853, 0.0913, 0.0793, 0.0834, 0.1049, 0.0747],
        [0.1782, 0.1519, 0.1596, 0.1126, 0.1022, 0.1086, 0.0960, 0.0909],
        [0.1850, 0.1650, 0.1572, 0.1033, 0.0998, 0.1070, 0.1075, 0.0752],
        [0.1988, 0.1799, 0.2101, 0.0939, 0.0887, 0.0838, 0.0765, 0.0683],
        [0.1891, 0.1655, 0.2315, 0.0726, 0.0888, 0.0934, 0.0898, 0.0694]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1497, 0.1285, 0.1322, 0.1292, 0.1381, 0.0956, 0.1167, 0.1100],
        [0.1215, 0.1105, 0.1089, 0.1350, 0.1223, 0.1318, 0.1432, 0.1268]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1525, 0.1328, 0.1100, 0.1129, 0.1199, 0.1219, 0.1265, 0.1234],
        [0.1120, 0.1028, 0.1326, 0.1235, 0.1198, 0.1547, 0.1056, 0.1488],
        [0.1317, 0.1159, 0.1135, 0.1130, 0.1179, 0.1460, 0.1370, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1532, 0.1271, 0.1140, 0.1185, 0.1145, 0.1121, 0.1168, 0.1438],
        [0.1154, 0.1069, 0.1215, 0.1306, 0.1537, 0.1208, 0.1263, 0.1246],
        [0.1175, 0.1032, 0.1023, 0.1381, 0.1278, 0.1394, 0.1506, 0.1212],
        [0.1473, 0.1250, 0.1342, 0.0970, 0.1135, 0.1000, 0.1401, 0.1429]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1755, 0.1528, 0.1212, 0.1083, 0.1018, 0.1073, 0.1218, 0.1114],
        [0.1109, 0.1058, 0.1420, 0.1110, 0.1229, 0.1424, 0.1336, 0.1315],
        [0.1590, 0.1483, 0.1300, 0.0910, 0.1231, 0.1214, 0.1122, 0.1150],
        [0.1528, 0.1325, 0.1246, 0.1305, 0.1080, 0.1234, 0.1201, 0.1081],
        [0.1482, 0.1273, 0.1227, 0.1170, 0.1328, 0.1224, 0.1052, 0.1243]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/22 07:46:05 PM | size of train loader is 391
07/22 07:46:11 PM | Train: [15/50] Step 000/390 Loss 0.010 Prec@(1,5) (14.1%, 32.8%)
07/22 07:49:34 PM | Train: [15/50] Step 050/390 Loss 0.009 Prec@(1,5) (14.2%, 37.9%)
07/22 07:52:57 PM | Train: [15/50] Step 100/390 Loss 0.009 Prec@(1,5) (13.9%, 37.0%)
07/22 07:56:23 PM | Train: [15/50] Step 150/390 Loss 0.009 Prec@(1,5) (14.1%, 37.6%)
07/22 07:59:46 PM | Train: [15/50] Step 200/390 Loss 0.009 Prec@(1,5) (14.0%, 37.4%)
07/22 08:03:03 PM | Train: [15/50] Step 250/390 Loss 0.009 Prec@(1,5) (13.9%, 37.4%)
07/22 08:06:05 PM | Train: [15/50] Step 300/390 Loss 0.010 Prec@(1,5) (13.9%, 37.3%)
07/22 08:09:26 PM | Train: [15/50] Step 350/390 Loss 0.010 Prec@(1,5) (13.9%, 37.2%)
07/22 08:12:00 PM | Train: [15/50] Step 390/390 Loss 0.010 Prec@(1,5) (13.8%, 37.3%)
07/22 08:12:01 PM | Train: [15/50] Final Prec@1 13.8120%
07/22 08:12:02 PM | Valid: [15/50] Step 000/390 Loss 3.762 Prec@(1,5) (10.9%, 40.6%)
07/22 08:12:23 PM | Valid: [15/50] Step 050/390 Loss 3.729 Prec@(1,5) (13.7%, 37.5%)
07/22 08:12:44 PM | Valid: [15/50] Step 100/390 Loss 3.732 Prec@(1,5) (13.7%, 36.7%)
07/22 08:13:05 PM | Valid: [15/50] Step 150/390 Loss 3.721 Prec@(1,5) (14.0%, 37.2%)
07/22 08:13:26 PM | Valid: [15/50] Step 200/390 Loss 3.718 Prec@(1,5) (14.0%, 37.3%)
07/22 08:13:47 PM | Valid: [15/50] Step 250/390 Loss 3.716 Prec@(1,5) (14.0%, 37.5%)
07/22 08:14:07 PM | Valid: [15/50] Step 300/390 Loss 3.712 Prec@(1,5) (14.0%, 37.6%)
07/22 08:14:27 PM | Valid: [15/50] Step 350/390 Loss 3.708 Prec@(1,5) (13.9%, 37.7%)
07/22 08:14:42 PM | Valid: [15/50] Step 390/390 Loss 3.708 Prec@(1,5) (14.0%, 37.7%)
07/22 08:14:42 PM | Valid: [15/50] Final Prec@1 14.0080%
07/22 08:14:42 PM | genotype = Genotype(normal=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 0)], [('skip_connect', 3), ('max_pool_3x3', 0)], [('skip_connect', 4), ('skip_connect', 3)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_5x5', 1)], [('dil_conv_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 3)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1971, 0.1542, 0.1457, 0.0996, 0.1118, 0.1037, 0.1023, 0.0855],
        [0.2041, 0.1474, 0.1478, 0.0844, 0.1131, 0.1102, 0.1028, 0.0902]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1802, 0.1605, 0.1471, 0.1172, 0.1216, 0.0975, 0.1045, 0.0714],
        [0.1664, 0.1503, 0.1589, 0.1123, 0.1153, 0.0943, 0.1098, 0.0926],
        [0.1950, 0.1674, 0.1604, 0.1129, 0.0874, 0.0990, 0.1027, 0.0753]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1999, 0.1607, 0.1570, 0.0879, 0.1030, 0.0869, 0.1203, 0.0843],
        [0.1810, 0.1483, 0.1520, 0.0950, 0.1248, 0.1014, 0.1046, 0.0928],
        [0.1952, 0.1653, 0.1592, 0.1013, 0.1073, 0.0971, 0.0937, 0.0808],
        [0.2021, 0.1715, 0.2029, 0.0974, 0.0873, 0.0833, 0.0919, 0.0636]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1942, 0.1914, 0.1875, 0.0895, 0.0771, 0.0820, 0.1055, 0.0728],
        [0.1810, 0.1487, 0.1572, 0.1121, 0.1022, 0.1112, 0.0964, 0.0912],
        [0.1892, 0.1645, 0.1585, 0.1020, 0.0986, 0.1064, 0.1059, 0.0748],
        [0.2037, 0.1794, 0.2134, 0.0915, 0.0882, 0.0821, 0.0745, 0.0673],
        [0.1924, 0.1632, 0.2377, 0.0706, 0.0870, 0.0919, 0.0887, 0.0686]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1515, 0.1263, 0.1338, 0.1295, 0.1365, 0.0953, 0.1148, 0.1123],
        [0.1222, 0.1089, 0.1087, 0.1361, 0.1220, 0.1327, 0.1446, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1536, 0.1303, 0.1107, 0.1140, 0.1189, 0.1240, 0.1271, 0.1215],
        [0.1123, 0.1012, 0.1325, 0.1239, 0.1222, 0.1553, 0.1029, 0.1495],
        [0.1333, 0.1141, 0.1125, 0.1119, 0.1162, 0.1488, 0.1374, 0.1257]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1560, 0.1262, 0.1131, 0.1165, 0.1130, 0.1116, 0.1196, 0.1440],
        [0.1168, 0.1062, 0.1194, 0.1298, 0.1533, 0.1241, 0.1260, 0.1244],
        [0.1177, 0.1013, 0.1020, 0.1376, 0.1259, 0.1408, 0.1521, 0.1226],
        [0.1511, 0.1246, 0.1349, 0.0951, 0.1103, 0.0995, 0.1414, 0.1431]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1772, 0.1511, 0.1229, 0.1069, 0.1004, 0.1073, 0.1243, 0.1098],
        [0.1097, 0.1040, 0.1421, 0.1096, 0.1251, 0.1431, 0.1356, 0.1309],
        [0.1613, 0.1469, 0.1286, 0.0899, 0.1240, 0.1240, 0.1103, 0.1149],
        [0.1542, 0.1305, 0.1222, 0.1350, 0.1069, 0.1241, 0.1201, 0.1071],
        [0.1516, 0.1269, 0.1224, 0.1153, 0.1311, 0.1229, 0.1025, 0.1273]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/22 08:14:43 PM | size of train loader is 391
07/22 08:14:48 PM | Train: [16/50] Step 000/390 Loss 0.010 Prec@(1,5) (12.5%, 39.1%)
07/22 08:17:56 PM | Train: [16/50] Step 050/390 Loss 0.009 Prec@(1,5) (14.0%, 37.0%)
07/22 08:21:15 PM | Train: [16/50] Step 100/390 Loss 0.009 Prec@(1,5) (13.8%, 37.6%)
07/22 08:24:40 PM | Train: [16/50] Step 150/390 Loss 0.009 Prec@(1,5) (13.8%, 37.9%)
07/22 08:28:07 PM | Train: [16/50] Step 200/390 Loss 0.009 Prec@(1,5) (14.1%, 38.0%)
07/22 08:31:33 PM | Train: [16/50] Step 250/390 Loss 0.009 Prec@(1,5) (14.2%, 38.2%)
07/22 08:34:54 PM | Train: [16/50] Step 300/390 Loss 0.009 Prec@(1,5) (14.2%, 38.1%)
07/22 08:38:19 PM | Train: [16/50] Step 350/390 Loss 0.009 Prec@(1,5) (14.2%, 38.1%)
07/22 08:41:04 PM | Train: [16/50] Step 390/390 Loss 0.009 Prec@(1,5) (14.4%, 38.4%)
07/22 08:41:04 PM | Train: [16/50] Final Prec@1 14.3560%
07/22 08:41:05 PM | Valid: [16/50] Step 000/390 Loss 3.839 Prec@(1,5) (17.2%, 31.2%)
07/22 08:41:26 PM | Valid: [16/50] Step 050/390 Loss 3.705 Prec@(1,5) (15.0%, 39.3%)
07/22 08:41:46 PM | Valid: [16/50] Step 100/390 Loss 3.692 Prec@(1,5) (14.6%, 39.2%)
07/22 08:42:06 PM | Valid: [16/50] Step 150/390 Loss 3.692 Prec@(1,5) (14.4%, 38.3%)
07/22 08:42:26 PM | Valid: [16/50] Step 200/390 Loss 3.693 Prec@(1,5) (14.6%, 38.0%)
07/22 08:42:47 PM | Valid: [16/50] Step 250/390 Loss 3.689 Prec@(1,5) (14.5%, 38.3%)
07/22 08:43:06 PM | Valid: [16/50] Step 300/390 Loss 3.683 Prec@(1,5) (14.6%, 38.6%)
07/22 08:43:27 PM | Valid: [16/50] Step 350/390 Loss 3.683 Prec@(1,5) (14.6%, 38.6%)
07/22 08:43:41 PM | Valid: [16/50] Step 390/390 Loss 3.686 Prec@(1,5) (14.5%, 38.5%)
07/22 08:43:41 PM | Valid: [16/50] Final Prec@1 14.5120%
07/22 08:43:41 PM | genotype = Genotype(normal=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 0)], [('max_pool_3x3', 3), ('max_pool_3x3', 0)], [('skip_connect', 4), ('skip_connect', 3)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_5x5', 1)], [('dil_conv_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 3)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.2064, 0.1507, 0.1433, 0.0999, 0.1111, 0.1041, 0.1019, 0.0827],
        [0.2119, 0.1424, 0.1441, 0.0844, 0.1131, 0.1104, 0.1021, 0.0916]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1873, 0.1582, 0.1464, 0.1192, 0.1212, 0.0957, 0.1025, 0.0695],
        [0.1708, 0.1468, 0.1572, 0.1132, 0.1146, 0.0932, 0.1109, 0.0933],
        [0.2006, 0.1645, 0.1590, 0.1124, 0.0868, 0.0988, 0.1040, 0.0739]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.2068, 0.1594, 0.1577, 0.0874, 0.1021, 0.0839, 0.1195, 0.0832],
        [0.1842, 0.1453, 0.1500, 0.0964, 0.1272, 0.1002, 0.1030, 0.0936],
        [0.1992, 0.1633, 0.1590, 0.0989, 0.1074, 0.0979, 0.0939, 0.0805],
        [0.2050, 0.1682, 0.2048, 0.0982, 0.0860, 0.0830, 0.0924, 0.0623]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.2003, 0.1910, 0.1883, 0.0875, 0.0749, 0.0803, 0.1067, 0.0709],
        [0.1833, 0.1441, 0.1532, 0.1118, 0.1031, 0.1143, 0.0979, 0.0923],
        [0.1935, 0.1622, 0.1565, 0.1012, 0.0996, 0.1061, 0.1063, 0.0746],
        [0.2095, 0.1778, 0.2151, 0.0902, 0.0881, 0.0804, 0.0723, 0.0666],
        [0.1968, 0.1603, 0.2429, 0.0687, 0.0853, 0.0905, 0.0880, 0.0676]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1540, 0.1243, 0.1355, 0.1287, 0.1340, 0.0944, 0.1144, 0.1147],
        [0.1228, 0.1072, 0.1110, 0.1377, 0.1216, 0.1331, 0.1439, 0.1227]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1554, 0.1280, 0.1101, 0.1142, 0.1184, 0.1261, 0.1289, 0.1189],
        [0.1127, 0.0995, 0.1328, 0.1250, 0.1232, 0.1552, 0.1012, 0.1503],
        [0.1352, 0.1124, 0.1124, 0.1100, 0.1135, 0.1521, 0.1376, 0.1269]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1589, 0.1249, 0.1121, 0.1144, 0.1131, 0.1107, 0.1211, 0.1448],
        [0.1180, 0.1050, 0.1173, 0.1285, 0.1539, 0.1264, 0.1264, 0.1244],
        [0.1190, 0.0996, 0.1012, 0.1381, 0.1256, 0.1407, 0.1527, 0.1231],
        [0.1542, 0.1242, 0.1354, 0.0930, 0.1085, 0.0979, 0.1435, 0.1433]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1797, 0.1497, 0.1259, 0.1037, 0.0991, 0.1067, 0.1266, 0.1086],
        [0.1078, 0.1013, 0.1445, 0.1089, 0.1261, 0.1437, 0.1366, 0.1312],
        [0.1624, 0.1442, 0.1274, 0.0894, 0.1255, 0.1262, 0.1090, 0.1158],
        [0.1558, 0.1288, 0.1205, 0.1385, 0.1061, 0.1246, 0.1203, 0.1055],
        [0.1533, 0.1255, 0.1215, 0.1152, 0.1316, 0.1231, 0.1002, 0.1296]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/22 08:43:42 PM | size of train loader is 391
07/22 08:43:48 PM | Train: [17/50] Step 000/390 Loss 0.010 Prec@(1,5) (10.9%, 29.7%)
07/22 08:46:42 PM | Train: [17/50] Step 050/390 Loss 0.009 Prec@(1,5) (14.6%, 38.8%)
07/22 08:49:34 PM | Train: [17/50] Step 100/390 Loss 0.009 Prec@(1,5) (14.7%, 39.1%)
07/22 08:52:35 PM | Train: [17/50] Step 150/390 Loss 0.009 Prec@(1,5) (14.8%, 39.2%)
07/22 08:55:51 PM | Train: [17/50] Step 200/390 Loss 0.009 Prec@(1,5) (14.7%, 39.2%)
07/22 08:59:17 PM | Train: [17/50] Step 250/390 Loss 0.009 Prec@(1,5) (14.8%, 39.3%)
07/22 09:02:50 PM | Train: [17/50] Step 300/390 Loss 0.009 Prec@(1,5) (15.1%, 39.4%)
07/22 09:06:13 PM | Train: [17/50] Step 350/390 Loss 0.009 Prec@(1,5) (14.9%, 39.3%)
07/22 09:08:53 PM | Train: [17/50] Step 390/390 Loss 0.009 Prec@(1,5) (15.0%, 39.2%)
07/22 09:08:53 PM | Train: [17/50] Final Prec@1 14.9840%
07/22 09:08:54 PM | Valid: [17/50] Step 000/390 Loss 3.589 Prec@(1,5) (17.2%, 37.5%)
07/22 09:09:15 PM | Valid: [17/50] Step 050/390 Loss 3.664 Prec@(1,5) (15.1%, 39.4%)
07/22 09:09:35 PM | Valid: [17/50] Step 100/390 Loss 3.647 Prec@(1,5) (15.2%, 40.0%)
07/22 09:09:55 PM | Valid: [17/50] Step 150/390 Loss 3.653 Prec@(1,5) (15.1%, 39.5%)
07/22 09:10:15 PM | Valid: [17/50] Step 200/390 Loss 3.657 Prec@(1,5) (14.8%, 39.2%)
07/22 09:10:35 PM | Valid: [17/50] Step 250/390 Loss 3.649 Prec@(1,5) (15.1%, 39.4%)
07/22 09:10:55 PM | Valid: [17/50] Step 300/390 Loss 3.649 Prec@(1,5) (14.9%, 39.6%)
07/22 09:11:15 PM | Valid: [17/50] Step 350/390 Loss 3.649 Prec@(1,5) (15.0%, 39.6%)
07/22 09:11:29 PM | Valid: [17/50] Step 390/390 Loss 3.649 Prec@(1,5) (14.9%, 39.5%)
07/22 09:11:29 PM | Valid: [17/50] Final Prec@1 14.9480%
07/22 09:11:29 PM | genotype = Genotype(normal=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 0)], [('max_pool_3x3', 3), ('max_pool_3x3', 0)], [('skip_connect', 4), ('skip_connect', 3)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_5x5', 1)], [('dil_conv_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 3)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.2159, 0.1461, 0.1408, 0.0998, 0.1112, 0.1046, 0.1012, 0.0805],
        [0.2202, 0.1379, 0.1408, 0.0838, 0.1130, 0.1105, 0.1010, 0.0927]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1940, 0.1538, 0.1445, 0.1222, 0.1208, 0.0960, 0.1005, 0.0683],
        [0.1763, 0.1437, 0.1566, 0.1138, 0.1130, 0.0919, 0.1113, 0.0935],
        [0.2056, 0.1609, 0.1579, 0.1116, 0.0858, 0.1003, 0.1047, 0.0732]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.2127, 0.1566, 0.1579, 0.0869, 0.1016, 0.0823, 0.1186, 0.0835],
        [0.1872, 0.1426, 0.1486, 0.0968, 0.1305, 0.0985, 0.1022, 0.0937],
        [0.2025, 0.1607, 0.1591, 0.0969, 0.1062, 0.0992, 0.0940, 0.0814],
        [0.2072, 0.1641, 0.2050, 0.0999, 0.0855, 0.0831, 0.0935, 0.0618]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.2060, 0.1889, 0.1881, 0.0868, 0.0739, 0.0784, 0.1083, 0.0697],
        [0.1860, 0.1397, 0.1495, 0.1127, 0.1035, 0.1166, 0.0989, 0.0932],
        [0.1983, 0.1595, 0.1544, 0.1007, 0.1003, 0.1057, 0.1064, 0.0749],
        [0.2157, 0.1751, 0.2157, 0.0886, 0.0879, 0.0795, 0.0710, 0.0663],
        [0.2017, 0.1573, 0.2467, 0.0666, 0.0844, 0.0887, 0.0875, 0.0672]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1572, 0.1222, 0.1370, 0.1282, 0.1320, 0.0936, 0.1134, 0.1164],
        [0.1232, 0.1048, 0.1130, 0.1386, 0.1220, 0.1333, 0.1435, 0.1216]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1574, 0.1250, 0.1090, 0.1159, 0.1182, 0.1274, 0.1306, 0.1166],
        [0.1130, 0.0973, 0.1324, 0.1266, 0.1249, 0.1556, 0.0993, 0.1510],
        [0.1371, 0.1098, 0.1112, 0.1076, 0.1124, 0.1542, 0.1399, 0.1278]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1622, 0.1240, 0.1110, 0.1126, 0.1125, 0.1091, 0.1235, 0.1451],
        [0.1188, 0.1037, 0.1145, 0.1285, 0.1543, 0.1288, 0.1264, 0.1250],
        [0.1202, 0.0980, 0.1000, 0.1377, 0.1241, 0.1427, 0.1541, 0.1232],
        [0.1578, 0.1238, 0.1360, 0.0904, 0.1073, 0.0974, 0.1440, 0.1433]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1836, 0.1491, 0.1288, 0.1005, 0.0981, 0.1054, 0.1277, 0.1068],
        [0.1062, 0.0988, 0.1471, 0.1071, 0.1255, 0.1451, 0.1374, 0.1328],
        [0.1655, 0.1422, 0.1269, 0.0886, 0.1253, 0.1275, 0.1079, 0.1161],
        [0.1583, 0.1278, 0.1197, 0.1414, 0.1052, 0.1244, 0.1195, 0.1036],
        [0.1557, 0.1244, 0.1208, 0.1146, 0.1311, 0.1228, 0.0985, 0.1322]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/22 09:11:30 PM | size of train loader is 391
07/22 09:11:36 PM | Train: [18/50] Step 000/390 Loss 0.010 Prec@(1,5) (17.2%, 39.1%)
07/22 09:14:49 PM | Train: [18/50] Step 050/390 Loss 0.009 Prec@(1,5) (15.8%, 39.7%)
07/22 09:18:20 PM | Train: [18/50] Step 100/390 Loss 0.009 Prec@(1,5) (15.6%, 40.6%)
07/22 09:21:39 PM | Train: [18/50] Step 150/390 Loss 0.009 Prec@(1,5) (15.5%, 40.4%)
07/22 09:24:50 PM | Train: [18/50] Step 200/390 Loss 0.009 Prec@(1,5) (15.5%, 40.3%)
07/22 09:27:43 PM | Train: [18/50] Step 250/390 Loss 0.009 Prec@(1,5) (15.6%, 40.4%)
07/22 09:30:35 PM | Train: [18/50] Step 300/390 Loss 0.009 Prec@(1,5) (15.5%, 40.2%)
07/22 09:33:31 PM | Train: [18/50] Step 350/390 Loss 0.009 Prec@(1,5) (15.5%, 40.2%)
07/22 09:35:49 PM | Train: [18/50] Step 390/390 Loss 0.009 Prec@(1,5) (15.5%, 40.0%)
07/22 09:35:50 PM | Train: [18/50] Final Prec@1 15.5200%
07/22 09:35:51 PM | Valid: [18/50] Step 000/390 Loss 3.661 Prec@(1,5) (15.6%, 35.9%)
07/22 09:36:12 PM | Valid: [18/50] Step 050/390 Loss 3.632 Prec@(1,5) (15.0%, 39.5%)
07/22 09:36:32 PM | Valid: [18/50] Step 100/390 Loss 3.635 Prec@(1,5) (15.2%, 39.4%)
07/22 09:36:52 PM | Valid: [18/50] Step 150/390 Loss 3.629 Prec@(1,5) (15.1%, 39.5%)
07/22 09:37:12 PM | Valid: [18/50] Step 200/390 Loss 3.626 Prec@(1,5) (15.4%, 39.6%)
07/22 09:37:32 PM | Valid: [18/50] Step 250/390 Loss 3.622 Prec@(1,5) (15.6%, 39.9%)
07/22 09:37:52 PM | Valid: [18/50] Step 300/390 Loss 3.622 Prec@(1,5) (15.5%, 40.2%)
07/22 09:38:12 PM | Valid: [18/50] Step 350/390 Loss 3.620 Prec@(1,5) (15.6%, 40.3%)
07/22 09:38:26 PM | Valid: [18/50] Step 390/390 Loss 3.619 Prec@(1,5) (15.6%, 40.4%)
07/22 09:38:26 PM | Valid: [18/50] Final Prec@1 15.6200%
07/22 09:38:26 PM | genotype = Genotype(normal=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 3)], [('skip_connect', 4), ('max_pool_3x3', 3)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_5x5', 1)], [('dil_conv_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 3), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.2243, 0.1417, 0.1374, 0.1002, 0.1117, 0.1059, 0.1008, 0.0780],
        [0.2286, 0.1332, 0.1374, 0.0825, 0.1133, 0.1109, 0.1000, 0.0939]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.2012, 0.1506, 0.1429, 0.1250, 0.1195, 0.0961, 0.0978, 0.0669],
        [0.1815, 0.1402, 0.1550, 0.1145, 0.1127, 0.0909, 0.1114, 0.0938],
        [0.2108, 0.1582, 0.1568, 0.1111, 0.0852, 0.1000, 0.1059, 0.0720]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.2185, 0.1540, 0.1581, 0.0869, 0.1015, 0.0797, 0.1177, 0.0836],
        [0.1900, 0.1396, 0.1470, 0.0983, 0.1344, 0.0973, 0.0996, 0.0939],
        [0.2058, 0.1585, 0.1584, 0.0944, 0.1052, 0.1014, 0.0951, 0.0812],
        [0.2090, 0.1600, 0.2055, 0.1010, 0.0849, 0.0838, 0.0949, 0.0609]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.2119, 0.1870, 0.1868, 0.0861, 0.0724, 0.0777, 0.1093, 0.0687],
        [0.1883, 0.1353, 0.1456, 0.1137, 0.1037, 0.1195, 0.1002, 0.0937],
        [0.2023, 0.1573, 0.1532, 0.1001, 0.1009, 0.1047, 0.1064, 0.0752],
        [0.2220, 0.1727, 0.2165, 0.0867, 0.0881, 0.0783, 0.0697, 0.0661],
        [0.2068, 0.1543, 0.2505, 0.0650, 0.0829, 0.0869, 0.0868, 0.0668]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1601, 0.1201, 0.1392, 0.1255, 0.1306, 0.0936, 0.1118, 0.1191],
        [0.1245, 0.1037, 0.1144, 0.1389, 0.1210, 0.1349, 0.1432, 0.1195]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1583, 0.1215, 0.1085, 0.1172, 0.1183, 0.1306, 0.1311, 0.1144],
        [0.1146, 0.0962, 0.1318, 0.1273, 0.1262, 0.1549, 0.0974, 0.1515],
        [0.1399, 0.1081, 0.1107, 0.1056, 0.1100, 0.1573, 0.1400, 0.1285]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1640, 0.1220, 0.1102, 0.1114, 0.1122, 0.1080, 0.1263, 0.1459],
        [0.1200, 0.1025, 0.1121, 0.1260, 0.1568, 0.1328, 0.1258, 0.1239],
        [0.1217, 0.0963, 0.0994, 0.1369, 0.1226, 0.1432, 0.1561, 0.1238],
        [0.1614, 0.1231, 0.1352, 0.0887, 0.1060, 0.0960, 0.1463, 0.1432]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1861, 0.1472, 0.1305, 0.0977, 0.0976, 0.1055, 0.1290, 0.1064],
        [0.1050, 0.0970, 0.1504, 0.1069, 0.1252, 0.1443, 0.1391, 0.1320],
        [0.1674, 0.1405, 0.1259, 0.0883, 0.1259, 0.1281, 0.1073, 0.1166],
        [0.1593, 0.1256, 0.1175, 0.1466, 0.1040, 0.1249, 0.1195, 0.1027],
        [0.1577, 0.1234, 0.1205, 0.1140, 0.1299, 0.1226, 0.0961, 0.1357]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/22 09:38:27 PM | size of train loader is 391
07/22 09:38:33 PM | Train: [19/50] Step 000/390 Loss 0.011 Prec@(1,5) (17.2%, 37.5%)
07/22 09:41:24 PM | Train: [19/50] Step 050/390 Loss 0.010 Prec@(1,5) (15.8%, 40.2%)
07/22 09:44:14 PM | Train: [19/50] Step 100/390 Loss 0.009 Prec@(1,5) (15.3%, 40.8%)
07/22 09:47:03 PM | Train: [19/50] Step 150/390 Loss 0.009 Prec@(1,5) (15.4%, 40.7%)
07/22 09:49:56 PM | Train: [19/50] Step 200/390 Loss 0.009 Prec@(1,5) (15.7%, 41.0%)
07/22 09:53:02 PM | Train: [19/50] Step 250/390 Loss 0.009 Prec@(1,5) (15.8%, 40.9%)
07/22 09:56:06 PM | Train: [19/50] Step 300/390 Loss 0.009 Prec@(1,5) (15.7%, 40.8%)
07/22 09:58:59 PM | Train: [19/50] Step 350/390 Loss 0.009 Prec@(1,5) (15.9%, 40.8%)
07/22 10:01:14 PM | Train: [19/50] Step 390/390 Loss 0.009 Prec@(1,5) (15.7%, 40.9%)
07/22 10:01:14 PM | Train: [19/50] Final Prec@1 15.7360%
07/22 10:01:15 PM | Valid: [19/50] Step 000/390 Loss 3.686 Prec@(1,5) (14.1%, 32.8%)
07/22 10:01:35 PM | Valid: [19/50] Step 050/390 Loss 3.571 Prec@(1,5) (16.5%, 41.9%)
07/22 10:01:56 PM | Valid: [19/50] Step 100/390 Loss 3.595 Prec@(1,5) (15.9%, 41.2%)
07/22 10:02:16 PM | Valid: [19/50] Step 150/390 Loss 3.599 Prec@(1,5) (16.1%, 40.8%)
07/22 10:02:36 PM | Valid: [19/50] Step 200/390 Loss 3.606 Prec@(1,5) (16.1%, 40.6%)
07/22 10:02:56 PM | Valid: [19/50] Step 250/390 Loss 3.604 Prec@(1,5) (15.9%, 40.7%)
07/22 10:03:16 PM | Valid: [19/50] Step 300/390 Loss 3.600 Prec@(1,5) (15.8%, 41.0%)
07/22 10:03:36 PM | Valid: [19/50] Step 350/390 Loss 3.601 Prec@(1,5) (15.7%, 40.9%)
07/22 10:03:51 PM | Valid: [19/50] Step 390/390 Loss 3.602 Prec@(1,5) (15.7%, 41.0%)
07/22 10:03:51 PM | Valid: [19/50] Final Prec@1 15.6880%
07/22 10:03:51 PM | genotype = Genotype(normal=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 3)], [('skip_connect', 4), ('max_pool_3x3', 3)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_5x5', 1)], [('dil_conv_3x3', 2), ('dil_conv_3x3', 1)], [('max_pool_3x3', 3), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.2344, 0.1369, 0.1345, 0.1004, 0.1110, 0.1067, 0.1004, 0.0756],
        [0.2367, 0.1282, 0.1337, 0.0824, 0.1142, 0.1110, 0.0982, 0.0955]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.2086, 0.1461, 0.1407, 0.1287, 0.1183, 0.0958, 0.0958, 0.0661],
        [0.1872, 0.1364, 0.1537, 0.1148, 0.1122, 0.0894, 0.1120, 0.0943],
        [0.2171, 0.1542, 0.1552, 0.1102, 0.0842, 0.1011, 0.1068, 0.0711]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.2266, 0.1505, 0.1582, 0.0868, 0.1003, 0.0778, 0.1163, 0.0834],
        [0.1930, 0.1362, 0.1447, 0.0990, 0.1382, 0.0962, 0.0982, 0.0945],
        [0.2103, 0.1559, 0.1574, 0.0921, 0.1044, 0.1031, 0.0953, 0.0816],
        [0.2121, 0.1560, 0.2054, 0.1023, 0.0838, 0.0846, 0.0956, 0.0601]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.2182, 0.1839, 0.1856, 0.0853, 0.0716, 0.0763, 0.1112, 0.0679],
        [0.1895, 0.1303, 0.1415, 0.1128, 0.1054, 0.1240, 0.1010, 0.0955],
        [0.2061, 0.1538, 0.1505, 0.1004, 0.1016, 0.1046, 0.1068, 0.0761],
        [0.2283, 0.1691, 0.2146, 0.0853, 0.0889, 0.0780, 0.0691, 0.0667],
        [0.2116, 0.1499, 0.2515, 0.0645, 0.0820, 0.0861, 0.0874, 0.0670]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1635, 0.1182, 0.1392, 0.1259, 0.1293, 0.0922, 0.1099, 0.1218],
        [0.1255, 0.1019, 0.1163, 0.1382, 0.1207, 0.1358, 0.1439, 0.1176]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1586, 0.1179, 0.1098, 0.1184, 0.1179, 0.1328, 0.1327, 0.1119],
        [0.1159, 0.0948, 0.1320, 0.1271, 0.1273, 0.1548, 0.0958, 0.1524],
        [0.1419, 0.1059, 0.1101, 0.1036, 0.1079, 0.1589, 0.1415, 0.1302]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1657, 0.1198, 0.1093, 0.1100, 0.1119, 0.1072, 0.1295, 0.1466],
        [0.1221, 0.1022, 0.1096, 0.1249, 0.1576, 0.1359, 0.1251, 0.1225],
        [0.1240, 0.0951, 0.0992, 0.1349, 0.1211, 0.1435, 0.1581, 0.1243],
        [0.1664, 0.1225, 0.1345, 0.0878, 0.1042, 0.0951, 0.1475, 0.1420]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1889, 0.1455, 0.1320, 0.0945, 0.0976, 0.1041, 0.1318, 0.1056],
        [0.1038, 0.0948, 0.1529, 0.1057, 0.1245, 0.1453, 0.1404, 0.1328],
        [0.1693, 0.1384, 0.1254, 0.0871, 0.1264, 0.1291, 0.1070, 0.1172],
        [0.1615, 0.1244, 0.1164, 0.1499, 0.1026, 0.1244, 0.1193, 0.1015],
        [0.1602, 0.1223, 0.1199, 0.1127, 0.1298, 0.1219, 0.0946, 0.1387]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/22 10:03:52 PM | size of train loader is 391
07/22 10:03:57 PM | Train: [20/50] Step 000/390 Loss 0.005 Prec@(1,5) (26.6%, 53.1%)
07/22 10:06:50 PM | Train: [20/50] Step 050/390 Loss 0.006 Prec@(1,5) (15.3%, 41.2%)
07/22 10:09:38 PM | Train: [20/50] Step 100/390 Loss 0.008 Prec@(1,5) (15.3%, 41.4%)
07/22 10:12:45 PM | Train: [20/50] Step 150/390 Loss 0.009 Prec@(1,5) (15.4%, 41.0%)
07/22 10:15:41 PM | Train: [20/50] Step 200/390 Loss 0.009 Prec@(1,5) (15.7%, 41.3%)
07/22 10:18:34 PM | Train: [20/50] Step 250/390 Loss 0.009 Prec@(1,5) (15.7%, 41.2%)
07/22 10:21:36 PM | Train: [20/50] Step 300/390 Loss 0.009 Prec@(1,5) (15.8%, 41.1%)
07/22 10:24:41 PM | Train: [20/50] Step 350/390 Loss 0.009 Prec@(1,5) (15.8%, 41.1%)
07/22 10:26:59 PM | Train: [20/50] Step 390/390 Loss 0.009 Prec@(1,5) (15.9%, 41.1%)
07/22 10:26:59 PM | Train: [20/50] Final Prec@1 15.8520%
07/22 10:27:01 PM | Valid: [20/50] Step 000/390 Loss 3.405 Prec@(1,5) (17.2%, 48.4%)
07/22 10:27:21 PM | Valid: [20/50] Step 050/390 Loss 3.563 Prec@(1,5) (16.7%, 42.2%)
07/22 10:27:41 PM | Valid: [20/50] Step 100/390 Loss 3.574 Prec@(1,5) (16.5%, 41.9%)
07/22 10:28:02 PM | Valid: [20/50] Step 150/390 Loss 3.590 Prec@(1,5) (16.1%, 41.4%)
07/22 10:28:22 PM | Valid: [20/50] Step 200/390 Loss 3.577 Prec@(1,5) (16.4%, 41.7%)
07/22 10:28:42 PM | Valid: [20/50] Step 250/390 Loss 3.574 Prec@(1,5) (16.5%, 41.8%)
07/22 10:29:02 PM | Valid: [20/50] Step 300/390 Loss 3.574 Prec@(1,5) (16.4%, 41.8%)
07/22 10:29:22 PM | Valid: [20/50] Step 350/390 Loss 3.574 Prec@(1,5) (16.5%, 41.7%)
07/22 10:29:36 PM | Valid: [20/50] Step 390/390 Loss 3.571 Prec@(1,5) (16.4%, 41.7%)
07/22 10:29:36 PM | Valid: [20/50] Final Prec@1 16.3960%
07/22 10:29:36 PM | genotype = Genotype(normal=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 3)], [('skip_connect', 4), ('max_pool_3x3', 3)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_5x5', 1)], [('dil_conv_3x3', 2), ('dil_conv_3x3', 1)], [('max_pool_3x3', 3), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.2438, 0.1334, 0.1323, 0.1001, 0.1102, 0.1069, 0.1003, 0.0729],
        [0.2441, 0.1246, 0.1310, 0.0817, 0.1134, 0.1103, 0.0975, 0.0975]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.2162, 0.1437, 0.1402, 0.1310, 0.1168, 0.0948, 0.0926, 0.0647],
        [0.1911, 0.1327, 0.1513, 0.1155, 0.1131, 0.0871, 0.1135, 0.0956],
        [0.2206, 0.1517, 0.1549, 0.1097, 0.0827, 0.1014, 0.1081, 0.0708]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.2349, 0.1489, 0.1586, 0.0864, 0.0988, 0.0753, 0.1143, 0.0827],
        [0.1963, 0.1339, 0.1431, 0.0999, 0.1411, 0.0940, 0.0971, 0.0946],
        [0.2136, 0.1545, 0.1562, 0.0903, 0.1038, 0.1059, 0.0946, 0.0812],
        [0.2151, 0.1536, 0.2065, 0.1027, 0.0827, 0.0847, 0.0961, 0.0587]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.2243, 0.1823, 0.1856, 0.0828, 0.0703, 0.0746, 0.1134, 0.0665],
        [0.1893, 0.1263, 0.1377, 0.1152, 0.1060, 0.1268, 0.1018, 0.0970],
        [0.2089, 0.1520, 0.1488, 0.1001, 0.1028, 0.1038, 0.1068, 0.0769],
        [0.2328, 0.1669, 0.2138, 0.0844, 0.0889, 0.0776, 0.0685, 0.0672],
        [0.2159, 0.1479, 0.2540, 0.0631, 0.0809, 0.0844, 0.0869, 0.0669]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1649, 0.1156, 0.1421, 0.1238, 0.1289, 0.0918, 0.1091, 0.1238],
        [0.1248, 0.0993, 0.1180, 0.1396, 0.1200, 0.1380, 0.1442, 0.1161]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1586, 0.1145, 0.1098, 0.1199, 0.1193, 0.1349, 0.1335, 0.1095],
        [0.1154, 0.0923, 0.1305, 0.1287, 0.1295, 0.1550, 0.0952, 0.1534],
        [0.1448, 0.1046, 0.1100, 0.1013, 0.1056, 0.1599, 0.1431, 0.1306]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1671, 0.1176, 0.1087, 0.1095, 0.1119, 0.1052, 0.1323, 0.1477],
        [0.1216, 0.0997, 0.1077, 0.1248, 0.1598, 0.1399, 0.1246, 0.1218],
        [0.1241, 0.0928, 0.0982, 0.1350, 0.1199, 0.1449, 0.1592, 0.1259],
        [0.1697, 0.1215, 0.1332, 0.0871, 0.1032, 0.0940, 0.1493, 0.1418]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1903, 0.1437, 0.1348, 0.0918, 0.0967, 0.1043, 0.1340, 0.1044],
        [0.1023, 0.0927, 0.1531, 0.1040, 0.1236, 0.1470, 0.1426, 0.1346],
        [0.1720, 0.1378, 0.1260, 0.0872, 0.1259, 0.1282, 0.1062, 0.1168],
        [0.1640, 0.1236, 0.1155, 0.1516, 0.1017, 0.1254, 0.1178, 0.1005],
        [0.1625, 0.1216, 0.1197, 0.1125, 0.1296, 0.1209, 0.0926, 0.1407]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/22 10:29:37 PM | size of train loader is 391
07/22 10:29:44 PM | Train: [21/50] Step 000/390 Loss 0.008 Prec@(1,5) (20.3%, 50.0%)
07/22 10:33:01 PM | Train: [21/50] Step 050/390 Loss 0.008 Prec@(1,5) (16.6%, 41.2%)
07/22 10:35:52 PM | Train: [21/50] Step 100/390 Loss 0.009 Prec@(1,5) (17.1%, 42.1%)
07/22 10:38:48 PM | Train: [21/50] Step 150/390 Loss 0.009 Prec@(1,5) (16.7%, 42.0%)
07/22 10:41:37 PM | Train: [21/50] Step 200/390 Loss 0.009 Prec@(1,5) (16.8%, 42.3%)
07/22 10:44:27 PM | Train: [21/50] Step 250/390 Loss 0.009 Prec@(1,5) (16.8%, 42.2%)
07/22 10:47:17 PM | Train: [21/50] Step 300/390 Loss 0.009 Prec@(1,5) (16.6%, 42.4%)
07/22 10:50:10 PM | Train: [21/50] Step 350/390 Loss 0.009 Prec@(1,5) (16.7%, 42.2%)
07/22 10:52:25 PM | Train: [21/50] Step 390/390 Loss 0.009 Prec@(1,5) (16.7%, 42.2%)
07/22 10:52:25 PM | Train: [21/50] Final Prec@1 16.6760%
07/22 10:52:27 PM | Valid: [21/50] Step 000/390 Loss 3.262 Prec@(1,5) (26.6%, 54.7%)
07/22 10:52:47 PM | Valid: [21/50] Step 050/390 Loss 3.565 Prec@(1,5) (16.7%, 41.8%)
07/22 10:53:07 PM | Valid: [21/50] Step 100/390 Loss 3.560 Prec@(1,5) (17.3%, 42.2%)
07/22 10:53:27 PM | Valid: [21/50] Step 150/390 Loss 3.565 Prec@(1,5) (17.1%, 42.2%)
07/22 10:53:47 PM | Valid: [21/50] Step 200/390 Loss 3.557 Prec@(1,5) (17.0%, 42.5%)
07/22 10:54:07 PM | Valid: [21/50] Step 250/390 Loss 3.562 Prec@(1,5) (16.7%, 42.2%)
07/22 10:54:27 PM | Valid: [21/50] Step 300/390 Loss 3.558 Prec@(1,5) (16.7%, 42.4%)
07/22 10:54:49 PM | Valid: [21/50] Step 350/390 Loss 3.556 Prec@(1,5) (16.8%, 42.4%)
07/22 10:55:05 PM | Valid: [21/50] Step 390/390 Loss 3.557 Prec@(1,5) (16.7%, 42.4%)
07/22 10:55:05 PM | Valid: [21/50] Final Prec@1 16.7360%
07/22 10:55:05 PM | genotype = Genotype(normal=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 3)], [('skip_connect', 4), ('max_pool_3x3', 3)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_5x5', 1)], [('dil_conv_3x3', 2), ('max_pool_3x3', 0)], [('max_pool_3x3', 3), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.2520, 0.1293, 0.1299, 0.1001, 0.1099, 0.1072, 0.1002, 0.0714],
        [0.2514, 0.1205, 0.1276, 0.0815, 0.1135, 0.1107, 0.0962, 0.0986]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.2215, 0.1404, 0.1397, 0.1331, 0.1150, 0.0952, 0.0909, 0.0642],
        [0.1977, 0.1304, 0.1509, 0.1152, 0.1114, 0.0857, 0.1135, 0.0952],
        [0.2251, 0.1490, 0.1539, 0.1094, 0.0822, 0.1017, 0.1090, 0.0697]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.2433, 0.1468, 0.1589, 0.0859, 0.0972, 0.0738, 0.1115, 0.0826],
        [0.2001, 0.1317, 0.1419, 0.0997, 0.1439, 0.0931, 0.0953, 0.0943],
        [0.2176, 0.1532, 0.1558, 0.0882, 0.1028, 0.1071, 0.0945, 0.0808],
        [0.2180, 0.1514, 0.2084, 0.1035, 0.0813, 0.0848, 0.0949, 0.0576]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.2303, 0.1804, 0.1856, 0.0814, 0.0693, 0.0733, 0.1141, 0.0656],
        [0.1899, 0.1226, 0.1342, 0.1164, 0.1077, 0.1292, 0.1029, 0.0972],
        [0.2136, 0.1505, 0.1472, 0.0996, 0.1033, 0.1019, 0.1069, 0.0770],
        [0.2385, 0.1649, 0.2136, 0.0829, 0.0886, 0.0769, 0.0677, 0.0669],
        [0.2197, 0.1451, 0.2574, 0.0615, 0.0798, 0.0828, 0.0873, 0.0663]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1684, 0.1143, 0.1427, 0.1234, 0.1277, 0.0911, 0.1075, 0.1249],
        [0.1250, 0.0978, 0.1192, 0.1395, 0.1199, 0.1374, 0.1454, 0.1157]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1604, 0.1129, 0.1100, 0.1210, 0.1186, 0.1372, 0.1331, 0.1068],
        [0.1157, 0.0910, 0.1306, 0.1298, 0.1304, 0.1540, 0.0943, 0.1542],
        [0.1461, 0.1030, 0.1100, 0.0993, 0.1039, 0.1617, 0.1442, 0.1319]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1700, 0.1170, 0.1078, 0.1092, 0.1105, 0.1031, 0.1345, 0.1479],
        [0.1216, 0.0983, 0.1060, 0.1243, 0.1621, 0.1439, 0.1229, 0.1209],
        [0.1254, 0.0915, 0.0978, 0.1336, 0.1189, 0.1453, 0.1614, 0.1262],
        [0.1730, 0.1210, 0.1322, 0.0856, 0.1022, 0.0935, 0.1517, 0.1406]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1928, 0.1426, 0.1347, 0.0903, 0.0957, 0.1046, 0.1361, 0.1031],
        [0.1017, 0.0915, 0.1533, 0.1028, 0.1231, 0.1469, 0.1444, 0.1364],
        [0.1724, 0.1359, 0.1262, 0.0871, 0.1264, 0.1283, 0.1054, 0.1183],
        [0.1652, 0.1223, 0.1145, 0.1543, 0.0997, 0.1261, 0.1180, 0.0999],
        [0.1631, 0.1201, 0.1190, 0.1124, 0.1295, 0.1205, 0.0919, 0.1433]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/22 10:55:07 PM | size of train loader is 391
07/22 10:55:12 PM | Train: [22/50] Step 000/390 Loss 0.009 Prec@(1,5) (20.3%, 34.4%)
07/22 10:58:05 PM | Train: [22/50] Step 050/390 Loss 0.009 Prec@(1,5) (16.5%, 42.5%)
07/22 11:00:59 PM | Train: [22/50] Step 100/390 Loss 0.009 Prec@(1,5) (16.6%, 42.7%)
07/22 11:04:05 PM | Train: [22/50] Step 150/390 Loss 0.009 Prec@(1,5) (16.9%, 42.9%)
07/22 11:06:54 PM | Train: [22/50] Step 200/390 Loss 0.009 Prec@(1,5) (16.9%, 43.0%)
07/22 11:09:47 PM | Train: [22/50] Step 250/390 Loss 0.009 Prec@(1,5) (16.7%, 42.7%)
07/22 11:12:40 PM | Train: [22/50] Step 300/390 Loss 0.009 Prec@(1,5) (16.6%, 42.5%)
07/22 11:15:32 PM | Train: [22/50] Step 350/390 Loss 0.009 Prec@(1,5) (16.8%, 42.8%)
07/22 11:17:52 PM | Train: [22/50] Step 390/390 Loss 0.009 Prec@(1,5) (16.8%, 42.9%)
07/22 11:17:53 PM | Train: [22/50] Final Prec@1 16.7720%
07/22 11:17:54 PM | Valid: [22/50] Step 000/390 Loss 3.474 Prec@(1,5) (23.4%, 46.9%)
07/22 11:18:14 PM | Valid: [22/50] Step 050/390 Loss 3.517 Prec@(1,5) (18.1%, 43.7%)
07/22 11:18:34 PM | Valid: [22/50] Step 100/390 Loss 3.532 Prec@(1,5) (17.3%, 43.5%)
07/22 11:18:54 PM | Valid: [22/50] Step 150/390 Loss 3.539 Prec@(1,5) (17.2%, 43.8%)
07/22 11:19:14 PM | Valid: [22/50] Step 200/390 Loss 3.536 Prec@(1,5) (17.2%, 43.7%)
07/22 11:19:34 PM | Valid: [22/50] Step 250/390 Loss 3.543 Prec@(1,5) (16.8%, 43.3%)
07/22 11:19:54 PM | Valid: [22/50] Step 300/390 Loss 3.542 Prec@(1,5) (16.8%, 43.4%)
07/22 11:20:14 PM | Valid: [22/50] Step 350/390 Loss 3.543 Prec@(1,5) (16.8%, 43.2%)
07/22 11:20:29 PM | Valid: [22/50] Step 390/390 Loss 3.544 Prec@(1,5) (16.8%, 43.1%)
07/22 11:20:29 PM | Valid: [22/50] Final Prec@1 16.8280%
07/22 11:20:29 PM | genotype = Genotype(normal=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 3)], [('skip_connect', 4), ('max_pool_3x3', 3)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_5x5', 1)], [('dil_conv_3x3', 2), ('max_pool_3x3', 0)], [('max_pool_3x3', 3), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.2630, 0.1256, 0.1277, 0.0996, 0.1092, 0.1063, 0.0993, 0.0693],
        [0.2602, 0.1166, 0.1250, 0.0809, 0.1124, 0.1100, 0.0956, 0.0993]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.2300, 0.1368, 0.1383, 0.1346, 0.1140, 0.0946, 0.0885, 0.0633],
        [0.2023, 0.1267, 0.1488, 0.1154, 0.1105, 0.0849, 0.1156, 0.0959],
        [0.2299, 0.1456, 0.1520, 0.1092, 0.0817, 0.1026, 0.1098, 0.0692]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.2519, 0.1438, 0.1589, 0.0854, 0.0962, 0.0715, 0.1098, 0.0826],
        [0.2022, 0.1285, 0.1400, 0.1003, 0.1480, 0.0926, 0.0938, 0.0946],
        [0.2207, 0.1507, 0.1543, 0.0863, 0.1025, 0.1093, 0.0951, 0.0811],
        [0.2200, 0.1478, 0.2081, 0.1054, 0.0815, 0.0855, 0.0947, 0.0571]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.2367, 0.1771, 0.1846, 0.0798, 0.0683, 0.0725, 0.1163, 0.0646],
        [0.1897, 0.1183, 0.1301, 0.1174, 0.1091, 0.1331, 0.1036, 0.0986],
        [0.2158, 0.1472, 0.1448, 0.1004, 0.1039, 0.1019, 0.1082, 0.0778],
        [0.2441, 0.1619, 0.2117, 0.0821, 0.0887, 0.0764, 0.0677, 0.0674],
        [0.2240, 0.1420, 0.2591, 0.0603, 0.0791, 0.0815, 0.0877, 0.0663]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1718, 0.1130, 0.1442, 0.1224, 0.1263, 0.0906, 0.1057, 0.1260],
        [0.1262, 0.0967, 0.1208, 0.1384, 0.1199, 0.1375, 0.1455, 0.1150]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1613, 0.1102, 0.1099, 0.1220, 0.1200, 0.1383, 0.1337, 0.1045],
        [0.1172, 0.0901, 0.1304, 0.1290, 0.1315, 0.1537, 0.0939, 0.1542],
        [0.1475, 0.1013, 0.1098, 0.0977, 0.1025, 0.1627, 0.1455, 0.1331]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1722, 0.1153, 0.1070, 0.1098, 0.1095, 0.1022, 0.1365, 0.1476],
        [0.1234, 0.0977, 0.1041, 0.1234, 0.1638, 0.1460, 0.1216, 0.1201],
        [0.1269, 0.0901, 0.0972, 0.1325, 0.1175, 0.1461, 0.1635, 0.1264],
        [0.1773, 0.1205, 0.1324, 0.0838, 0.1011, 0.0915, 0.1537, 0.1399]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1933, 0.1401, 0.1368, 0.0890, 0.0954, 0.1037, 0.1407, 0.1010],
        [0.0998, 0.0892, 0.1555, 0.1021, 0.1221, 0.1464, 0.1474, 0.1374],
        [0.1706, 0.1332, 0.1262, 0.0867, 0.1277, 0.1292, 0.1056, 0.1210],
        [0.1640, 0.1195, 0.1120, 0.1589, 0.0990, 0.1280, 0.1184, 0.1001],
        [0.1623, 0.1179, 0.1174, 0.1125, 0.1310, 0.1205, 0.0901, 0.1483]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/22 11:20:30 PM | size of train loader is 391
07/22 11:20:36 PM | Train: [23/50] Step 000/390 Loss 0.010 Prec@(1,5) (9.4%, 37.5%)
07/22 11:23:44 PM | Train: [23/50] Step 050/390 Loss 0.009 Prec@(1,5) (16.6%, 43.0%)
07/22 11:26:53 PM | Train: [23/50] Step 100/390 Loss 0.009 Prec@(1,5) (17.0%, 43.3%)
07/22 11:30:01 PM | Train: [23/50] Step 150/390 Loss 0.009 Prec@(1,5) (16.9%, 43.1%)
07/22 11:33:17 PM | Train: [23/50] Step 200/390 Loss 0.009 Prec@(1,5) (17.2%, 43.6%)
07/22 11:36:34 PM | Train: [23/50] Step 250/390 Loss 0.009 Prec@(1,5) (17.2%, 43.7%)
07/22 11:39:37 PM | Train: [23/50] Step 300/390 Loss 0.009 Prec@(1,5) (17.3%, 43.7%)
07/22 11:42:33 PM | Train: [23/50] Step 350/390 Loss 0.009 Prec@(1,5) (17.3%, 43.5%)
07/22 11:45:00 PM | Train: [23/50] Step 390/390 Loss 0.009 Prec@(1,5) (17.2%, 43.5%)
07/22 11:45:00 PM | Train: [23/50] Final Prec@1 17.2440%
07/22 11:45:01 PM | Valid: [23/50] Step 000/390 Loss 3.577 Prec@(1,5) (10.9%, 43.8%)
07/22 11:45:22 PM | Valid: [23/50] Step 050/390 Loss 3.553 Prec@(1,5) (16.3%, 43.1%)
07/22 11:45:43 PM | Valid: [23/50] Step 100/390 Loss 3.548 Prec@(1,5) (17.2%, 42.5%)
07/22 11:46:05 PM | Valid: [23/50] Step 150/390 Loss 3.545 Prec@(1,5) (17.4%, 42.8%)
07/22 11:46:26 PM | Valid: [23/50] Step 200/390 Loss 3.542 Prec@(1,5) (17.4%, 42.8%)
07/22 11:46:46 PM | Valid: [23/50] Step 250/390 Loss 3.536 Prec@(1,5) (17.5%, 43.0%)
07/22 11:47:07 PM | Valid: [23/50] Step 300/390 Loss 3.531 Prec@(1,5) (17.5%, 43.3%)
07/22 11:47:28 PM | Valid: [23/50] Step 350/390 Loss 3.532 Prec@(1,5) (17.4%, 43.2%)
07/22 11:47:44 PM | Valid: [23/50] Step 390/390 Loss 3.530 Prec@(1,5) (17.4%, 43.3%)
07/22 11:47:44 PM | Valid: [23/50] Final Prec@1 17.4040%
07/22 11:47:44 PM | genotype = Genotype(normal=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 2), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 3)], [('skip_connect', 4), ('max_pool_3x3', 3)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_5x5', 1)], [('dil_conv_3x3', 2), ('max_pool_3x3', 0)], [('max_pool_3x3', 3), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.2734, 0.1209, 0.1248, 0.0998, 0.1083, 0.1063, 0.0990, 0.0675],
        [0.2665, 0.1121, 0.1215, 0.0813, 0.1115, 0.1107, 0.0956, 0.1009]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.2383, 0.1329, 0.1368, 0.1358, 0.1131, 0.0941, 0.0862, 0.0628],
        [0.2069, 0.1234, 0.1472, 0.1155, 0.1095, 0.0840, 0.1170, 0.0965],
        [0.2342, 0.1418, 0.1498, 0.1095, 0.0811, 0.1029, 0.1123, 0.0685]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.2612, 0.1399, 0.1577, 0.0854, 0.0955, 0.0700, 0.1078, 0.0825],
        [0.2054, 0.1258, 0.1382, 0.1014, 0.1503, 0.0913, 0.0927, 0.0949],
        [0.2246, 0.1483, 0.1524, 0.0849, 0.1026, 0.1112, 0.0953, 0.0808],
        [0.2222, 0.1447, 0.2084, 0.1064, 0.0810, 0.0864, 0.0945, 0.0565]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.2457, 0.1739, 0.1831, 0.0779, 0.0669, 0.0712, 0.1178, 0.0636],
        [0.1898, 0.1142, 0.1261, 0.1186, 0.1108, 0.1366, 0.1041, 0.0998],
        [0.2214, 0.1454, 0.1432, 0.0992, 0.1044, 0.1003, 0.1082, 0.0780],
        [0.2514, 0.1591, 0.2101, 0.0807, 0.0885, 0.0755, 0.0673, 0.0674],
        [0.2304, 0.1391, 0.2615, 0.0592, 0.0777, 0.0795, 0.0869, 0.0657]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1762, 0.1120, 0.1442, 0.1214, 0.1255, 0.0901, 0.1034, 0.1272],
        [0.1260, 0.0949, 0.1233, 0.1376, 0.1196, 0.1375, 0.1468, 0.1144]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1633, 0.1083, 0.1099, 0.1242, 0.1186, 0.1410, 0.1334, 0.1014],
        [0.1176, 0.0886, 0.1310, 0.1292, 0.1326, 0.1521, 0.0933, 0.1555],
        [0.1494, 0.1002, 0.1104, 0.0950, 0.1004, 0.1650, 0.1448, 0.1348]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1737, 0.1132, 0.1062, 0.1090, 0.1093, 0.1013, 0.1403, 0.1469],
        [0.1232, 0.0960, 0.1025, 0.1233, 0.1663, 0.1483, 0.1207, 0.1197],
        [0.1275, 0.0883, 0.0967, 0.1316, 0.1164, 0.1472, 0.1651, 0.1273],
        [0.1797, 0.1183, 0.1306, 0.0832, 0.1009, 0.0915, 0.1551, 0.1406]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1961, 0.1385, 0.1380, 0.0874, 0.0947, 0.1040, 0.1417, 0.0996],
        [0.0988, 0.0876, 0.1573, 0.1018, 0.1213, 0.1461, 0.1488, 0.1384],
        [0.1719, 0.1320, 0.1272, 0.0865, 0.1284, 0.1274, 0.1044, 0.1223],
        [0.1653, 0.1179, 0.1106, 0.1618, 0.0979, 0.1278, 0.1192, 0.0994],
        [0.1646, 0.1170, 0.1172, 0.1130, 0.1304, 0.1190, 0.0883, 0.1506]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/22 11:47:45 PM | size of train loader is 391
07/22 11:47:50 PM | Train: [24/50] Step 000/390 Loss 0.009 Prec@(1,5) (21.9%, 45.3%)
07/22 11:50:50 PM | Train: [24/50] Step 050/390 Loss 0.009 Prec@(1,5) (17.0%, 42.7%)
07/22 11:54:08 PM | Train: [24/50] Step 100/390 Loss 0.009 Prec@(1,5) (17.5%, 43.4%)
07/22 11:57:12 PM | Train: [24/50] Step 150/390 Loss 0.009 Prec@(1,5) (17.7%, 43.7%)
07/23 12:00:29 AM | Train: [24/50] Step 200/390 Loss 0.009 Prec@(1,5) (18.0%, 44.0%)
07/23 12:03:24 AM | Train: [24/50] Step 250/390 Loss 0.009 Prec@(1,5) (17.7%, 44.0%)
07/23 12:06:19 AM | Train: [24/50] Step 300/390 Loss 0.009 Prec@(1,5) (17.6%, 43.9%)
07/23 12:09:21 AM | Train: [24/50] Step 350/390 Loss 0.009 Prec@(1,5) (17.5%, 44.0%)
07/23 12:11:41 AM | Train: [24/50] Step 390/390 Loss 0.009 Prec@(1,5) (17.6%, 44.2%)
07/23 12:11:41 AM | Train: [24/50] Final Prec@1 17.5920%
07/23 12:11:43 AM | Valid: [24/50] Step 000/390 Loss 3.494 Prec@(1,5) (10.9%, 42.2%)
07/23 12:12:03 AM | Valid: [24/50] Step 050/390 Loss 3.505 Prec@(1,5) (17.2%, 43.3%)
07/23 12:12:23 AM | Valid: [24/50] Step 100/390 Loss 3.504 Prec@(1,5) (17.4%, 43.8%)
07/23 12:12:43 AM | Valid: [24/50] Step 150/390 Loss 3.500 Prec@(1,5) (17.4%, 44.0%)
07/23 12:13:04 AM | Valid: [24/50] Step 200/390 Loss 3.494 Prec@(1,5) (17.9%, 44.2%)
07/23 12:13:26 AM | Valid: [24/50] Step 250/390 Loss 3.493 Prec@(1,5) (17.6%, 44.3%)
07/23 12:13:48 AM | Valid: [24/50] Step 300/390 Loss 3.491 Prec@(1,5) (17.8%, 44.2%)
07/23 12:14:09 AM | Valid: [24/50] Step 350/390 Loss 3.492 Prec@(1,5) (17.7%, 44.1%)
07/23 12:14:24 AM | Valid: [24/50] Step 390/390 Loss 3.491 Prec@(1,5) (17.7%, 44.1%)
07/23 12:14:24 AM | Valid: [24/50] Final Prec@1 17.7320%
07/23 12:14:24 AM | genotype = Genotype(normal=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)], [('max_pool_3x3', 0), ('max_pool_3x3', 3)], [('skip_connect', 4), ('max_pool_3x3', 3)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_5x5', 1)], [('dil_conv_3x3', 2), ('max_pool_3x3', 0)], [('max_pool_3x3', 3), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.2830, 0.1162, 0.1217, 0.0997, 0.1076, 0.1072, 0.0985, 0.0660],
        [0.2737, 0.1083, 0.1186, 0.0823, 0.1108, 0.1100, 0.0947, 0.1018]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.2469, 0.1286, 0.1347, 0.1377, 0.1121, 0.0930, 0.0846, 0.0624],
        [0.2121, 0.1200, 0.1452, 0.1161, 0.1087, 0.0832, 0.1181, 0.0966],
        [0.2391, 0.1387, 0.1487, 0.1085, 0.0806, 0.1046, 0.1121, 0.0678]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.2696, 0.1362, 0.1575, 0.0852, 0.0947, 0.0687, 0.1054, 0.0825],
        [0.2099, 0.1236, 0.1368, 0.1024, 0.1524, 0.0905, 0.0906, 0.0939],
        [0.2296, 0.1465, 0.1514, 0.0830, 0.1009, 0.1135, 0.0946, 0.0805],
        [0.2259, 0.1418, 0.2091, 0.1073, 0.0802, 0.0858, 0.0947, 0.0552]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.2539, 0.1704, 0.1810, 0.0759, 0.0664, 0.0703, 0.1193, 0.0628],
        [0.1890, 0.1106, 0.1226, 0.1186, 0.1138, 0.1390, 0.1057, 0.1008],
        [0.2257, 0.1436, 0.1417, 0.0986, 0.1046, 0.0995, 0.1078, 0.0784],
        [0.2558, 0.1558, 0.2083, 0.0804, 0.0889, 0.0761, 0.0670, 0.0677],
        [0.2338, 0.1360, 0.2635, 0.0587, 0.0772, 0.0781, 0.0870, 0.0657]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1786, 0.1104, 0.1458, 0.1193, 0.1247, 0.0892, 0.1023, 0.1296],
        [0.1277, 0.0947, 0.1243, 0.1368, 0.1198, 0.1368, 0.1467, 0.1131]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1647, 0.1063, 0.1100, 0.1247, 0.1188, 0.1427, 0.1336, 0.0992],
        [0.1186, 0.0877, 0.1305, 0.1284, 0.1325, 0.1522, 0.0934, 0.1566],
        [0.1507, 0.0987, 0.1102, 0.0932, 0.0991, 0.1665, 0.1454, 0.1362]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1766, 0.1125, 0.1051, 0.1094, 0.1076, 0.0994, 0.1437, 0.1456],
        [0.1239, 0.0951, 0.1007, 0.1217, 0.1684, 0.1509, 0.1200, 0.1193],
        [0.1292, 0.0874, 0.0970, 0.1302, 0.1155, 0.1472, 0.1662, 0.1272],
        [0.1839, 0.1179, 0.1299, 0.0820, 0.1002, 0.0904, 0.1562, 0.1394]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1967, 0.1363, 0.1395, 0.0862, 0.0942, 0.1038, 0.1441, 0.0991],
        [0.0997, 0.0880, 0.1581, 0.1015, 0.1199, 0.1435, 0.1496, 0.1398],
        [0.1724, 0.1308, 0.1281, 0.0857, 0.1291, 0.1263, 0.1039, 0.1238],
        [0.1670, 0.1170, 0.1098, 0.1632, 0.0968, 0.1279, 0.1197, 0.0986],
        [0.1657, 0.1161, 0.1168, 0.1136, 0.1303, 0.1182, 0.0867, 0.1525]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/23 12:14:25 AM | size of train loader is 391
07/23 12:14:31 AM | Train: [25/50] Step 000/390 Loss 0.008 Prec@(1,5) (29.7%, 57.8%)
07/23 12:17:48 AM | Train: [25/50] Step 050/390 Loss 0.008 Prec@(1,5) (18.0%, 45.4%)
07/23 12:20:42 AM | Train: [25/50] Step 100/390 Loss 0.009 Prec@(1,5) (18.1%, 44.8%)
07/23 12:23:34 AM | Train: [25/50] Step 150/390 Loss 0.009 Prec@(1,5) (18.0%, 44.8%)
07/23 12:26:31 AM | Train: [25/50] Step 200/390 Loss 0.009 Prec@(1,5) (17.8%, 44.7%)
07/23 12:29:22 AM | Train: [25/50] Step 250/390 Loss 0.009 Prec@(1,5) (17.8%, 44.7%)
07/23 12:32:13 AM | Train: [25/50] Step 300/390 Loss 0.009 Prec@(1,5) (17.7%, 44.7%)
07/23 12:35:03 AM | Train: [25/50] Step 350/390 Loss 0.009 Prec@(1,5) (17.9%, 44.9%)
07/23 12:37:16 AM | Train: [25/50] Step 390/390 Loss 0.009 Prec@(1,5) (17.8%, 44.7%)
07/23 12:37:16 AM | Train: [25/50] Final Prec@1 17.7720%
07/23 12:37:18 AM | Valid: [25/50] Step 000/390 Loss 3.406 Prec@(1,5) (23.4%, 53.1%)
07/23 12:37:39 AM | Valid: [25/50] Step 050/390 Loss 3.471 Prec@(1,5) (17.1%, 44.0%)
07/23 12:37:59 AM | Valid: [25/50] Step 100/390 Loss 3.460 Prec@(1,5) (17.8%, 44.5%)
07/23 12:38:19 AM | Valid: [25/50] Step 150/390 Loss 3.462 Prec@(1,5) (18.0%, 44.6%)
07/23 12:38:39 AM | Valid: [25/50] Step 200/390 Loss 3.463 Prec@(1,5) (18.0%, 44.6%)
07/23 12:38:59 AM | Valid: [25/50] Step 250/390 Loss 3.460 Prec@(1,5) (18.1%, 44.8%)
07/23 12:39:19 AM | Valid: [25/50] Step 300/390 Loss 3.459 Prec@(1,5) (18.2%, 45.1%)
07/23 12:39:39 AM | Valid: [25/50] Step 350/390 Loss 3.462 Prec@(1,5) (18.1%, 45.1%)
07/23 12:39:53 AM | Valid: [25/50] Step 390/390 Loss 3.465 Prec@(1,5) (18.0%, 45.0%)
07/23 12:39:54 AM | Valid: [25/50] Final Prec@1 18.0440%
07/23 12:39:54 AM | genotype = Genotype(normal=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)], [('max_pool_3x3', 0), ('max_pool_3x3', 3)], [('max_pool_3x3', 3), ('skip_connect', 4)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_5x5', 1)], [('dil_conv_3x3', 2), ('max_pool_3x3', 0)], [('max_pool_3x3', 3), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.2923, 0.1120, 0.1193, 0.0993, 0.1068, 0.1071, 0.0988, 0.0643],
        [0.2803, 0.1042, 0.1153, 0.0828, 0.1102, 0.1098, 0.0938, 0.1036]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.2556, 0.1248, 0.1327, 0.1390, 0.1104, 0.0932, 0.0824, 0.0618],
        [0.2170, 0.1166, 0.1432, 0.1170, 0.1079, 0.0823, 0.1184, 0.0976],
        [0.2441, 0.1353, 0.1471, 0.1078, 0.0795, 0.1062, 0.1124, 0.0674]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.2792, 0.1332, 0.1571, 0.0850, 0.0932, 0.0670, 0.1034, 0.0818],
        [0.2122, 0.1211, 0.1349, 0.1042, 0.1546, 0.0901, 0.0888, 0.0943],
        [0.2334, 0.1443, 0.1492, 0.0817, 0.1001, 0.1154, 0.0959, 0.0800],
        [0.2295, 0.1393, 0.2095, 0.1081, 0.0789, 0.0859, 0.0944, 0.0543]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.2625, 0.1672, 0.1795, 0.0743, 0.0658, 0.0686, 0.1205, 0.0616],
        [0.1894, 0.1075, 0.1199, 0.1205, 0.1152, 0.1403, 0.1056, 0.1016],
        [0.2303, 0.1417, 0.1403, 0.0966, 0.1059, 0.0987, 0.1085, 0.0780],
        [0.2624, 0.1533, 0.2077, 0.0786, 0.0878, 0.0761, 0.0664, 0.0676],
        [0.2392, 0.1330, 0.2663, 0.0574, 0.0765, 0.0762, 0.0863, 0.0652]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1814, 0.1089, 0.1464, 0.1188, 0.1237, 0.0883, 0.1015, 0.1311],
        [0.1280, 0.0932, 0.1268, 0.1371, 0.1195, 0.1372, 0.1461, 0.1121]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1662, 0.1045, 0.1096, 0.1255, 0.1189, 0.1443, 0.1335, 0.0974],
        [0.1199, 0.0869, 0.1300, 0.1277, 0.1336, 0.1528, 0.0930, 0.1561],
        [0.1541, 0.0987, 0.1118, 0.0907, 0.0972, 0.1654, 0.1449, 0.1372]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1785, 0.1114, 0.1037, 0.1096, 0.1072, 0.0991, 0.1466, 0.1439],
        [0.1248, 0.0943, 0.0996, 0.1206, 0.1704, 0.1531, 0.1183, 0.1189],
        [0.1308, 0.0866, 0.0967, 0.1287, 0.1145, 0.1491, 0.1667, 0.1269],
        [0.1881, 0.1171, 0.1293, 0.0811, 0.0988, 0.0889, 0.1581, 0.1387]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1960, 0.1333, 0.1416, 0.0855, 0.0944, 0.1035, 0.1469, 0.0989],
        [0.0997, 0.0872, 0.1585, 0.1012, 0.1182, 0.1422, 0.1513, 0.1416],
        [0.1723, 0.1294, 0.1292, 0.0857, 0.1290, 0.1245, 0.1040, 0.1259],
        [0.1668, 0.1151, 0.1081, 0.1646, 0.0967, 0.1288, 0.1209, 0.0990],
        [0.1657, 0.1140, 0.1150, 0.1149, 0.1310, 0.1182, 0.0856, 0.1556]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/23 12:39:55 AM | size of train loader is 391
07/23 12:40:01 AM | Train: [26/50] Step 000/390 Loss 0.009 Prec@(1,5) (31.2%, 40.6%)
07/23 12:42:57 AM | Train: [26/50] Step 050/390 Loss 0.008 Prec@(1,5) (19.2%, 45.4%)
07/23 12:46:16 AM | Train: [26/50] Step 100/390 Loss 0.009 Prec@(1,5) (18.9%, 45.0%)
07/23 12:49:47 AM | Train: [26/50] Step 150/390 Loss 0.009 Prec@(1,5) (18.5%, 45.1%)
07/23 12:53:18 AM | Train: [26/50] Step 200/390 Loss 0.009 Prec@(1,5) (18.4%, 45.0%)
07/23 12:56:44 AM | Train: [26/50] Step 250/390 Loss 0.009 Prec@(1,5) (18.4%, 45.1%)
07/23 01:00:14 AM | Train: [26/50] Step 300/390 Loss 0.009 Prec@(1,5) (18.4%, 45.1%)
07/23 01:03:39 AM | Train: [26/50] Step 350/390 Loss 0.009 Prec@(1,5) (18.6%, 45.1%)
07/23 01:06:26 AM | Train: [26/50] Step 390/390 Loss 0.009 Prec@(1,5) (18.6%, 45.0%)
07/23 01:06:27 AM | Train: [26/50] Final Prec@1 18.5720%
07/23 01:06:28 AM | Valid: [26/50] Step 000/390 Loss 3.318 Prec@(1,5) (15.6%, 54.7%)
07/23 01:06:48 AM | Valid: [26/50] Step 050/390 Loss 3.469 Prec@(1,5) (17.7%, 44.5%)
07/23 01:07:09 AM | Valid: [26/50] Step 100/390 Loss 3.467 Prec@(1,5) (17.7%, 43.9%)
07/23 01:07:29 AM | Valid: [26/50] Step 150/390 Loss 3.474 Prec@(1,5) (17.7%, 43.7%)
07/23 01:07:49 AM | Valid: [26/50] Step 200/390 Loss 3.469 Prec@(1,5) (17.9%, 43.9%)
07/23 01:08:09 AM | Valid: [26/50] Step 250/390 Loss 3.473 Prec@(1,5) (17.9%, 44.0%)
07/23 01:08:29 AM | Valid: [26/50] Step 300/390 Loss 3.474 Prec@(1,5) (17.9%, 44.2%)
07/23 01:08:49 AM | Valid: [26/50] Step 350/390 Loss 3.478 Prec@(1,5) (17.8%, 44.1%)
07/23 01:09:03 AM | Valid: [26/50] Step 390/390 Loss 3.478 Prec@(1,5) (17.8%, 44.1%)
07/23 01:09:03 AM | Valid: [26/50] Final Prec@1 17.8080%
07/23 01:09:03 AM | genotype = Genotype(normal=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)], [('max_pool_3x3', 0), ('max_pool_3x3', 3)], [('max_pool_3x3', 3), ('skip_connect', 4)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_5x5', 1)], [('dil_conv_3x3', 2), ('max_pool_3x3', 0)], [('max_pool_3x3', 3), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.3034, 0.1083, 0.1168, 0.0981, 0.1061, 0.1063, 0.0988, 0.0622],
        [0.2874, 0.1006, 0.1125, 0.0830, 0.1090, 0.1089, 0.0929, 0.1057]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.2651, 0.1218, 0.1311, 0.1399, 0.1088, 0.0922, 0.0805, 0.0607],
        [0.2216, 0.1135, 0.1408, 0.1171, 0.1077, 0.0810, 0.1200, 0.0984],
        [0.2492, 0.1328, 0.1466, 0.1070, 0.0787, 0.1062, 0.1129, 0.0667]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.2898, 0.1298, 0.1562, 0.0842, 0.0924, 0.0656, 0.1009, 0.0812],
        [0.2141, 0.1182, 0.1324, 0.1055, 0.1571, 0.0900, 0.0879, 0.0947],
        [0.2360, 0.1415, 0.1469, 0.0806, 0.0995, 0.1181, 0.0976, 0.0799],
        [0.2324, 0.1366, 0.2096, 0.1087, 0.0785, 0.0873, 0.0933, 0.0536]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.2707, 0.1642, 0.1786, 0.0729, 0.0648, 0.0669, 0.1213, 0.0606],
        [0.1875, 0.1038, 0.1160, 0.1224, 0.1184, 0.1420, 0.1066, 0.1032],
        [0.2337, 0.1398, 0.1387, 0.0962, 0.1068, 0.0983, 0.1081, 0.0784],
        [0.2673, 0.1505, 0.2059, 0.0781, 0.0876, 0.0763, 0.0663, 0.0680],
        [0.2433, 0.1309, 0.2682, 0.0563, 0.0753, 0.0745, 0.0864, 0.0650]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1846, 0.1083, 0.1480, 0.1164, 0.1230, 0.0878, 0.1005, 0.1314],
        [0.1288, 0.0923, 0.1274, 0.1363, 0.1199, 0.1369, 0.1461, 0.1122]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1671, 0.1029, 0.1089, 0.1267, 0.1192, 0.1478, 0.1325, 0.0949],
        [0.1214, 0.0865, 0.1295, 0.1266, 0.1331, 0.1530, 0.0924, 0.1573],
        [0.1564, 0.0984, 0.1131, 0.0881, 0.0950, 0.1657, 0.1446, 0.1387]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1814, 0.1109, 0.1029, 0.1099, 0.1062, 0.0975, 0.1489, 0.1422],
        [0.1259, 0.0938, 0.0974, 0.1189, 0.1730, 0.1544, 0.1175, 0.1190],
        [0.1325, 0.0864, 0.0967, 0.1271, 0.1128, 0.1497, 0.1692, 0.1256],
        [0.1906, 0.1165, 0.1287, 0.0802, 0.0981, 0.0884, 0.1595, 0.1381]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1975, 0.1321, 0.1428, 0.0840, 0.0939, 0.1021, 0.1499, 0.0977],
        [0.0990, 0.0863, 0.1593, 0.1003, 0.1173, 0.1421, 0.1521, 0.1437],
        [0.1731, 0.1286, 0.1295, 0.0859, 0.1308, 0.1225, 0.1022, 0.1274],
        [0.1659, 0.1138, 0.1069, 0.1656, 0.0965, 0.1306, 0.1219, 0.0988],
        [0.1667, 0.1130, 0.1143, 0.1146, 0.1297, 0.1177, 0.0844, 0.1596]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/23 01:09:05 AM | size of train loader is 391
07/23 01:09:10 AM | Train: [27/50] Step 000/390 Loss 0.009 Prec@(1,5) (23.4%, 48.4%)
07/23 01:12:26 AM | Train: [27/50] Step 050/390 Loss 0.008 Prec@(1,5) (18.4%, 46.0%)
07/23 01:15:51 AM | Train: [27/50] Step 100/390 Loss 0.008 Prec@(1,5) (18.0%, 45.7%)
07/23 01:19:06 AM | Train: [27/50] Step 150/390 Loss 0.009 Prec@(1,5) (18.0%, 45.2%)
07/23 01:21:58 AM | Train: [27/50] Step 200/390 Loss 0.009 Prec@(1,5) (18.2%, 45.2%)
07/23 01:24:53 AM | Train: [27/50] Step 250/390 Loss 0.009 Prec@(1,5) (18.4%, 45.4%)
07/23 01:27:49 AM | Train: [27/50] Step 300/390 Loss 0.009 Prec@(1,5) (18.3%, 45.4%)
07/23 01:30:50 AM | Train: [27/50] Step 350/390 Loss 0.009 Prec@(1,5) (18.3%, 45.6%)
07/23 01:33:08 AM | Train: [27/50] Step 390/390 Loss 0.009 Prec@(1,5) (18.5%, 45.5%)
07/23 01:33:08 AM | Train: [27/50] Final Prec@1 18.4600%
07/23 01:33:10 AM | Valid: [27/50] Step 000/390 Loss 3.603 Prec@(1,5) (17.2%, 42.2%)
07/23 01:33:30 AM | Valid: [27/50] Step 050/390 Loss 3.457 Prec@(1,5) (19.4%, 45.0%)
07/23 01:33:50 AM | Valid: [27/50] Step 100/390 Loss 3.447 Prec@(1,5) (19.0%, 45.5%)
07/23 01:34:10 AM | Valid: [27/50] Step 150/390 Loss 3.437 Prec@(1,5) (18.7%, 45.5%)
07/23 01:34:30 AM | Valid: [27/50] Step 200/390 Loss 3.435 Prec@(1,5) (18.4%, 45.6%)
07/23 01:34:50 AM | Valid: [27/50] Step 250/390 Loss 3.433 Prec@(1,5) (18.6%, 45.4%)
07/23 01:35:10 AM | Valid: [27/50] Step 300/390 Loss 3.437 Prec@(1,5) (18.5%, 45.5%)
07/23 01:35:30 AM | Valid: [27/50] Step 350/390 Loss 3.440 Prec@(1,5) (18.5%, 45.4%)
07/23 01:35:45 AM | Valid: [27/50] Step 390/390 Loss 3.439 Prec@(1,5) (18.5%, 45.4%)
07/23 01:35:45 AM | Valid: [27/50] Final Prec@1 18.5000%
07/23 01:35:45 AM | genotype = Genotype(normal=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)], [('max_pool_3x3', 0), ('max_pool_3x3', 3)], [('max_pool_3x3', 3), ('skip_connect', 4)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_5x5', 1)], [('dil_conv_3x3', 2), ('max_pool_3x3', 0)], [('max_pool_3x3', 3), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.3109, 0.1045, 0.1142, 0.0975, 0.1065, 0.1067, 0.0986, 0.0611],
        [0.2931, 0.0977, 0.1104, 0.0834, 0.1080, 0.1080, 0.0931, 0.1063]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.2741, 0.1190, 0.1300, 0.1403, 0.1075, 0.0909, 0.0785, 0.0598],
        [0.2277, 0.1115, 0.1400, 0.1164, 0.1069, 0.0794, 0.1203, 0.0978],
        [0.2557, 0.1311, 0.1464, 0.1048, 0.0777, 0.1067, 0.1123, 0.0653]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.2990, 0.1267, 0.1548, 0.0842, 0.0909, 0.0647, 0.0985, 0.0811],
        [0.2182, 0.1172, 0.1324, 0.1060, 0.1573, 0.0893, 0.0855, 0.0942],
        [0.2407, 0.1403, 0.1456, 0.0788, 0.0985, 0.1192, 0.0981, 0.0788],
        [0.2368, 0.1351, 0.2116, 0.1082, 0.0765, 0.0870, 0.0922, 0.0526]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.2769, 0.1617, 0.1778, 0.0714, 0.0642, 0.0655, 0.1228, 0.0598],
        [0.1874, 0.1013, 0.1136, 0.1244, 0.1206, 0.1422, 0.1062, 0.1044],
        [0.2374, 0.1383, 0.1374, 0.0956, 0.1072, 0.0975, 0.1083, 0.0783],
        [0.2723, 0.1481, 0.2050, 0.0769, 0.0873, 0.0764, 0.0659, 0.0680],
        [0.2475, 0.1288, 0.2707, 0.0555, 0.0744, 0.0728, 0.0855, 0.0648]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1876, 0.1073, 0.1487, 0.1151, 0.1231, 0.0878, 0.0983, 0.1321],
        [0.1292, 0.0914, 0.1298, 0.1352, 0.1194, 0.1371, 0.1463, 0.1115]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1674, 0.1010, 0.1084, 0.1283, 0.1194, 0.1494, 0.1322, 0.0938],
        [0.1234, 0.0864, 0.1286, 0.1246, 0.1337, 0.1526, 0.0925, 0.1580],
        [0.1589, 0.0982, 0.1145, 0.0858, 0.0936, 0.1650, 0.1440, 0.1400]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1835, 0.1098, 0.1027, 0.1101, 0.1047, 0.0971, 0.1520, 0.1400],
        [0.1272, 0.0932, 0.0967, 0.1184, 0.1735, 0.1565, 0.1161, 0.1185],
        [0.1348, 0.0862, 0.0969, 0.1252, 0.1127, 0.1503, 0.1694, 0.1245],
        [0.1950, 0.1159, 0.1275, 0.0788, 0.0976, 0.0869, 0.1613, 0.1370]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1976, 0.1298, 0.1446, 0.0840, 0.0933, 0.1016, 0.1523, 0.0968],
        [0.0988, 0.0856, 0.1601, 0.0992, 0.1156, 0.1419, 0.1534, 0.1455],
        [0.1734, 0.1276, 0.1301, 0.0852, 0.1309, 0.1214, 0.1018, 0.1296],
        [0.1653, 0.1117, 0.1051, 0.1681, 0.0960, 0.1312, 0.1229, 0.0996],
        [0.1674, 0.1115, 0.1130, 0.1149, 0.1302, 0.1160, 0.0842, 0.1628]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/23 01:35:47 AM | size of train loader is 391
07/23 01:35:52 AM | Train: [28/50] Step 000/390 Loss 0.010 Prec@(1,5) (26.6%, 46.9%)
07/23 01:38:51 AM | Train: [28/50] Step 050/390 Loss 0.008 Prec@(1,5) (19.7%, 47.4%)
07/23 01:41:53 AM | Train: [28/50] Step 100/390 Loss 0.008 Prec@(1,5) (20.1%, 46.7%)
07/23 01:44:49 AM | Train: [28/50] Step 150/390 Loss 0.008 Prec@(1,5) (19.6%, 46.7%)
07/23 01:47:51 AM | Train: [28/50] Step 200/390 Loss 0.009 Prec@(1,5) (19.3%, 45.9%)
07/23 01:51:23 AM | Train: [28/50] Step 250/390 Loss 0.009 Prec@(1,5) (19.2%, 45.9%)
07/23 01:54:59 AM | Train: [28/50] Step 300/390 Loss 0.009 Prec@(1,5) (19.2%, 46.1%)
07/23 01:58:31 AM | Train: [28/50] Step 350/390 Loss 0.009 Prec@(1,5) (19.0%, 46.0%)
07/23 02:01:22 AM | Train: [28/50] Step 390/390 Loss 0.009 Prec@(1,5) (19.0%, 46.0%)
07/23 02:01:22 AM | Train: [28/50] Final Prec@1 18.9720%
07/23 02:01:23 AM | Valid: [28/50] Step 000/390 Loss 3.475 Prec@(1,5) (17.2%, 46.9%)
07/23 02:01:44 AM | Valid: [28/50] Step 050/390 Loss 3.436 Prec@(1,5) (19.0%, 46.0%)
07/23 02:02:04 AM | Valid: [28/50] Step 100/390 Loss 3.435 Prec@(1,5) (18.6%, 46.0%)
07/23 02:02:24 AM | Valid: [28/50] Step 150/390 Loss 3.432 Prec@(1,5) (18.6%, 45.9%)
07/23 02:02:44 AM | Valid: [28/50] Step 200/390 Loss 3.439 Prec@(1,5) (18.5%, 45.7%)
07/23 02:03:04 AM | Valid: [28/50] Step 250/390 Loss 3.435 Prec@(1,5) (18.6%, 45.8%)
07/23 02:03:24 AM | Valid: [28/50] Step 300/390 Loss 3.431 Prec@(1,5) (18.8%, 46.0%)
07/23 02:03:44 AM | Valid: [28/50] Step 350/390 Loss 3.432 Prec@(1,5) (18.7%, 45.8%)
07/23 02:03:58 AM | Valid: [28/50] Step 390/390 Loss 3.429 Prec@(1,5) (18.7%, 45.9%)
07/23 02:03:58 AM | Valid: [28/50] Final Prec@1 18.7360%
07/23 02:03:58 AM | genotype = Genotype(normal=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)], [('max_pool_3x3', 0), ('max_pool_3x3', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_5x5', 1)], [('dil_conv_3x3', 2), ('max_pool_3x3', 0)], [('max_pool_3x3', 3), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.3198, 0.1005, 0.1105, 0.0971, 0.1058, 0.1068, 0.0994, 0.0600],
        [0.3005, 0.0945, 0.1082, 0.0834, 0.1068, 0.1070, 0.0930, 0.1065]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.2828, 0.1152, 0.1272, 0.1409, 0.1072, 0.0897, 0.0773, 0.0596],
        [0.2332, 0.1087, 0.1382, 0.1167, 0.1059, 0.0781, 0.1210, 0.0982],
        [0.2610, 0.1286, 0.1453, 0.1033, 0.0765, 0.1072, 0.1136, 0.0645]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.3096, 0.1243, 0.1544, 0.0830, 0.0894, 0.0632, 0.0960, 0.0801],
        [0.2239, 0.1157, 0.1319, 0.1061, 0.1571, 0.0883, 0.0836, 0.0933],
        [0.2466, 0.1396, 0.1450, 0.0773, 0.0977, 0.1197, 0.0971, 0.0770],
        [0.2418, 0.1336, 0.2143, 0.1080, 0.0744, 0.0865, 0.0902, 0.0512]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.2833, 0.1590, 0.1760, 0.0703, 0.0637, 0.0643, 0.1244, 0.0591],
        [0.1873, 0.0989, 0.1112, 0.1251, 0.1226, 0.1429, 0.1068, 0.1053],
        [0.2409, 0.1374, 0.1372, 0.0945, 0.1081, 0.0964, 0.1075, 0.0780],
        [0.2766, 0.1459, 0.2036, 0.0761, 0.0875, 0.0762, 0.0662, 0.0680],
        [0.2501, 0.1262, 0.2738, 0.0549, 0.0742, 0.0711, 0.0852, 0.0643]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1904, 0.1062, 0.1482, 0.1145, 0.1226, 0.0872, 0.0975, 0.1334],
        [0.1300, 0.0908, 0.1311, 0.1344, 0.1193, 0.1375, 0.1459, 0.1109]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1680, 0.0996, 0.1077, 0.1289, 0.1209, 0.1510, 0.1316, 0.0924],
        [0.1249, 0.0864, 0.1287, 0.1238, 0.1341, 0.1507, 0.0928, 0.1586],
        [0.1618, 0.0982, 0.1160, 0.0840, 0.0917, 0.1630, 0.1443, 0.1410]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1851, 0.1089, 0.1022, 0.1102, 0.1037, 0.0974, 0.1540, 0.1387],
        [0.1291, 0.0937, 0.0960, 0.1173, 0.1733, 0.1577, 0.1153, 0.1176],
        [0.1370, 0.0860, 0.0972, 0.1238, 0.1117, 0.1508, 0.1699, 0.1236],
        [0.1985, 0.1156, 0.1272, 0.0778, 0.0963, 0.0853, 0.1637, 0.1357]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1981, 0.1283, 0.1456, 0.0833, 0.0931, 0.1012, 0.1537, 0.0968],
        [0.0981, 0.0848, 0.1604, 0.0985, 0.1139, 0.1411, 0.1560, 0.1472],
        [0.1735, 0.1267, 0.1302, 0.0852, 0.1304, 0.1203, 0.1023, 0.1314],
        [0.1647, 0.1103, 0.1039, 0.1692, 0.0966, 0.1317, 0.1237, 0.0999],
        [0.1672, 0.1103, 0.1115, 0.1158, 0.1301, 0.1161, 0.0836, 0.1654]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/23 02:04:00 AM | size of train loader is 391
07/23 02:04:05 AM | Train: [29/50] Step 000/390 Loss 0.010 Prec@(1,5) (20.3%, 51.6%)
07/23 02:07:18 AM | Train: [29/50] Step 050/390 Loss 0.008 Prec@(1,5) (19.9%, 47.6%)
07/23 02:10:11 AM | Train: [29/50] Step 100/390 Loss 0.008 Prec@(1,5) (19.6%, 47.3%)
07/23 02:13:01 AM | Train: [29/50] Step 150/390 Loss 0.008 Prec@(1,5) (19.3%, 46.8%)
07/23 02:15:58 AM | Train: [29/50] Step 200/390 Loss 0.008 Prec@(1,5) (19.3%, 46.7%)
07/23 02:18:50 AM | Train: [29/50] Step 250/390 Loss 0.008 Prec@(1,5) (19.2%, 46.4%)
07/23 02:21:38 AM | Train: [29/50] Step 300/390 Loss 0.009 Prec@(1,5) (19.2%, 46.5%)
07/23 02:24:28 AM | Train: [29/50] Step 350/390 Loss 0.009 Prec@(1,5) (19.1%, 46.4%)
07/23 02:26:40 AM | Train: [29/50] Step 390/390 Loss 0.009 Prec@(1,5) (19.1%, 46.3%)
07/23 02:26:41 AM | Train: [29/50] Final Prec@1 19.1400%
07/23 02:26:42 AM | Valid: [29/50] Step 000/390 Loss 3.491 Prec@(1,5) (17.2%, 45.3%)
07/23 02:27:02 AM | Valid: [29/50] Step 050/390 Loss 3.420 Prec@(1,5) (18.5%, 46.4%)
07/23 02:27:22 AM | Valid: [29/50] Step 100/390 Loss 3.412 Prec@(1,5) (18.8%, 46.2%)
07/23 02:27:43 AM | Valid: [29/50] Step 150/390 Loss 3.410 Prec@(1,5) (18.9%, 46.3%)
07/23 02:28:03 AM | Valid: [29/50] Step 200/390 Loss 3.411 Prec@(1,5) (19.0%, 46.5%)
07/23 02:28:23 AM | Valid: [29/50] Step 250/390 Loss 3.411 Prec@(1,5) (18.8%, 46.5%)
07/23 02:28:44 AM | Valid: [29/50] Step 300/390 Loss 3.409 Prec@(1,5) (18.8%, 46.5%)
07/23 02:29:04 AM | Valid: [29/50] Step 350/390 Loss 3.407 Prec@(1,5) (18.9%, 46.6%)
07/23 02:29:18 AM | Valid: [29/50] Step 390/390 Loss 3.411 Prec@(1,5) (18.9%, 46.5%)
07/23 02:29:19 AM | Valid: [29/50] Final Prec@1 18.8520%
07/23 02:29:19 AM | genotype = Genotype(normal=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)], [('max_pool_3x3', 0), ('max_pool_3x3', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_5x5', 1)], [('dil_conv_3x3', 2), ('max_pool_3x3', 0)], [('max_pool_3x3', 3), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.3294, 0.0972, 0.1082, 0.0960, 0.1049, 0.1061, 0.0994, 0.0588],
        [0.3082, 0.0919, 0.1068, 0.0832, 0.1056, 0.1053, 0.0917, 0.1073]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.2907, 0.1121, 0.1250, 0.1415, 0.1060, 0.0896, 0.0761, 0.0591],
        [0.2382, 0.1065, 0.1376, 0.1157, 0.1049, 0.0769, 0.1224, 0.0979],
        [0.2646, 0.1264, 0.1451, 0.1023, 0.0760, 0.1072, 0.1141, 0.0643]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.3200, 0.1221, 0.1541, 0.0814, 0.0881, 0.0619, 0.0937, 0.0788],
        [0.2281, 0.1140, 0.1314, 0.1064, 0.1573, 0.0879, 0.0826, 0.0923],
        [0.2519, 0.1387, 0.1449, 0.0758, 0.0959, 0.1210, 0.0961, 0.0758],
        [0.2458, 0.1324, 0.2179, 0.1072, 0.0723, 0.0857, 0.0885, 0.0501]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.2912, 0.1565, 0.1743, 0.0690, 0.0631, 0.0627, 0.1249, 0.0581],
        [0.1871, 0.0968, 0.1093, 0.1258, 0.1244, 0.1437, 0.1071, 0.1056],
        [0.2447, 0.1363, 0.1369, 0.0934, 0.1082, 0.0961, 0.1067, 0.0777],
        [0.2817, 0.1445, 0.2043, 0.0744, 0.0868, 0.0755, 0.0653, 0.0675],
        [0.2524, 0.1237, 0.2783, 0.0543, 0.0731, 0.0696, 0.0848, 0.0637]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1940, 0.1057, 0.1497, 0.1124, 0.1218, 0.0861, 0.0969, 0.1334],
        [0.1297, 0.0894, 0.1329, 0.1324, 0.1209, 0.1378, 0.1463, 0.1108]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1676, 0.0974, 0.1079, 0.1297, 0.1215, 0.1540, 0.1312, 0.0907],
        [0.1249, 0.0851, 0.1283, 0.1225, 0.1360, 0.1516, 0.0925, 0.1592],
        [0.1631, 0.0971, 0.1164, 0.0825, 0.0900, 0.1634, 0.1443, 0.1431]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1863, 0.1076, 0.1018, 0.1098, 0.1027, 0.0965, 0.1582, 0.1372],
        [0.1293, 0.0927, 0.0946, 0.1169, 0.1761, 0.1582, 0.1151, 0.1171],
        [0.1380, 0.0850, 0.0966, 0.1223, 0.1113, 0.1519, 0.1712, 0.1236],
        [0.2011, 0.1146, 0.1257, 0.0771, 0.0960, 0.0849, 0.1662, 0.1344]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1986, 0.1266, 0.1470, 0.0826, 0.0922, 0.1011, 0.1550, 0.0968],
        [0.0967, 0.0832, 0.1612, 0.0987, 0.1125, 0.1408, 0.1575, 0.1494],
        [0.1720, 0.1251, 0.1296, 0.0854, 0.1319, 0.1195, 0.1027, 0.1337],
        [0.1634, 0.1086, 0.1025, 0.1702, 0.0968, 0.1334, 0.1247, 0.1004],
        [0.1663, 0.1086, 0.1100, 0.1168, 0.1307, 0.1153, 0.0840, 0.1684]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/23 02:29:20 AM | size of train loader is 391
07/23 02:29:26 AM | Train: [30/50] Step 000/390 Loss 0.012 Prec@(1,5) (18.8%, 46.9%)
07/23 02:32:18 AM | Train: [30/50] Step 050/390 Loss 0.009 Prec@(1,5) (18.3%, 45.8%)
07/23 02:35:08 AM | Train: [30/50] Step 100/390 Loss 0.007 Prec@(1,5) (18.4%, 46.3%)
07/23 02:37:56 AM | Train: [30/50] Step 150/390 Loss 0.007 Prec@(1,5) (18.7%, 47.0%)
07/23 02:40:45 AM | Train: [30/50] Step 200/390 Loss 0.006 Prec@(1,5) (18.5%, 46.6%)
07/23 02:43:33 AM | Train: [30/50] Step 250/390 Loss 0.007 Prec@(1,5) (18.3%, 46.4%)
07/23 02:46:21 AM | Train: [30/50] Step 300/390 Loss 0.007 Prec@(1,5) (18.4%, 46.2%)
07/23 02:49:11 AM | Train: [30/50] Step 350/390 Loss 0.007 Prec@(1,5) (18.3%, 46.0%)
07/23 02:51:37 AM | Train: [30/50] Step 390/390 Loss 0.007 Prec@(1,5) (18.3%, 45.9%)
07/23 02:51:38 AM | Train: [30/50] Final Prec@1 18.2640%
07/23 02:51:39 AM | Valid: [30/50] Step 000/390 Loss 3.522 Prec@(1,5) (15.6%, 50.0%)
07/23 02:51:59 AM | Valid: [30/50] Step 050/390 Loss 3.443 Prec@(1,5) (19.0%, 45.7%)
07/23 02:52:19 AM | Valid: [30/50] Step 100/390 Loss 3.454 Prec@(1,5) (18.0%, 45.0%)
07/23 02:52:39 AM | Valid: [30/50] Step 150/390 Loss 3.452 Prec@(1,5) (17.7%, 45.0%)
07/23 02:52:59 AM | Valid: [30/50] Step 200/390 Loss 3.444 Prec@(1,5) (17.9%, 45.0%)
07/23 02:53:19 AM | Valid: [30/50] Step 250/390 Loss 3.442 Prec@(1,5) (18.1%, 45.0%)
07/23 02:53:40 AM | Valid: [30/50] Step 300/390 Loss 3.432 Prec@(1,5) (18.3%, 45.4%)
07/23 02:54:00 AM | Valid: [30/50] Step 350/390 Loss 3.430 Prec@(1,5) (18.4%, 45.7%)
07/23 02:54:14 AM | Valid: [30/50] Step 390/390 Loss 3.428 Prec@(1,5) (18.4%, 45.7%)
07/23 02:54:14 AM | Valid: [30/50] Final Prec@1 18.4080%
07/23 02:54:14 AM | genotype = Genotype(normal=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)], [('max_pool_3x3', 0), ('max_pool_3x3', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_5x5', 1)], [('max_pool_3x3', 2), ('max_pool_3x3', 0)], [('max_pool_3x3', 3), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.3343, 0.0943, 0.1066, 0.0949, 0.1052, 0.1061, 0.1002, 0.0584],
        [0.3151, 0.0901, 0.1053, 0.0829, 0.1045, 0.1038, 0.0915, 0.1067]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.2977, 0.1096, 0.1238, 0.1415, 0.1052, 0.0886, 0.0749, 0.0589],
        [0.2444, 0.1050, 0.1366, 0.1146, 0.1036, 0.0760, 0.1224, 0.0975],
        [0.2701, 0.1256, 0.1454, 0.1010, 0.0758, 0.1054, 0.1135, 0.0633]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.3264, 0.1202, 0.1533, 0.0807, 0.0873, 0.0614, 0.0925, 0.0782],
        [0.2328, 0.1134, 0.1314, 0.1062, 0.1564, 0.0871, 0.0814, 0.0913],
        [0.2561, 0.1384, 0.1448, 0.0745, 0.0947, 0.1212, 0.0957, 0.0746],
        [0.2482, 0.1310, 0.2207, 0.1069, 0.0707, 0.0863, 0.0870, 0.0493]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.2978, 0.1545, 0.1725, 0.0680, 0.0637, 0.0618, 0.1245, 0.0573],
        [0.1900, 0.0966, 0.1095, 0.1262, 0.1239, 0.1428, 0.1070, 0.1040],
        [0.2504, 0.1372, 0.1383, 0.0921, 0.1072, 0.0943, 0.1046, 0.0759],
        [0.2868, 0.1438, 0.2070, 0.0726, 0.0849, 0.0744, 0.0643, 0.0662],
        [0.2546, 0.1216, 0.2868, 0.0534, 0.0712, 0.0672, 0.0831, 0.0621]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1963, 0.1056, 0.1494, 0.1109, 0.1218, 0.0858, 0.0959, 0.1343],
        [0.1298, 0.0885, 0.1345, 0.1318, 0.1204, 0.1385, 0.1460, 0.1103]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1674, 0.0963, 0.1066, 0.1299, 0.1224, 0.1562, 0.1310, 0.0901],
        [0.1259, 0.0851, 0.1266, 0.1211, 0.1380, 0.1504, 0.0935, 0.1594],
        [0.1648, 0.0969, 0.1162, 0.0812, 0.0896, 0.1636, 0.1452, 0.1424]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1879, 0.1072, 0.1014, 0.1096, 0.1021, 0.0944, 0.1612, 0.1362],
        [0.1299, 0.0920, 0.0943, 0.1170, 0.1763, 0.1599, 0.1139, 0.1168],
        [0.1390, 0.0845, 0.0965, 0.1205, 0.1125, 0.1517, 0.1720, 0.1234],
        [0.2042, 0.1145, 0.1251, 0.0771, 0.0944, 0.0842, 0.1675, 0.1329]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1974, 0.1245, 0.1488, 0.0821, 0.0920, 0.1012, 0.1581, 0.0959],
        [0.0958, 0.0823, 0.1609, 0.0986, 0.1105, 0.1414, 0.1583, 0.1523],
        [0.1719, 0.1245, 0.1300, 0.0851, 0.1325, 0.1185, 0.1022, 0.1353],
        [0.1633, 0.1078, 0.1018, 0.1701, 0.0965, 0.1355, 0.1237, 0.1012],
        [0.1652, 0.1070, 0.1079, 0.1189, 0.1313, 0.1145, 0.0845, 0.1709]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/23 02:54:16 AM | size of train loader is 391
07/23 02:54:23 AM | Train: [31/50] Step 000/390 Loss 0.006 Prec@(1,5) (20.3%, 43.8%)
07/23 02:57:36 AM | Train: [31/50] Step 050/390 Loss 0.007 Prec@(1,5) (18.6%, 45.5%)
07/23 03:00:29 AM | Train: [31/50] Step 100/390 Loss 0.009 Prec@(1,5) (18.5%, 45.7%)
07/23 03:03:20 AM | Train: [31/50] Step 150/390 Loss 0.009 Prec@(1,5) (18.9%, 45.9%)
07/23 03:06:12 AM | Train: [31/50] Step 200/390 Loss 0.009 Prec@(1,5) (19.4%, 46.6%)
07/23 03:09:01 AM | Train: [31/50] Step 250/390 Loss 0.009 Prec@(1,5) (19.6%, 47.0%)
07/23 03:12:05 AM | Train: [31/50] Step 300/390 Loss 0.009 Prec@(1,5) (19.4%, 46.7%)
07/23 03:14:53 AM | Train: [31/50] Step 350/390 Loss 0.009 Prec@(1,5) (19.5%, 46.8%)
07/23 03:17:12 AM | Train: [31/50] Step 390/390 Loss 0.009 Prec@(1,5) (19.3%, 46.7%)
07/23 03:17:12 AM | Train: [31/50] Final Prec@1 19.3480%
07/23 03:17:14 AM | Valid: [31/50] Step 000/390 Loss 3.696 Prec@(1,5) (10.9%, 40.6%)
07/23 03:17:34 AM | Valid: [31/50] Step 050/390 Loss 3.384 Prec@(1,5) (18.9%, 47.8%)
07/23 03:17:55 AM | Valid: [31/50] Step 100/390 Loss 3.395 Prec@(1,5) (18.6%, 47.0%)
07/23 03:18:16 AM | Valid: [31/50] Step 150/390 Loss 3.406 Prec@(1,5) (18.9%, 46.4%)
07/23 03:18:37 AM | Valid: [31/50] Step 200/390 Loss 3.395 Prec@(1,5) (19.1%, 46.9%)
07/23 03:18:58 AM | Valid: [31/50] Step 250/390 Loss 3.388 Prec@(1,5) (19.3%, 47.1%)
07/23 03:19:18 AM | Valid: [31/50] Step 300/390 Loss 3.383 Prec@(1,5) (19.3%, 47.4%)
07/23 03:19:38 AM | Valid: [31/50] Step 350/390 Loss 3.387 Prec@(1,5) (19.4%, 47.3%)
07/23 03:19:53 AM | Valid: [31/50] Step 390/390 Loss 3.389 Prec@(1,5) (19.3%, 47.2%)
07/23 03:19:53 AM | Valid: [31/50] Final Prec@1 19.2720%
07/23 03:19:53 AM | genotype = Genotype(normal=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_5x5', 1)], [('max_pool_3x3', 2), ('max_pool_3x3', 0)], [('max_pool_3x3', 3), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.3428, 0.0917, 0.1053, 0.0931, 0.1039, 0.1054, 0.1002, 0.0576],
        [0.3229, 0.0884, 0.1041, 0.0823, 0.1040, 0.1025, 0.0899, 0.1058]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.3060, 0.1076, 0.1228, 0.1410, 0.1031, 0.0873, 0.0741, 0.0581],
        [0.2498, 0.1037, 0.1361, 0.1147, 0.1021, 0.0745, 0.1225, 0.0966],
        [0.2756, 0.1251, 0.1463, 0.0989, 0.0751, 0.1039, 0.1130, 0.0622]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.3340, 0.1177, 0.1519, 0.0804, 0.0871, 0.0611, 0.0907, 0.0771],
        [0.2383, 0.1132, 0.1319, 0.1049, 0.1551, 0.0864, 0.0803, 0.0898],
        [0.2620, 0.1387, 0.1450, 0.0729, 0.0940, 0.1200, 0.0949, 0.0726],
        [0.2519, 0.1301, 0.2250, 0.1056, 0.0695, 0.0853, 0.0844, 0.0480]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.3045, 0.1525, 0.1707, 0.0675, 0.0637, 0.0603, 0.1242, 0.0566],
        [0.1920, 0.0965, 0.1095, 0.1258, 0.1242, 0.1423, 0.1060, 0.1037],
        [0.2554, 0.1375, 0.1384, 0.0918, 0.1059, 0.0933, 0.1030, 0.0747],
        [0.2923, 0.1429, 0.2085, 0.0712, 0.0830, 0.0737, 0.0635, 0.0650],
        [0.2577, 0.1198, 0.2938, 0.0523, 0.0696, 0.0651, 0.0809, 0.0608]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1986, 0.1049, 0.1504, 0.1100, 0.1218, 0.0848, 0.0948, 0.1348],
        [0.1288, 0.0869, 0.1364, 0.1325, 0.1215, 0.1386, 0.1448, 0.1106]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1674, 0.0952, 0.1060, 0.1304, 0.1240, 0.1582, 0.1296, 0.0892],
        [0.1258, 0.0842, 0.1251, 0.1214, 0.1389, 0.1497, 0.0942, 0.1607],
        [0.1656, 0.0968, 0.1167, 0.0800, 0.0886, 0.1634, 0.1456, 0.1433]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1900, 0.1065, 0.1009, 0.1105, 0.1013, 0.0929, 0.1634, 0.1345],
        [0.1296, 0.0907, 0.0942, 0.1166, 0.1782, 0.1618, 0.1128, 0.1162],
        [0.1398, 0.0844, 0.0965, 0.1193, 0.1125, 0.1530, 0.1718, 0.1227],
        [0.2076, 0.1145, 0.1247, 0.0763, 0.0930, 0.0840, 0.1678, 0.1320]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1969, 0.1228, 0.1504, 0.0823, 0.0929, 0.1006, 0.1592, 0.0948],
        [0.0949, 0.0812, 0.1608, 0.0990, 0.1084, 0.1402, 0.1604, 0.1550],
        [0.1715, 0.1243, 0.1304, 0.0857, 0.1332, 0.1168, 0.1022, 0.1360],
        [0.1626, 0.1068, 0.1009, 0.1694, 0.0960, 0.1385, 0.1237, 0.1020],
        [0.1644, 0.1058, 0.1063, 0.1208, 0.1324, 0.1133, 0.0852, 0.1718]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/23 03:19:54 AM | size of train loader is 391
07/23 03:20:00 AM | Train: [32/50] Step 000/390 Loss 0.008 Prec@(1,5) (18.8%, 50.0%)
07/23 03:23:31 AM | Train: [32/50] Step 050/390 Loss 0.008 Prec@(1,5) (20.3%, 48.5%)
07/23 03:27:02 AM | Train: [32/50] Step 100/390 Loss 0.009 Prec@(1,5) (20.6%, 47.7%)
07/23 03:30:05 AM | Train: [32/50] Step 150/390 Loss 0.009 Prec@(1,5) (20.3%, 47.6%)
07/23 03:33:14 AM | Train: [32/50] Step 200/390 Loss 0.009 Prec@(1,5) (20.4%, 47.8%)
07/23 03:36:10 AM | Train: [32/50] Step 250/390 Loss 0.009 Prec@(1,5) (20.1%, 47.2%)
07/23 03:39:07 AM | Train: [32/50] Step 300/390 Loss 0.009 Prec@(1,5) (20.0%, 47.4%)
07/23 03:42:01 AM | Train: [32/50] Step 350/390 Loss 0.009 Prec@(1,5) (19.9%, 47.5%)
07/23 03:44:13 AM | Train: [32/50] Step 390/390 Loss 0.009 Prec@(1,5) (20.0%, 47.5%)
07/23 03:44:14 AM | Train: [32/50] Final Prec@1 19.9680%
07/23 03:44:15 AM | Valid: [32/50] Step 000/390 Loss 3.312 Prec@(1,5) (17.2%, 51.6%)
07/23 03:44:35 AM | Valid: [32/50] Step 050/390 Loss 3.373 Prec@(1,5) (19.4%, 47.3%)
07/23 03:44:55 AM | Valid: [32/50] Step 100/390 Loss 3.393 Prec@(1,5) (19.0%, 46.7%)
07/23 03:45:16 AM | Valid: [32/50] Step 150/390 Loss 3.375 Prec@(1,5) (19.6%, 47.4%)
07/23 03:45:38 AM | Valid: [32/50] Step 200/390 Loss 3.383 Prec@(1,5) (19.5%, 47.2%)
07/23 03:45:59 AM | Valid: [32/50] Step 250/390 Loss 3.385 Prec@(1,5) (19.7%, 47.1%)
07/23 03:46:21 AM | Valid: [32/50] Step 300/390 Loss 3.380 Prec@(1,5) (19.8%, 47.3%)
07/23 03:46:42 AM | Valid: [32/50] Step 350/390 Loss 3.376 Prec@(1,5) (19.8%, 47.4%)
07/23 03:46:57 AM | Valid: [32/50] Step 390/390 Loss 3.377 Prec@(1,5) (19.9%, 47.4%)
07/23 03:46:57 AM | Valid: [32/50] Final Prec@1 19.8560%
07/23 03:46:57 AM | genotype = Genotype(normal=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)], [('max_pool_3x3', 3), ('skip_connect', 4)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_5x5', 1)], [('max_pool_3x3', 2), ('max_pool_3x3', 0)], [('max_pool_3x3', 3), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.3516, 0.0894, 0.1033, 0.0916, 0.1030, 0.1039, 0.1004, 0.0568],
        [0.3300, 0.0865, 0.1025, 0.0827, 0.1025, 0.1015, 0.0891, 0.1051]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.3136, 0.1057, 0.1212, 0.1406, 0.1014, 0.0866, 0.0734, 0.0576],
        [0.2550, 0.1024, 0.1355, 0.1151, 0.1004, 0.0731, 0.1232, 0.0952],
        [0.2817, 0.1244, 0.1463, 0.0985, 0.0731, 0.1024, 0.1125, 0.0611]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.3418, 0.1159, 0.1506, 0.0792, 0.0868, 0.0603, 0.0891, 0.0762],
        [0.2424, 0.1122, 0.1314, 0.1049, 0.1546, 0.0857, 0.0800, 0.0888],
        [0.2682, 0.1385, 0.1439, 0.0714, 0.0932, 0.1200, 0.0935, 0.0713],
        [0.2556, 0.1289, 0.2279, 0.1044, 0.0682, 0.0850, 0.0830, 0.0471]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.3124, 0.1502, 0.1682, 0.0666, 0.0632, 0.0589, 0.1246, 0.0558],
        [0.1928, 0.0952, 0.1080, 0.1271, 0.1260, 0.1420, 0.1059, 0.1030],
        [0.2606, 0.1373, 0.1380, 0.0910, 0.1056, 0.0925, 0.1017, 0.0733],
        [0.2977, 0.1415, 0.2099, 0.0696, 0.0825, 0.0724, 0.0626, 0.0638],
        [0.2591, 0.1172, 0.3019, 0.0513, 0.0681, 0.0634, 0.0797, 0.0594]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.2004, 0.1043, 0.1515, 0.1093, 0.1221, 0.0839, 0.0931, 0.1354],
        [0.1290, 0.0862, 0.1387, 0.1319, 0.1223, 0.1377, 0.1434, 0.1107]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1671, 0.0942, 0.1054, 0.1308, 0.1247, 0.1593, 0.1296, 0.0890],
        [0.1270, 0.0841, 0.1239, 0.1211, 0.1392, 0.1507, 0.0932, 0.1607],
        [0.1671, 0.0968, 0.1172, 0.0785, 0.0868, 0.1632, 0.1461, 0.1443]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1908, 0.1055, 0.1018, 0.1112, 0.1010, 0.0917, 0.1650, 0.1330],
        [0.1307, 0.0905, 0.0936, 0.1167, 0.1804, 0.1610, 0.1115, 0.1157],
        [0.1412, 0.0843, 0.0963, 0.1186, 0.1116, 0.1528, 0.1739, 0.1214],
        [0.2114, 0.1151, 0.1243, 0.0760, 0.0914, 0.0830, 0.1685, 0.1302]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1968, 0.1215, 0.1511, 0.0824, 0.0922, 0.1000, 0.1620, 0.0940],
        [0.0946, 0.0806, 0.1634, 0.0986, 0.1067, 0.1395, 0.1593, 0.1572],
        [0.1714, 0.1237, 0.1314, 0.0863, 0.1330, 0.1145, 0.1019, 0.1379],
        [0.1618, 0.1059, 0.1001, 0.1705, 0.0957, 0.1390, 0.1242, 0.1028],
        [0.1639, 0.1044, 0.1051, 0.1219, 0.1330, 0.1127, 0.0859, 0.1731]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/23 03:46:58 AM | size of train loader is 391
07/23 03:47:05 AM | Train: [33/50] Step 000/390 Loss 0.008 Prec@(1,5) (17.2%, 50.0%)
07/23 03:50:05 AM | Train: [33/50] Step 050/390 Loss 0.008 Prec@(1,5) (19.7%, 47.5%)
07/23 03:53:01 AM | Train: [33/50] Step 100/390 Loss 0.009 Prec@(1,5) (19.8%, 48.5%)
07/23 03:55:59 AM | Train: [33/50] Step 150/390 Loss 0.009 Prec@(1,5) (19.7%, 48.5%)
07/23 03:58:52 AM | Train: [33/50] Step 200/390 Loss 0.009 Prec@(1,5) (19.9%, 48.5%)
07/23 04:01:45 AM | Train: [33/50] Step 250/390 Loss 0.009 Prec@(1,5) (20.0%, 48.3%)
07/23 04:04:35 AM | Train: [33/50] Step 300/390 Loss 0.009 Prec@(1,5) (19.9%, 48.0%)
07/23 04:07:30 AM | Train: [33/50] Step 350/390 Loss 0.009 Prec@(1,5) (20.1%, 48.2%)
07/23 04:09:47 AM | Train: [33/50] Step 390/390 Loss 0.009 Prec@(1,5) (20.1%, 48.1%)
07/23 04:09:47 AM | Train: [33/50] Final Prec@1 20.1440%
07/23 04:09:49 AM | Valid: [33/50] Step 000/390 Loss 3.167 Prec@(1,5) (20.3%, 50.0%)
07/23 04:10:09 AM | Valid: [33/50] Step 050/390 Loss 3.381 Prec@(1,5) (18.1%, 46.2%)
07/23 04:10:29 AM | Valid: [33/50] Step 100/390 Loss 3.372 Prec@(1,5) (18.9%, 46.7%)
07/23 04:10:51 AM | Valid: [33/50] Step 150/390 Loss 3.355 Prec@(1,5) (19.6%, 47.6%)
07/23 04:11:13 AM | Valid: [33/50] Step 200/390 Loss 3.353 Prec@(1,5) (19.8%, 47.8%)
07/23 04:11:35 AM | Valid: [33/50] Step 250/390 Loss 3.357 Prec@(1,5) (19.6%, 47.7%)
07/23 04:11:56 AM | Valid: [33/50] Step 300/390 Loss 3.355 Prec@(1,5) (19.8%, 47.7%)
07/23 04:12:18 AM | Valid: [33/50] Step 350/390 Loss 3.353 Prec@(1,5) (19.9%, 47.8%)
07/23 04:12:34 AM | Valid: [33/50] Step 390/390 Loss 3.357 Prec@(1,5) (19.7%, 47.6%)
07/23 04:12:34 AM | Valid: [33/50] Final Prec@1 19.7000%
07/23 04:12:34 AM | genotype = Genotype(normal=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)], [('skip_connect', 4), ('max_pool_3x3', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_5x5', 1)], [('max_pool_3x3', 2), ('max_pool_3x3', 0)], [('max_pool_3x3', 3), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.3582, 0.0872, 0.1017, 0.0904, 0.1026, 0.1031, 0.1009, 0.0558],
        [0.3366, 0.0852, 0.1015, 0.0830, 0.1010, 0.0997, 0.0882, 0.1047]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.3206, 0.1039, 0.1199, 0.1410, 0.0992, 0.0857, 0.0730, 0.0568],
        [0.2603, 0.1016, 0.1354, 0.1145, 0.0984, 0.0722, 0.1236, 0.0941],
        [0.2868, 0.1239, 0.1475, 0.0977, 0.0716, 0.1007, 0.1117, 0.0601]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.3498, 0.1143, 0.1502, 0.0785, 0.0860, 0.0598, 0.0864, 0.0751],
        [0.2473, 0.1124, 0.1325, 0.1038, 0.1534, 0.0845, 0.0790, 0.0871],
        [0.2738, 0.1388, 0.1444, 0.0706, 0.0917, 0.1184, 0.0927, 0.0696],
        [0.2584, 0.1281, 0.2329, 0.1033, 0.0666, 0.0837, 0.0810, 0.0460]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.3173, 0.1487, 0.1676, 0.0659, 0.0629, 0.0576, 0.1248, 0.0552],
        [0.1934, 0.0949, 0.1077, 0.1277, 0.1273, 0.1417, 0.1052, 0.1022],
        [0.2639, 0.1371, 0.1381, 0.0900, 0.1047, 0.0926, 0.1013, 0.0724],
        [0.3008, 0.1402, 0.2121, 0.0690, 0.0815, 0.0717, 0.0617, 0.0629],
        [0.2584, 0.1147, 0.3101, 0.0506, 0.0672, 0.0615, 0.0790, 0.0585]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.2043, 0.1043, 0.1521, 0.1078, 0.1214, 0.0832, 0.0919, 0.1350],
        [0.1290, 0.0854, 0.1400, 0.1315, 0.1226, 0.1374, 0.1429, 0.1113]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1677, 0.0936, 0.1037, 0.1310, 0.1265, 0.1616, 0.1291, 0.0868],
        [0.1274, 0.0834, 0.1221, 0.1210, 0.1392, 0.1500, 0.0933, 0.1635],
        [0.1684, 0.0969, 0.1178, 0.0771, 0.0862, 0.1628, 0.1460, 0.1447]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1918, 0.1048, 0.1012, 0.1113, 0.1005, 0.0918, 0.1671, 0.1315],
        [0.1305, 0.0896, 0.0929, 0.1170, 0.1826, 0.1613, 0.1103, 0.1157],
        [0.1416, 0.0839, 0.0952, 0.1170, 0.1127, 0.1543, 0.1755, 0.1198],
        [0.2138, 0.1148, 0.1236, 0.0752, 0.0904, 0.0822, 0.1701, 0.1301]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1966, 0.1201, 0.1510, 0.0823, 0.0920, 0.1004, 0.1641, 0.0935],
        [0.0941, 0.0799, 0.1649, 0.0981, 0.1060, 0.1391, 0.1582, 0.1598],
        [0.1710, 0.1232, 0.1317, 0.0867, 0.1335, 0.1131, 0.1015, 0.1393],
        [0.1598, 0.1045, 0.0990, 0.1717, 0.0957, 0.1404, 0.1253, 0.1036],
        [0.1625, 0.1032, 0.1036, 0.1238, 0.1335, 0.1117, 0.0863, 0.1754]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/23 04:12:36 AM | size of train loader is 391
07/23 04:12:41 AM | Train: [34/50] Step 000/390 Loss 0.008 Prec@(1,5) (18.8%, 40.6%)
07/23 04:16:16 AM | Train: [34/50] Step 050/390 Loss 0.008 Prec@(1,5) (20.0%, 47.2%)
07/23 04:19:52 AM | Train: [34/50] Step 100/390 Loss 0.008 Prec@(1,5) (20.5%, 47.9%)
07/23 04:23:34 AM | Train: [34/50] Step 150/390 Loss 0.009 Prec@(1,5) (20.6%, 48.0%)
07/23 04:27:12 AM | Train: [34/50] Step 200/390 Loss 0.009 Prec@(1,5) (20.8%, 48.4%)
07/23 04:30:48 AM | Train: [34/50] Step 250/390 Loss 0.009 Prec@(1,5) (20.7%, 48.4%)
07/23 04:34:05 AM | Train: [34/50] Step 300/390 Loss 0.009 Prec@(1,5) (20.6%, 48.5%)
07/23 04:37:24 AM | Train: [34/50] Step 350/390 Loss 0.009 Prec@(1,5) (20.6%, 48.6%)
07/23 04:39:46 AM | Train: [34/50] Step 390/390 Loss 0.009 Prec@(1,5) (20.5%, 48.4%)
07/23 04:39:46 AM | Train: [34/50] Final Prec@1 20.4520%
07/23 04:39:48 AM | Valid: [34/50] Step 000/390 Loss 3.392 Prec@(1,5) (18.8%, 45.3%)
07/23 04:40:09 AM | Valid: [34/50] Step 050/390 Loss 3.378 Prec@(1,5) (19.1%, 46.1%)
07/23 04:40:31 AM | Valid: [34/50] Step 100/390 Loss 3.372 Prec@(1,5) (19.6%, 46.5%)
07/23 04:40:53 AM | Valid: [34/50] Step 150/390 Loss 3.359 Prec@(1,5) (19.7%, 46.9%)
07/23 04:41:15 AM | Valid: [34/50] Step 200/390 Loss 3.353 Prec@(1,5) (19.9%, 47.3%)
07/23 04:41:36 AM | Valid: [34/50] Step 250/390 Loss 3.355 Prec@(1,5) (19.8%, 47.2%)
07/23 04:41:58 AM | Valid: [34/50] Step 300/390 Loss 3.345 Prec@(1,5) (20.0%, 47.5%)
07/23 04:42:19 AM | Valid: [34/50] Step 350/390 Loss 3.345 Prec@(1,5) (20.1%, 47.7%)
07/23 04:42:34 AM | Valid: [34/50] Step 390/390 Loss 3.346 Prec@(1,5) (20.0%, 47.6%)
07/23 04:42:34 AM | Valid: [34/50] Final Prec@1 20.0360%
07/23 04:42:34 AM | genotype = Genotype(normal=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)], [('skip_connect', 4), ('max_pool_3x3', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_5x5', 1)], [('max_pool_3x3', 2), ('max_pool_3x3', 0)], [('max_pool_3x3', 3), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.3650, 0.0853, 0.1000, 0.0897, 0.1017, 0.1024, 0.1006, 0.0553],
        [0.3436, 0.0837, 0.1004, 0.0827, 0.0995, 0.0988, 0.0874, 0.1040]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.3267, 0.1022, 0.1190, 0.1411, 0.0976, 0.0846, 0.0723, 0.0564],
        [0.2660, 0.1004, 0.1343, 0.1141, 0.0979, 0.0708, 0.1240, 0.0926],
        [0.2919, 0.1235, 0.1485, 0.0960, 0.0711, 0.0986, 0.1113, 0.0592]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.3562, 0.1134, 0.1500, 0.0778, 0.0851, 0.0596, 0.0842, 0.0738],
        [0.2514, 0.1121, 0.1326, 0.1040, 0.1525, 0.0835, 0.0780, 0.0859],
        [0.2787, 0.1394, 0.1450, 0.0694, 0.0904, 0.1172, 0.0917, 0.0682],
        [0.2605, 0.1278, 0.2377, 0.1022, 0.0652, 0.0827, 0.0788, 0.0451]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.3233, 0.1476, 0.1673, 0.0645, 0.0627, 0.0562, 0.1242, 0.0542],
        [0.1947, 0.0946, 0.1074, 0.1276, 0.1283, 0.1419, 0.1045, 0.1010],
        [0.2680, 0.1373, 0.1381, 0.0888, 0.1043, 0.0921, 0.1006, 0.0706],
        [0.3039, 0.1395, 0.2152, 0.0677, 0.0803, 0.0710, 0.0611, 0.0613],
        [0.2555, 0.1124, 0.3214, 0.0498, 0.0657, 0.0602, 0.0779, 0.0571]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.2068, 0.1039, 0.1521, 0.1068, 0.1212, 0.0826, 0.0912, 0.1354],
        [0.1294, 0.0847, 0.1402, 0.1309, 0.1251, 0.1364, 0.1419, 0.1114]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1692, 0.0935, 0.1026, 0.1320, 0.1263, 0.1620, 0.1286, 0.0857],
        [0.1287, 0.0833, 0.1216, 0.1207, 0.1386, 0.1492, 0.0929, 0.1649],
        [0.1715, 0.0977, 0.1189, 0.0759, 0.0845, 0.1627, 0.1450, 0.1437]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1931, 0.1044, 0.1011, 0.1115, 0.0989, 0.0918, 0.1691, 0.1300],
        [0.1314, 0.0893, 0.0920, 0.1150, 0.1864, 0.1609, 0.1099, 0.1151],
        [0.1429, 0.0838, 0.0951, 0.1152, 0.1131, 0.1546, 0.1760, 0.1192],
        [0.2158, 0.1143, 0.1231, 0.0744, 0.0894, 0.0820, 0.1713, 0.1296]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1962, 0.1183, 0.1527, 0.0823, 0.0925, 0.1000, 0.1647, 0.0933],
        [0.0944, 0.0797, 0.1639, 0.0980, 0.1064, 0.1380, 0.1571, 0.1625],
        [0.1715, 0.1227, 0.1319, 0.0874, 0.1337, 0.1120, 0.1010, 0.1397],
        [0.1590, 0.1035, 0.0989, 0.1699, 0.0960, 0.1424, 0.1254, 0.1050],
        [0.1615, 0.1016, 0.1023, 0.1259, 0.1340, 0.1116, 0.0865, 0.1766]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/23 04:42:36 AM | size of train loader is 391
07/23 04:42:42 AM | Train: [35/50] Step 000/390 Loss 0.009 Prec@(1,5) (18.8%, 40.6%)
07/23 04:46:19 AM | Train: [35/50] Step 050/390 Loss 0.008 Prec@(1,5) (20.7%, 48.6%)
07/23 04:49:50 AM | Train: [35/50] Step 100/390 Loss 0.008 Prec@(1,5) (20.7%, 48.7%)
07/23 04:52:45 AM | Train: [35/50] Step 150/390 Loss 0.008 Prec@(1,5) (20.8%, 48.8%)
07/23 04:55:37 AM | Train: [35/50] Step 200/390 Loss 0.009 Prec@(1,5) (20.6%, 48.8%)
07/23 04:58:27 AM | Train: [35/50] Step 250/390 Loss 0.009 Prec@(1,5) (20.6%, 48.6%)
07/23 05:01:19 AM | Train: [35/50] Step 300/390 Loss 0.009 Prec@(1,5) (20.6%, 48.6%)
07/23 05:04:10 AM | Train: [35/50] Step 350/390 Loss 0.009 Prec@(1,5) (20.4%, 48.4%)
07/23 05:06:25 AM | Train: [35/50] Step 390/390 Loss 0.009 Prec@(1,5) (20.5%, 48.5%)
07/23 05:06:25 AM | Train: [35/50] Final Prec@1 20.5080%
07/23 05:06:27 AM | Valid: [35/50] Step 000/390 Loss 3.300 Prec@(1,5) (23.4%, 48.4%)
07/23 05:06:47 AM | Valid: [35/50] Step 050/390 Loss 3.324 Prec@(1,5) (21.1%, 48.7%)
07/23 05:07:09 AM | Valid: [35/50] Step 100/390 Loss 3.334 Prec@(1,5) (20.2%, 47.9%)
07/23 05:07:29 AM | Valid: [35/50] Step 150/390 Loss 3.348 Prec@(1,5) (20.3%, 47.5%)
07/23 05:07:49 AM | Valid: [35/50] Step 200/390 Loss 3.347 Prec@(1,5) (20.3%, 47.8%)
07/23 05:08:09 AM | Valid: [35/50] Step 250/390 Loss 3.342 Prec@(1,5) (20.4%, 48.0%)
07/23 05:08:29 AM | Valid: [35/50] Step 300/390 Loss 3.345 Prec@(1,5) (20.3%, 47.8%)
07/23 05:08:49 AM | Valid: [35/50] Step 350/390 Loss 3.342 Prec@(1,5) (20.3%, 47.9%)
07/23 05:09:04 AM | Valid: [35/50] Step 390/390 Loss 3.341 Prec@(1,5) (20.3%, 47.9%)
07/23 05:09:04 AM | Valid: [35/50] Final Prec@1 20.2760%
07/23 05:09:04 AM | genotype = Genotype(normal=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)], [('skip_connect', 4), ('max_pool_3x3', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_5x5', 1)], [('max_pool_3x3', 2), ('max_pool_3x3', 0)], [('max_pool_3x3', 3), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.3722, 0.0839, 0.0994, 0.0889, 0.0997, 0.1010, 0.1007, 0.0542],
        [0.3479, 0.0823, 0.0993, 0.0832, 0.0987, 0.0983, 0.0865, 0.1037]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.3348, 0.1014, 0.1185, 0.1395, 0.0956, 0.0832, 0.0714, 0.0555],
        [0.2703, 0.0995, 0.1343, 0.1141, 0.0971, 0.0693, 0.1242, 0.0913],
        [0.2984, 0.1240, 0.1496, 0.0935, 0.0697, 0.0966, 0.1105, 0.0578]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.3657, 0.1129, 0.1503, 0.0762, 0.0835, 0.0585, 0.0807, 0.0721],
        [0.2550, 0.1119, 0.1334, 0.1033, 0.1510, 0.0830, 0.0778, 0.0847],
        [0.2842, 0.1399, 0.1460, 0.0676, 0.0888, 0.1161, 0.0909, 0.0665],
        [0.2630, 0.1268, 0.2435, 0.1016, 0.0636, 0.0809, 0.0766, 0.0440]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.3267, 0.1463, 0.1665, 0.0633, 0.0623, 0.0552, 0.1262, 0.0535],
        [0.1948, 0.0941, 0.1070, 0.1279, 0.1302, 0.1419, 0.1040, 0.1002],
        [0.2708, 0.1376, 0.1390, 0.0875, 0.1039, 0.0915, 0.0998, 0.0699],
        [0.3059, 0.1385, 0.2174, 0.0670, 0.0790, 0.0706, 0.0609, 0.0606],
        [0.2524, 0.1102, 0.3307, 0.0497, 0.0650, 0.0588, 0.0768, 0.0564]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.2068, 0.1023, 0.1523, 0.1068, 0.1217, 0.0824, 0.0911, 0.1366],
        [0.1302, 0.0844, 0.1406, 0.1292, 0.1251, 0.1365, 0.1429, 0.1110]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1686, 0.0923, 0.1019, 0.1333, 0.1257, 0.1646, 0.1282, 0.0855],
        [0.1301, 0.0835, 0.1210, 0.1196, 0.1396, 0.1491, 0.0927, 0.1645],
        [0.1736, 0.0987, 0.1203, 0.0753, 0.0827, 0.1620, 0.1438, 0.1436]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1915, 0.1031, 0.1018, 0.1115, 0.0982, 0.0918, 0.1713, 0.1309],
        [0.1310, 0.0884, 0.0917, 0.1151, 0.1879, 0.1622, 0.1091, 0.1146],
        [0.1418, 0.0830, 0.0944, 0.1144, 0.1138, 0.1561, 0.1777, 0.1189],
        [0.2168, 0.1132, 0.1214, 0.0747, 0.0878, 0.0818, 0.1750, 0.1293]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1952, 0.1164, 0.1539, 0.0825, 0.0923, 0.1003, 0.1659, 0.0934],
        [0.0947, 0.0796, 0.1641, 0.0983, 0.1056, 0.1364, 0.1573, 0.1641],
        [0.1724, 0.1232, 0.1321, 0.0875, 0.1329, 0.1109, 0.1010, 0.1400],
        [0.1595, 0.1034, 0.0987, 0.1696, 0.0954, 0.1433, 0.1251, 0.1049],
        [0.1627, 0.1017, 0.1019, 0.1269, 0.1332, 0.1111, 0.0862, 0.1762]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/23 05:09:05 AM | size of train loader is 391
07/23 05:09:11 AM | Train: [36/50] Step 000/390 Loss 0.009 Prec@(1,5) (17.2%, 45.3%)
07/23 05:12:09 AM | Train: [36/50] Step 050/390 Loss 0.008 Prec@(1,5) (20.1%, 48.8%)
07/23 05:15:08 AM | Train: [36/50] Step 100/390 Loss 0.008 Prec@(1,5) (20.9%, 49.1%)
07/23 05:18:08 AM | Train: [36/50] Step 150/390 Loss 0.008 Prec@(1,5) (20.7%, 49.0%)
07/23 05:21:03 AM | Train: [36/50] Step 200/390 Loss 0.008 Prec@(1,5) (20.7%, 49.3%)
07/23 05:24:02 AM | Train: [36/50] Step 250/390 Loss 0.008 Prec@(1,5) (20.7%, 49.0%)
07/23 05:27:11 AM | Train: [36/50] Step 300/390 Loss 0.009 Prec@(1,5) (20.6%, 49.0%)
07/23 05:30:10 AM | Train: [36/50] Step 350/390 Loss 0.008 Prec@(1,5) (20.8%, 49.2%)
07/23 05:32:28 AM | Train: [36/50] Step 390/390 Loss 0.008 Prec@(1,5) (20.7%, 49.1%)
07/23 05:32:28 AM | Train: [36/50] Final Prec@1 20.7120%
07/23 05:32:30 AM | Valid: [36/50] Step 000/390 Loss 3.321 Prec@(1,5) (18.8%, 48.4%)
07/23 05:32:50 AM | Valid: [36/50] Step 050/390 Loss 3.343 Prec@(1,5) (20.4%, 48.2%)
07/23 05:33:10 AM | Valid: [36/50] Step 100/390 Loss 3.319 Prec@(1,5) (20.8%, 48.6%)
07/23 05:33:30 AM | Valid: [36/50] Step 150/390 Loss 3.315 Prec@(1,5) (20.6%, 48.6%)
07/23 05:33:50 AM | Valid: [36/50] Step 200/390 Loss 3.312 Prec@(1,5) (20.5%, 48.9%)
07/23 05:34:10 AM | Valid: [36/50] Step 250/390 Loss 3.319 Prec@(1,5) (20.4%, 48.7%)
07/23 05:34:30 AM | Valid: [36/50] Step 300/390 Loss 3.318 Prec@(1,5) (20.5%, 48.6%)
07/23 05:34:50 AM | Valid: [36/50] Step 350/390 Loss 3.322 Prec@(1,5) (20.4%, 48.5%)
07/23 05:35:05 AM | Valid: [36/50] Step 390/390 Loss 3.320 Prec@(1,5) (20.4%, 48.4%)
07/23 05:35:05 AM | Valid: [36/50] Final Prec@1 20.4120%
07/23 05:35:05 AM | genotype = Genotype(normal=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)], [('skip_connect', 4), ('max_pool_3x3', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_5x5', 1)], [('max_pool_3x3', 2), ('max_pool_3x3', 0)], [('max_pool_3x3', 3), ('sep_conv_5x5', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.3798, 0.0825, 0.0983, 0.0883, 0.0982, 0.0995, 0.1003, 0.0531],
        [0.3519, 0.0810, 0.0984, 0.0834, 0.0984, 0.0979, 0.0854, 0.1036]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.3430, 0.1003, 0.1176, 0.1387, 0.0937, 0.0822, 0.0699, 0.0547],
        [0.2750, 0.0986, 0.1338, 0.1139, 0.0958, 0.0680, 0.1243, 0.0905],
        [0.3039, 0.1240, 0.1501, 0.0916, 0.0688, 0.0952, 0.1098, 0.0566]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.3751, 0.1117, 0.1500, 0.0751, 0.0826, 0.0576, 0.0775, 0.0704],
        [0.2583, 0.1116, 0.1337, 0.1031, 0.1506, 0.0822, 0.0769, 0.0836],
        [0.2890, 0.1403, 0.1465, 0.0661, 0.0877, 0.1158, 0.0894, 0.0652],
        [0.2630, 0.1258, 0.2496, 0.1009, 0.0624, 0.0803, 0.0748, 0.0431]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.3321, 0.1448, 0.1643, 0.0625, 0.0621, 0.0543, 0.1268, 0.0532],
        [0.1950, 0.0938, 0.1066, 0.1291, 0.1313, 0.1426, 0.1023, 0.0993],
        [0.2726, 0.1375, 0.1396, 0.0872, 0.1035, 0.0907, 0.0997, 0.0692],
        [0.3066, 0.1375, 0.2203, 0.0659, 0.0789, 0.0702, 0.0608, 0.0598],
        [0.2490, 0.1084, 0.3411, 0.0492, 0.0638, 0.0573, 0.0756, 0.0557]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.2091, 0.1020, 0.1536, 0.1051, 0.1199, 0.0818, 0.0907, 0.1378],
        [0.1305, 0.0839, 0.1408, 0.1290, 0.1258, 0.1367, 0.1425, 0.1108]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1679, 0.0913, 0.1009, 0.1343, 0.1262, 0.1665, 0.1282, 0.0847],
        [0.1307, 0.0833, 0.1204, 0.1207, 0.1388, 0.1480, 0.0926, 0.1655],
        [0.1752, 0.0989, 0.1206, 0.0745, 0.0813, 0.1613, 0.1443, 0.1439]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1918, 0.1024, 0.1014, 0.1122, 0.0972, 0.0923, 0.1738, 0.1288],
        [0.1305, 0.0875, 0.0915, 0.1145, 0.1904, 0.1631, 0.1082, 0.1142],
        [0.1421, 0.0825, 0.0940, 0.1129, 0.1133, 0.1572, 0.1792, 0.1188],
        [0.2196, 0.1130, 0.1205, 0.0743, 0.0870, 0.0804, 0.1772, 0.1280]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1945, 0.1148, 0.1556, 0.0828, 0.0925, 0.1005, 0.1664, 0.0929],
        [0.0944, 0.0791, 0.1652, 0.0979, 0.1049, 0.1354, 0.1565, 0.1667],
        [0.1722, 0.1226, 0.1316, 0.0883, 0.1325, 0.1097, 0.1017, 0.1414],
        [0.1588, 0.1026, 0.0985, 0.1698, 0.0952, 0.1440, 0.1254, 0.1058],
        [0.1628, 0.1008, 0.1010, 0.1285, 0.1339, 0.1104, 0.0860, 0.1766]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/23 05:35:06 AM | size of train loader is 391
07/23 05:35:12 AM | Train: [37/50] Step 000/390 Loss 0.009 Prec@(1,5) (25.0%, 56.2%)
07/23 05:38:34 AM | Train: [37/50] Step 050/390 Loss 0.007 Prec@(1,5) (22.4%, 51.2%)
07/23 05:41:48 AM | Train: [37/50] Step 100/390 Loss 0.008 Prec@(1,5) (22.1%, 50.8%)
07/23 05:45:02 AM | Train: [37/50] Step 150/390 Loss 0.008 Prec@(1,5) (21.4%, 49.6%)
07/23 05:47:50 AM | Train: [37/50] Step 200/390 Loss 0.008 Prec@(1,5) (20.9%, 49.5%)
07/23 05:50:36 AM | Train: [37/50] Step 250/390 Loss 0.008 Prec@(1,5) (21.0%, 49.4%)
07/23 05:53:25 AM | Train: [37/50] Step 300/390 Loss 0.009 Prec@(1,5) (20.8%, 49.1%)
07/23 05:56:11 AM | Train: [37/50] Step 350/390 Loss 0.008 Prec@(1,5) (20.9%, 49.3%)
07/23 05:58:22 AM | Train: [37/50] Step 390/390 Loss 0.008 Prec@(1,5) (20.9%, 49.3%)
07/23 05:58:23 AM | Train: [37/50] Final Prec@1 20.8640%
07/23 05:58:24 AM | Valid: [37/50] Step 000/390 Loss 3.674 Prec@(1,5) (10.9%, 39.1%)
07/23 05:58:44 AM | Valid: [37/50] Step 050/390 Loss 3.325 Prec@(1,5) (19.9%, 48.7%)
07/23 05:59:04 AM | Valid: [37/50] Step 100/390 Loss 3.310 Prec@(1,5) (19.9%, 48.2%)
07/23 05:59:24 AM | Valid: [37/50] Step 150/390 Loss 3.307 Prec@(1,5) (20.3%, 48.2%)
07/23 05:59:44 AM | Valid: [37/50] Step 200/390 Loss 3.298 Prec@(1,5) (20.9%, 48.9%)
07/23 06:00:04 AM | Valid: [37/50] Step 250/390 Loss 3.306 Prec@(1,5) (20.6%, 48.7%)
07/23 06:00:24 AM | Valid: [37/50] Step 300/390 Loss 3.311 Prec@(1,5) (20.6%, 48.8%)
07/23 06:00:44 AM | Valid: [37/50] Step 350/390 Loss 3.315 Prec@(1,5) (20.6%, 48.7%)
07/23 06:00:59 AM | Valid: [37/50] Step 390/390 Loss 3.318 Prec@(1,5) (20.6%, 48.6%)
07/23 06:00:59 AM | Valid: [37/50] Final Prec@1 20.5800%
07/23 06:00:59 AM | genotype = Genotype(normal=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)], [('skip_connect', 4), ('max_pool_3x3', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_5x5', 1)], [('max_pool_3x3', 2), ('dil_conv_3x3', 0)], [('max_pool_3x3', 3), ('sep_conv_5x5', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.3883, 0.0814, 0.0975, 0.0878, 0.0958, 0.0976, 0.0997, 0.0520],
        [0.3568, 0.0800, 0.0976, 0.0829, 0.0977, 0.0974, 0.0843, 0.1034]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.3507, 0.0991, 0.1163, 0.1373, 0.0931, 0.0806, 0.0690, 0.0538],
        [0.2787, 0.0974, 0.1327, 0.1143, 0.0950, 0.0672, 0.1250, 0.0898],
        [0.3091, 0.1244, 0.1510, 0.0895, 0.0679, 0.0935, 0.1087, 0.0558]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.3832, 0.1113, 0.1489, 0.0736, 0.0816, 0.0574, 0.0755, 0.0685],
        [0.2619, 0.1116, 0.1335, 0.1028, 0.1503, 0.0813, 0.0761, 0.0824],
        [0.2949, 0.1422, 0.1485, 0.0641, 0.0863, 0.1132, 0.0872, 0.0635],
        [0.2635, 0.1253, 0.2567, 0.0999, 0.0608, 0.0786, 0.0730, 0.0422]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.3376, 0.1448, 0.1636, 0.0616, 0.0620, 0.0527, 0.1252, 0.0525],
        [0.1970, 0.0944, 0.1069, 0.1278, 0.1332, 0.1424, 0.1008, 0.0976],
        [0.2769, 0.1387, 0.1408, 0.0867, 0.1024, 0.0893, 0.0975, 0.0677],
        [0.3093, 0.1374, 0.2253, 0.0639, 0.0771, 0.0689, 0.0600, 0.0581],
        [0.2434, 0.1057, 0.3565, 0.0484, 0.0622, 0.0559, 0.0737, 0.0542]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.2102, 0.1013, 0.1522, 0.1045, 0.1213, 0.0816, 0.0910, 0.1379],
        [0.1307, 0.0833, 0.1405, 0.1289, 0.1259, 0.1365, 0.1426, 0.1116]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1683, 0.0910, 0.0991, 0.1346, 0.1268, 0.1699, 0.1268, 0.0835],
        [0.1309, 0.0828, 0.1200, 0.1202, 0.1388, 0.1476, 0.0930, 0.1668],
        [0.1773, 0.0999, 0.1220, 0.0738, 0.0807, 0.1594, 0.1436, 0.1433]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1915, 0.1015, 0.1014, 0.1130, 0.0967, 0.0923, 0.1756, 0.1279],
        [0.1298, 0.0866, 0.0914, 0.1146, 0.1917, 0.1639, 0.1075, 0.1146],
        [0.1421, 0.0823, 0.0937, 0.1118, 0.1134, 0.1580, 0.1805, 0.1180],
        [0.2202, 0.1123, 0.1192, 0.0739, 0.0863, 0.0805, 0.1797, 0.1279]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1952, 0.1143, 0.1552, 0.0834, 0.0935, 0.0998, 0.1655, 0.0930],
        [0.0944, 0.0789, 0.1644, 0.0980, 0.1049, 0.1342, 0.1567, 0.1686],
        [0.1721, 0.1228, 0.1323, 0.0886, 0.1321, 0.1088, 0.1010, 0.1424],
        [0.1579, 0.1023, 0.0982, 0.1690, 0.0958, 0.1460, 0.1249, 0.1060],
        [0.1626, 0.1005, 0.1007, 0.1293, 0.1343, 0.1105, 0.0863, 0.1758]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/23 06:01:00 AM | size of train loader is 391
07/23 06:01:06 AM | Train: [38/50] Step 000/390 Loss 0.010 Prec@(1,5) (14.1%, 45.3%)
07/23 06:04:17 AM | Train: [38/50] Step 050/390 Loss 0.008 Prec@(1,5) (20.8%, 48.3%)
07/23 06:07:04 AM | Train: [38/50] Step 100/390 Loss 0.008 Prec@(1,5) (20.7%, 48.9%)
07/23 06:09:49 AM | Train: [38/50] Step 150/390 Loss 0.008 Prec@(1,5) (21.1%, 49.5%)
07/23 06:12:36 AM | Train: [38/50] Step 200/390 Loss 0.008 Prec@(1,5) (21.2%, 49.4%)
07/23 06:15:23 AM | Train: [38/50] Step 250/390 Loss 0.008 Prec@(1,5) (21.3%, 49.6%)
07/23 06:18:10 AM | Train: [38/50] Step 300/390 Loss 0.009 Prec@(1,5) (21.4%, 49.7%)
07/23 06:20:57 AM | Train: [38/50] Step 350/390 Loss 0.008 Prec@(1,5) (21.6%, 49.9%)
07/23 06:23:07 AM | Train: [38/50] Step 390/390 Loss 0.008 Prec@(1,5) (21.5%, 49.9%)
07/23 06:23:08 AM | Train: [38/50] Final Prec@1 21.4680%
07/23 06:23:09 AM | Valid: [38/50] Step 000/390 Loss 3.267 Prec@(1,5) (17.2%, 53.1%)
07/23 06:23:29 AM | Valid: [38/50] Step 050/390 Loss 3.273 Prec@(1,5) (20.8%, 49.4%)
07/23 06:23:49 AM | Valid: [38/50] Step 100/390 Loss 3.286 Prec@(1,5) (20.7%, 48.9%)
07/23 06:24:09 AM | Valid: [38/50] Step 150/390 Loss 3.287 Prec@(1,5) (20.6%, 49.1%)
07/23 06:24:30 AM | Valid: [38/50] Step 200/390 Loss 3.301 Prec@(1,5) (20.8%, 48.8%)
07/23 06:24:50 AM | Valid: [38/50] Step 250/390 Loss 3.299 Prec@(1,5) (20.9%, 48.9%)
07/23 06:25:10 AM | Valid: [38/50] Step 300/390 Loss 3.294 Prec@(1,5) (21.1%, 49.0%)
07/23 06:25:30 AM | Valid: [38/50] Step 350/390 Loss 3.295 Prec@(1,5) (21.1%, 49.0%)
07/23 06:25:44 AM | Valid: [38/50] Step 390/390 Loss 3.298 Prec@(1,5) (21.1%, 49.1%)
07/23 06:25:45 AM | Valid: [38/50] Final Prec@1 21.0520%
07/23 06:25:45 AM | genotype = Genotype(normal=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)], [('skip_connect', 4), ('max_pool_3x3', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('skip_connect', 1)], [('max_pool_3x3', 2), ('dil_conv_3x3', 0)], [('max_pool_3x3', 3), ('sep_conv_5x5', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.3928, 0.0802, 0.0968, 0.0865, 0.0949, 0.0972, 0.0998, 0.0517],
        [0.3626, 0.0795, 0.0971, 0.0828, 0.0965, 0.0965, 0.0833, 0.1017]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.3543, 0.0988, 0.1167, 0.1361, 0.0921, 0.0795, 0.0688, 0.0537],
        [0.2839, 0.0974, 0.1327, 0.1136, 0.0935, 0.0663, 0.1246, 0.0879],
        [0.3130, 0.1252, 0.1531, 0.0878, 0.0666, 0.0916, 0.1075, 0.0551]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.3908, 0.1112, 0.1487, 0.0720, 0.0808, 0.0565, 0.0732, 0.0668],
        [0.2671, 0.1124, 0.1350, 0.1013, 0.1486, 0.0797, 0.0751, 0.0809],
        [0.3008, 0.1440, 0.1507, 0.0624, 0.0847, 0.1112, 0.0847, 0.0615],
        [0.2635, 0.1251, 0.2647, 0.0980, 0.0595, 0.0770, 0.0709, 0.0413]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.3403, 0.1449, 0.1640, 0.0610, 0.0617, 0.0517, 0.1242, 0.0522],
        [0.2000, 0.0959, 0.1083, 0.1266, 0.1330, 0.1414, 0.0996, 0.0952],
        [0.2806, 0.1399, 0.1425, 0.0860, 0.1010, 0.0880, 0.0960, 0.0660],
        [0.3097, 0.1370, 0.2306, 0.0626, 0.0762, 0.0674, 0.0599, 0.0566],
        [0.2357, 0.1028, 0.3735, 0.0475, 0.0609, 0.0543, 0.0724, 0.0529]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.2120, 0.1010, 0.1514, 0.1039, 0.1219, 0.0813, 0.0898, 0.1388],
        [0.1308, 0.0828, 0.1418, 0.1283, 0.1275, 0.1358, 0.1416, 0.1114]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1676, 0.0904, 0.0984, 0.1351, 0.1277, 0.1721, 0.1257, 0.0831],
        [0.1315, 0.0830, 0.1179, 0.1207, 0.1386, 0.1478, 0.0923, 0.1683],
        [0.1789, 0.1003, 0.1223, 0.0732, 0.0799, 0.1586, 0.1444, 0.1424]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1919, 0.1012, 0.1019, 0.1132, 0.0959, 0.0922, 0.1761, 0.1276],
        [0.1302, 0.0865, 0.0919, 0.1140, 0.1918, 0.1644, 0.1062, 0.1149],
        [0.1425, 0.0821, 0.0936, 0.1108, 0.1131, 0.1581, 0.1815, 0.1183],
        [0.2215, 0.1118, 0.1183, 0.0736, 0.0859, 0.0802, 0.1810, 0.1276]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1958, 0.1141, 0.1545, 0.0835, 0.0941, 0.1007, 0.1639, 0.0934],
        [0.0948, 0.0791, 0.1642, 0.0974, 0.1045, 0.1347, 0.1557, 0.1697],
        [0.1736, 0.1237, 0.1329, 0.0883, 0.1324, 0.1072, 0.1006, 0.1412],
        [0.1586, 0.1028, 0.0990, 0.1675, 0.0952, 0.1470, 0.1240, 0.1059],
        [0.1632, 0.1003, 0.1006, 0.1304, 0.1349, 0.1100, 0.0868, 0.1738]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/23 06:25:46 AM | size of train loader is 391
07/23 06:25:51 AM | Train: [39/50] Step 000/390 Loss 0.010 Prec@(1,5) (25.0%, 57.8%)
07/23 06:28:46 AM | Train: [39/50] Step 050/390 Loss 0.007 Prec@(1,5) (22.4%, 52.1%)
07/23 06:31:56 AM | Train: [39/50] Step 100/390 Loss 0.008 Prec@(1,5) (22.0%, 51.0%)
07/23 06:35:19 AM | Train: [39/50] Step 150/390 Loss 0.008 Prec@(1,5) (21.3%, 50.1%)
07/23 06:38:22 AM | Train: [39/50] Step 200/390 Loss 0.008 Prec@(1,5) (21.1%, 49.7%)
07/23 06:41:11 AM | Train: [39/50] Step 250/390 Loss 0.008 Prec@(1,5) (21.0%, 49.5%)
07/23 06:44:02 AM | Train: [39/50] Step 300/390 Loss 0.009 Prec@(1,5) (21.2%, 49.4%)
07/23 06:46:51 AM | Train: [39/50] Step 350/390 Loss 0.008 Prec@(1,5) (21.1%, 49.5%)
07/23 06:49:07 AM | Train: [39/50] Step 390/390 Loss 0.008 Prec@(1,5) (21.2%, 49.5%)
07/23 06:49:07 AM | Train: [39/50] Final Prec@1 21.2120%
07/23 06:49:09 AM | Valid: [39/50] Step 000/390 Loss 2.941 Prec@(1,5) (25.0%, 57.8%)
07/23 06:49:29 AM | Valid: [39/50] Step 050/390 Loss 3.290 Prec@(1,5) (21.7%, 49.7%)
07/23 06:49:49 AM | Valid: [39/50] Step 100/390 Loss 3.286 Prec@(1,5) (21.1%, 49.6%)
07/23 06:50:09 AM | Valid: [39/50] Step 150/390 Loss 3.307 Prec@(1,5) (20.8%, 48.5%)
07/23 06:50:29 AM | Valid: [39/50] Step 200/390 Loss 3.311 Prec@(1,5) (20.6%, 48.5%)
07/23 06:50:49 AM | Valid: [39/50] Step 250/390 Loss 3.311 Prec@(1,5) (20.8%, 48.6%)
07/23 06:51:09 AM | Valid: [39/50] Step 300/390 Loss 3.308 Prec@(1,5) (20.6%, 48.6%)
07/23 06:51:29 AM | Valid: [39/50] Step 350/390 Loss 3.309 Prec@(1,5) (20.6%, 48.5%)
07/23 06:51:43 AM | Valid: [39/50] Step 390/390 Loss 3.307 Prec@(1,5) (20.7%, 48.5%)
07/23 06:51:43 AM | Valid: [39/50] Final Prec@1 20.6640%
07/23 06:51:43 AM | genotype = Genotype(normal=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)], [('skip_connect', 4), ('max_pool_3x3', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('skip_connect', 1)], [('max_pool_3x3', 2), ('dil_conv_3x3', 0)], [('max_pool_3x3', 3), ('sep_conv_5x5', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.3973, 0.0793, 0.0963, 0.0857, 0.0940, 0.0962, 0.0998, 0.0514],
        [0.3699, 0.0792, 0.0971, 0.0818, 0.0948, 0.0951, 0.0822, 0.0998]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.3604, 0.0979, 0.1156, 0.1342, 0.0909, 0.0788, 0.0688, 0.0533],
        [0.2896, 0.0970, 0.1327, 0.1123, 0.0924, 0.0655, 0.1243, 0.0862],
        [0.3182, 0.1258, 0.1551, 0.0859, 0.0656, 0.0889, 0.1064, 0.0542]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.3963, 0.1106, 0.1483, 0.0709, 0.0805, 0.0561, 0.0717, 0.0656],
        [0.2729, 0.1130, 0.1363, 0.0997, 0.1465, 0.0785, 0.0738, 0.0792],
        [0.3047, 0.1455, 0.1529, 0.0612, 0.0836, 0.1089, 0.0831, 0.0601],
        [0.2625, 0.1244, 0.2727, 0.0962, 0.0580, 0.0759, 0.0695, 0.0408]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.3431, 0.1447, 0.1639, 0.0604, 0.0617, 0.0508, 0.1233, 0.0520],
        [0.2024, 0.0970, 0.1094, 0.1259, 0.1331, 0.1406, 0.0981, 0.0936],
        [0.2833, 0.1407, 0.1431, 0.0856, 0.0995, 0.0874, 0.0954, 0.0649],
        [0.3091, 0.1361, 0.2361, 0.0619, 0.0754, 0.0663, 0.0597, 0.0554],
        [0.2267, 0.0997, 0.3899, 0.0469, 0.0600, 0.0533, 0.0716, 0.0519]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.2137, 0.1004, 0.1516, 0.1030, 0.1224, 0.0808, 0.0897, 0.1384],
        [0.1297, 0.0816, 0.1432, 0.1275, 0.1289, 0.1361, 0.1410, 0.1120]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1673, 0.0897, 0.0969, 0.1360, 0.1289, 0.1736, 0.1246, 0.0829],
        [0.1319, 0.0828, 0.1166, 0.1202, 0.1380, 0.1481, 0.0928, 0.1696],
        [0.1821, 0.1014, 0.1232, 0.0730, 0.0791, 0.1583, 0.1419, 0.1409]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1923, 0.1008, 0.1027, 0.1142, 0.0956, 0.0919, 0.1763, 0.1262],
        [0.1299, 0.0861, 0.0921, 0.1134, 0.1949, 0.1631, 0.1054, 0.1151],
        [0.1432, 0.0823, 0.0937, 0.1109, 0.1123, 0.1580, 0.1825, 0.1172],
        [0.2224, 0.1111, 0.1173, 0.0732, 0.0856, 0.0796, 0.1832, 0.1275]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1946, 0.1126, 0.1561, 0.0838, 0.0947, 0.1003, 0.1640, 0.0939],
        [0.0944, 0.0786, 0.1645, 0.0977, 0.1044, 0.1342, 0.1555, 0.1709],
        [0.1728, 0.1232, 0.1327, 0.0887, 0.1329, 0.1064, 0.1016, 0.1416],
        [0.1573, 0.1015, 0.0981, 0.1678, 0.0954, 0.1485, 0.1236, 0.1077],
        [0.1620, 0.0990, 0.0994, 0.1327, 0.1351, 0.1104, 0.0872, 0.1741]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/23 06:51:44 AM | size of train loader is 391
07/23 06:51:49 AM | Train: [40/50] Step 000/390 Loss 0.011 Prec@(1,5) (26.6%, 56.2%)
07/23 06:54:39 AM | Train: [40/50] Step 050/390 Loss 0.007 Prec@(1,5) (20.2%, 50.5%)
07/23 06:57:30 AM | Train: [40/50] Step 100/390 Loss 0.006 Prec@(1,5) (20.2%, 49.4%)
07/23 07:00:21 AM | Train: [40/50] Step 150/390 Loss 0.007 Prec@(1,5) (20.9%, 50.0%)
07/23 07:03:13 AM | Train: [40/50] Step 200/390 Loss 0.008 Prec@(1,5) (20.8%, 49.7%)
07/23 07:06:00 AM | Train: [40/50] Step 250/390 Loss 0.005 Prec@(1,5) (19.9%, 48.1%)
07/23 07:08:48 AM | Train: [40/50] Step 300/390 Loss 0.004 Prec@(1,5) (18.0%, 44.7%)
07/23 07:11:37 AM | Train: [40/50] Step 350/390 Loss 0.005 Prec@(1,5) (16.7%, 42.5%)
07/23 07:13:56 AM | Train: [40/50] Step 390/390 Loss 0.006 Prec@(1,5) (16.3%, 41.7%)
07/23 07:13:57 AM | Train: [40/50] Final Prec@1 16.3000%
07/23 07:13:58 AM | Valid: [40/50] Step 000/390 Loss 3.969 Prec@(1,5) (12.5%, 31.2%)
07/23 07:14:18 AM | Valid: [40/50] Step 050/390 Loss 3.779 Prec@(1,5) (12.8%, 34.7%)
07/23 07:14:38 AM | Valid: [40/50] Step 100/390 Loss 3.768 Prec@(1,5) (12.5%, 34.9%)
07/23 07:14:58 AM | Valid: [40/50] Step 150/390 Loss 3.794 Prec@(1,5) (12.3%, 34.6%)
07/23 07:15:18 AM | Valid: [40/50] Step 200/390 Loss 3.791 Prec@(1,5) (12.4%, 34.5%)
07/23 07:15:38 AM | Valid: [40/50] Step 250/390 Loss 3.785 Prec@(1,5) (12.6%, 34.8%)
07/23 07:15:58 AM | Valid: [40/50] Step 300/390 Loss 3.789 Prec@(1,5) (12.4%, 34.6%)
07/23 07:16:18 AM | Valid: [40/50] Step 350/390 Loss 3.788 Prec@(1,5) (12.5%, 34.7%)
07/23 07:16:32 AM | Valid: [40/50] Step 390/390 Loss 3.788 Prec@(1,5) (12.6%, 34.7%)
07/23 07:16:32 AM | Valid: [40/50] Final Prec@1 12.6120%
07/23 07:16:32 AM | genotype = Genotype(normal=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)], [('skip_connect', 4), ('max_pool_3x3', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_5x5', 1)], [('max_pool_3x3', 2), ('dil_conv_3x3', 0)], [('max_pool_3x3', 3), ('sep_conv_5x5', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.4037, 0.0760, 0.0930, 0.0857, 0.0939, 0.0961, 0.1017, 0.0499],
        [0.3611, 0.0753, 0.0926, 0.0827, 0.0988, 0.0983, 0.0863, 0.1049]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.3692, 0.0930, 0.1103, 0.1371, 0.0890, 0.0802, 0.0689, 0.0524],
        [0.2800, 0.0908, 0.1248, 0.1166, 0.0951, 0.0698, 0.1316, 0.0913],
        [0.3186, 0.1208, 0.1489, 0.0867, 0.0671, 0.0921, 0.1103, 0.0555]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.4036, 0.1077, 0.1455, 0.0709, 0.0803, 0.0565, 0.0706, 0.0650],
        [0.2650, 0.1090, 0.1322, 0.1043, 0.1507, 0.0803, 0.0754, 0.0831],
        [0.3018, 0.1422, 0.1488, 0.0609, 0.0848, 0.1128, 0.0868, 0.0619],
        [0.2610, 0.1226, 0.2720, 0.0994, 0.0582, 0.0750, 0.0691, 0.0428]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.3386, 0.1421, 0.1625, 0.0613, 0.0643, 0.0520, 0.1256, 0.0535],
        [0.1934, 0.0939, 0.1063, 0.1263, 0.1349, 0.1452, 0.1020, 0.0979],
        [0.2751, 0.1359, 0.1376, 0.0894, 0.1014, 0.0919, 0.1011, 0.0677],
        [0.3017, 0.1312, 0.2302, 0.0649, 0.0796, 0.0705, 0.0631, 0.0589],
        [0.2263, 0.1003, 0.3783, 0.0494, 0.0628, 0.0540, 0.0743, 0.0546]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.2169, 0.1010, 0.1536, 0.0999, 0.1231, 0.0810, 0.0868, 0.1378],
        [0.1295, 0.0806, 0.1410, 0.1284, 0.1280, 0.1351, 0.1443, 0.1131]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1689, 0.0900, 0.0957, 0.1331, 0.1299, 0.1743, 0.1255, 0.0825],
        [0.1306, 0.0813, 0.1139, 0.1185, 0.1359, 0.1504, 0.0951, 0.1744],
        [0.1795, 0.0996, 0.1207, 0.0735, 0.0811, 0.1605, 0.1456, 0.1394]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1971, 0.1031, 0.1007, 0.1142, 0.0945, 0.0933, 0.1743, 0.1228],
        [0.1295, 0.0852, 0.0894, 0.1115, 0.1991, 0.1627, 0.1049, 0.1177],
        [0.1439, 0.0830, 0.0948, 0.1117, 0.1149, 0.1559, 0.1794, 0.1163],
        [0.2238, 0.1114, 0.1185, 0.0715, 0.0862, 0.0792, 0.1825, 0.1269]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1938, 0.1120, 0.1577, 0.0846, 0.0940, 0.1006, 0.1632, 0.0941],
        [0.0949, 0.0789, 0.1635, 0.0954, 0.1077, 0.1314, 0.1539, 0.1742],
        [0.1747, 0.1241, 0.1307, 0.0908, 0.1357, 0.1055, 0.1022, 0.1364],
        [0.1565, 0.1007, 0.0981, 0.1674, 0.0945, 0.1505, 0.1238, 0.1085],
        [0.1621, 0.0990, 0.0994, 0.1348, 0.1335, 0.1071, 0.0887, 0.1754]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/23 07:16:34 AM | size of train loader is 391
07/23 07:16:39 AM | Train: [41/50] Step 000/390 Loss 0.006 Prec@(1,5) (10.9%, 37.5%)
07/23 07:19:53 AM | Train: [41/50] Step 050/390 Loss 0.009 Prec@(1,5) (13.1%, 37.1%)
07/23 07:23:17 AM | Train: [41/50] Step 100/390 Loss 0.010 Prec@(1,5) (13.6%, 38.3%)
07/23 07:26:42 AM | Train: [41/50] Step 150/390 Loss 0.010 Prec@(1,5) (14.4%, 39.2%)
07/23 07:30:09 AM | Train: [41/50] Step 200/390 Loss 0.010 Prec@(1,5) (14.9%, 40.4%)
07/23 07:33:38 AM | Train: [41/50] Step 250/390 Loss 0.009 Prec@(1,5) (15.4%, 40.8%)
07/23 07:36:59 AM | Train: [41/50] Step 300/390 Loss 0.009 Prec@(1,5) (15.7%, 41.4%)
07/23 07:40:18 AM | Train: [41/50] Step 350/390 Loss 0.009 Prec@(1,5) (16.3%, 42.1%)
07/23 07:42:42 AM | Train: [41/50] Step 390/390 Loss 0.009 Prec@(1,5) (16.6%, 42.4%)
07/23 07:42:42 AM | Train: [41/50] Final Prec@1 16.5520%
07/23 07:42:44 AM | Valid: [41/50] Step 000/390 Loss 3.774 Prec@(1,5) (14.1%, 23.4%)
07/23 07:43:04 AM | Valid: [41/50] Step 050/390 Loss 3.392 Prec@(1,5) (19.4%, 45.5%)
07/23 07:43:24 AM | Valid: [41/50] Step 100/390 Loss 3.388 Prec@(1,5) (19.0%, 45.8%)
07/23 07:43:44 AM | Valid: [41/50] Step 150/390 Loss 3.399 Prec@(1,5) (19.0%, 45.7%)
07/23 07:44:04 AM | Valid: [41/50] Step 200/390 Loss 3.399 Prec@(1,5) (19.2%, 45.8%)
07/23 07:44:24 AM | Valid: [41/50] Step 250/390 Loss 3.399 Prec@(1,5) (19.2%, 45.7%)
07/23 07:44:44 AM | Valid: [41/50] Step 300/390 Loss 3.401 Prec@(1,5) (19.0%, 45.7%)
07/23 07:45:04 AM | Valid: [41/50] Step 350/390 Loss 3.400 Prec@(1,5) (19.0%, 45.8%)
07/23 07:45:19 AM | Valid: [41/50] Step 390/390 Loss 3.402 Prec@(1,5) (18.9%, 45.9%)
07/23 07:45:19 AM | Valid: [41/50] Final Prec@1 18.8920%
07/23 07:45:19 AM | genotype = Genotype(normal=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)], [('skip_connect', 4), ('max_pool_3x3', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_5x5', 1)], [('max_pool_3x3', 2), ('dil_conv_3x3', 0)], [('max_pool_3x3', 3), ('sep_conv_5x5', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.4006, 0.0749, 0.0913, 0.0871, 0.0939, 0.0971, 0.1052, 0.0500],
        [0.3621, 0.0747, 0.0916, 0.0818, 0.0985, 0.1002, 0.0867, 0.1045]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.3718, 0.0913, 0.1078, 0.1397, 0.0878, 0.0810, 0.0685, 0.0521],
        [0.2799, 0.0890, 0.1221, 0.1159, 0.0943, 0.0721, 0.1340, 0.0927],
        [0.3231, 0.1205, 0.1476, 0.0862, 0.0667, 0.0923, 0.1087, 0.0549]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.4048, 0.1089, 0.1461, 0.0705, 0.0786, 0.0579, 0.0693, 0.0638],
        [0.2659, 0.1095, 0.1330, 0.1051, 0.1488, 0.0798, 0.0750, 0.0829],
        [0.3008, 0.1442, 0.1516, 0.0605, 0.0835, 0.1107, 0.0880, 0.0607],
        [0.2573, 0.1238, 0.2811, 0.0982, 0.0568, 0.0716, 0.0685, 0.0427]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.3375, 0.1426, 0.1634, 0.0610, 0.0655, 0.0531, 0.1228, 0.0541],
        [0.1950, 0.0964, 0.1089, 0.1220, 0.1323, 0.1444, 0.1022, 0.0990],
        [0.2728, 0.1357, 0.1363, 0.0899, 0.1019, 0.0926, 0.1050, 0.0658],
        [0.2990, 0.1303, 0.2320, 0.0653, 0.0791, 0.0719, 0.0634, 0.0589],
        [0.2213, 0.0989, 0.3848, 0.0504, 0.0633, 0.0529, 0.0737, 0.0547]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.2189, 0.1027, 0.1508, 0.0988, 0.1250, 0.0813, 0.0857, 0.1369],
        [0.1282, 0.0793, 0.1394, 0.1290, 0.1271, 0.1346, 0.1475, 0.1149]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1697, 0.0907, 0.0939, 0.1299, 0.1323, 0.1785, 0.1227, 0.0824],
        [0.1278, 0.0796, 0.1112, 0.1195, 0.1313, 0.1534, 0.0993, 0.1780],
        [0.1769, 0.0993, 0.1194, 0.0743, 0.0833, 0.1634, 0.1450, 0.1384]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1994, 0.1052, 0.0971, 0.1132, 0.0960, 0.0929, 0.1749, 0.1214],
        [0.1282, 0.0842, 0.0866, 0.1105, 0.1997, 0.1648, 0.1057, 0.1202],
        [0.1446, 0.0846, 0.0963, 0.1144, 0.1145, 0.1525, 0.1770, 0.1162],
        [0.2244, 0.1125, 0.1205, 0.0689, 0.0863, 0.0792, 0.1795, 0.1286]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1935, 0.1122, 0.1589, 0.0850, 0.0934, 0.1017, 0.1616, 0.0937],
        [0.0964, 0.0801, 0.1560, 0.0945, 0.1127, 0.1305, 0.1548, 0.1751],
        [0.1771, 0.1264, 0.1292, 0.0920, 0.1366, 0.1041, 0.1032, 0.1314],
        [0.1588, 0.1023, 0.0996, 0.1634, 0.0916, 0.1527, 0.1242, 0.1074],
        [0.1626, 0.0997, 0.0996, 0.1362, 0.1328, 0.1072, 0.0905, 0.1714]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/23 07:45:20 AM | size of train loader is 391
07/23 07:45:25 AM | Train: [42/50] Step 000/390 Loss 0.007 Prec@(1,5) (25.0%, 57.8%)
07/23 07:48:55 AM | Train: [42/50] Step 050/390 Loss 0.008 Prec@(1,5) (19.1%, 47.1%)
07/23 07:52:25 AM | Train: [42/50] Step 100/390 Loss 0.009 Prec@(1,5) (19.4%, 47.7%)
07/23 07:55:54 AM | Train: [42/50] Step 150/390 Loss 0.009 Prec@(1,5) (19.7%, 48.1%)
07/23 07:59:27 AM | Train: [42/50] Step 200/390 Loss 0.009 Prec@(1,5) (20.0%, 48.1%)
07/23 08:03:03 AM | Train: [42/50] Step 250/390 Loss 0.009 Prec@(1,5) (19.8%, 48.1%)
07/23 08:06:36 AM | Train: [42/50] Step 300/390 Loss 0.009 Prec@(1,5) (20.0%, 48.2%)
07/23 08:10:11 AM | Train: [42/50] Step 350/390 Loss 0.009 Prec@(1,5) (20.0%, 48.3%)
07/23 08:13:01 AM | Train: [42/50] Step 390/390 Loss 0.009 Prec@(1,5) (20.1%, 48.4%)
07/23 08:13:01 AM | Train: [42/50] Final Prec@1 20.0560%
07/23 08:13:02 AM | Valid: [42/50] Step 000/390 Loss 3.470 Prec@(1,5) (12.5%, 39.1%)
07/23 08:13:25 AM | Valid: [42/50] Step 050/390 Loss 3.344 Prec@(1,5) (19.9%, 48.6%)
07/23 08:13:46 AM | Valid: [42/50] Step 100/390 Loss 3.358 Prec@(1,5) (19.4%, 47.8%)
07/23 08:14:07 AM | Valid: [42/50] Step 150/390 Loss 3.338 Prec@(1,5) (19.8%, 47.9%)
07/23 08:14:28 AM | Valid: [42/50] Step 200/390 Loss 3.340 Prec@(1,5) (19.9%, 47.6%)
07/23 08:14:49 AM | Valid: [42/50] Step 250/390 Loss 3.339 Prec@(1,5) (20.1%, 47.8%)
07/23 08:15:09 AM | Valid: [42/50] Step 300/390 Loss 3.331 Prec@(1,5) (20.2%, 48.1%)
07/23 08:15:29 AM | Valid: [42/50] Step 350/390 Loss 3.327 Prec@(1,5) (20.2%, 48.3%)
07/23 08:15:44 AM | Valid: [42/50] Step 390/390 Loss 3.325 Prec@(1,5) (20.3%, 48.3%)
07/23 08:15:44 AM | Valid: [42/50] Final Prec@1 20.2760%
07/23 08:15:44 AM | genotype = Genotype(normal=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)], [('max_pool_3x3', 0), ('skip_connect', 3)], [('skip_connect', 4), ('max_pool_3x3', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_5x5', 1)], [('max_pool_3x3', 2), ('dil_conv_3x3', 0)], [('max_pool_3x3', 3), ('sep_conv_5x5', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.4003, 0.0766, 0.0930, 0.0878, 0.0926, 0.0951, 0.1043, 0.0502],
        [0.3712, 0.0767, 0.0938, 0.0785, 0.0960, 0.0985, 0.0847, 0.1006]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.3718, 0.0926, 0.1089, 0.1381, 0.0877, 0.0806, 0.0682, 0.0520],
        [0.2888, 0.0911, 0.1247, 0.1124, 0.0927, 0.0710, 0.1298, 0.0895],
        [0.3279, 0.1240, 0.1511, 0.0853, 0.0639, 0.0882, 0.1063, 0.0534]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.4036, 0.1123, 0.1501, 0.0687, 0.0774, 0.0577, 0.0680, 0.0621],
        [0.2730, 0.1128, 0.1373, 0.1016, 0.1448, 0.0772, 0.0738, 0.0796],
        [0.3037, 0.1495, 0.1579, 0.0584, 0.0809, 0.1055, 0.0858, 0.0582],
        [0.2516, 0.1260, 0.2960, 0.0945, 0.0547, 0.0692, 0.0668, 0.0413]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.3365, 0.1465, 0.1665, 0.0605, 0.0648, 0.0526, 0.1191, 0.0535],
        [0.1998, 0.1006, 0.1139, 0.1185, 0.1319, 0.1403, 0.0994, 0.0957],
        [0.2733, 0.1384, 0.1386, 0.0887, 0.1008, 0.0914, 0.1057, 0.0632],
        [0.2960, 0.1322, 0.2403, 0.0647, 0.0769, 0.0708, 0.0623, 0.0569],
        [0.2080, 0.0957, 0.4073, 0.0500, 0.0621, 0.0517, 0.0722, 0.0530]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.2200, 0.1032, 0.1505, 0.0982, 0.1252, 0.0813, 0.0849, 0.1368],
        [0.1278, 0.0789, 0.1390, 0.1285, 0.1276, 0.1353, 0.1478, 0.1151]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1692, 0.0911, 0.0935, 0.1297, 0.1317, 0.1804, 0.1210, 0.0835],
        [0.1279, 0.0797, 0.1103, 0.1202, 0.1292, 0.1537, 0.0997, 0.1794],
        [0.1775, 0.1009, 0.1203, 0.0751, 0.0831, 0.1627, 0.1432, 0.1372]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1984, 0.1053, 0.0972, 0.1123, 0.0971, 0.0913, 0.1766, 0.1217],
        [0.1289, 0.0849, 0.0864, 0.1096, 0.1959, 0.1648, 0.1074, 0.1221],
        [0.1442, 0.0854, 0.0966, 0.1141, 0.1146, 0.1522, 0.1778, 0.1152],
        [0.2242, 0.1132, 0.1208, 0.0691, 0.0856, 0.0792, 0.1787, 0.1292]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1920, 0.1115, 0.1581, 0.0855, 0.0933, 0.1032, 0.1622, 0.0943],
        [0.0971, 0.0806, 0.1538, 0.0951, 0.1150, 0.1305, 0.1535, 0.1744],
        [0.1795, 0.1288, 0.1296, 0.0918, 0.1347, 0.1026, 0.1035, 0.1294],
        [0.1599, 0.1036, 0.1005, 0.1608, 0.0895, 0.1539, 0.1251, 0.1067],
        [0.1633, 0.1000, 0.0996, 0.1360, 0.1344, 0.1077, 0.0904, 0.1685]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/23 08:15:45 AM | size of train loader is 391
07/23 08:15:51 AM | Train: [43/50] Step 000/390 Loss 0.009 Prec@(1,5) (10.9%, 45.3%)
07/23 08:18:56 AM | Train: [43/50] Step 050/390 Loss 0.008 Prec@(1,5) (20.6%, 49.4%)
07/23 08:22:27 AM | Train: [43/50] Step 100/390 Loss 0.009 Prec@(1,5) (20.6%, 49.4%)
07/23 08:26:00 AM | Train: [43/50] Step 150/390 Loss 0.009 Prec@(1,5) (20.9%, 49.2%)
07/23 08:29:31 AM | Train: [43/50] Step 200/390 Loss 0.009 Prec@(1,5) (21.0%, 49.1%)
07/23 08:33:01 AM | Train: [43/50] Step 250/390 Loss 0.009 Prec@(1,5) (20.9%, 49.1%)
07/23 08:36:34 AM | Train: [43/50] Step 300/390 Loss 0.008 Prec@(1,5) (20.8%, 49.0%)
07/23 08:40:07 AM | Train: [43/50] Step 350/390 Loss 0.008 Prec@(1,5) (20.9%, 49.2%)
07/23 08:42:55 AM | Train: [43/50] Step 390/390 Loss 0.008 Prec@(1,5) (20.9%, 49.3%)
07/23 08:42:55 AM | Train: [43/50] Final Prec@1 20.9440%
07/23 08:42:57 AM | Valid: [43/50] Step 000/390 Loss 3.622 Prec@(1,5) (15.6%, 39.1%)
07/23 08:43:18 AM | Valid: [43/50] Step 050/390 Loss 3.347 Prec@(1,5) (19.7%, 47.3%)
07/23 08:43:39 AM | Valid: [43/50] Step 100/390 Loss 3.330 Prec@(1,5) (20.3%, 47.5%)
07/23 08:44:01 AM | Valid: [43/50] Step 150/390 Loss 3.326 Prec@(1,5) (20.3%, 47.6%)
07/23 08:44:22 AM | Valid: [43/50] Step 200/390 Loss 3.333 Prec@(1,5) (20.1%, 47.5%)
07/23 08:44:43 AM | Valid: [43/50] Step 250/390 Loss 3.324 Prec@(1,5) (20.2%, 47.8%)
07/23 08:45:04 AM | Valid: [43/50] Step 300/390 Loss 3.318 Prec@(1,5) (20.3%, 48.0%)
07/23 08:45:25 AM | Valid: [43/50] Step 350/390 Loss 3.323 Prec@(1,5) (20.2%, 48.0%)
07/23 08:45:40 AM | Valid: [43/50] Step 390/390 Loss 3.320 Prec@(1,5) (20.3%, 48.2%)
07/23 08:45:40 AM | Valid: [43/50] Final Prec@1 20.3200%
07/23 08:45:40 AM | genotype = Genotype(normal=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)], [('max_pool_3x3', 0), ('skip_connect', 3)], [('skip_connect', 4), ('max_pool_3x3', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_5x5', 1)], [('max_pool_3x3', 2), ('dil_conv_3x3', 0)], [('max_pool_3x3', 3), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.4021, 0.0784, 0.0945, 0.0876, 0.0912, 0.0938, 0.1022, 0.0501],
        [0.3772, 0.0784, 0.0960, 0.0767, 0.0942, 0.0965, 0.0831, 0.0979]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.3737, 0.0946, 0.1100, 0.1352, 0.0877, 0.0798, 0.0672, 0.0517],
        [0.2966, 0.0931, 0.1272, 0.1102, 0.0908, 0.0697, 0.1261, 0.0862],
        [0.3328, 0.1272, 0.1551, 0.0835, 0.0618, 0.0837, 0.1040, 0.0519]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.4054, 0.1158, 0.1534, 0.0667, 0.0759, 0.0565, 0.0662, 0.0600],
        [0.2787, 0.1156, 0.1415, 0.0985, 0.1415, 0.0750, 0.0724, 0.0768],
        [0.3073, 0.1545, 0.1634, 0.0567, 0.0781, 0.1017, 0.0827, 0.0556],
        [0.2463, 0.1266, 0.3116, 0.0906, 0.0527, 0.0666, 0.0657, 0.0398]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.3356, 0.1501, 0.1682, 0.0598, 0.0645, 0.0520, 0.1168, 0.0530],
        [0.2039, 0.1044, 0.1180, 0.1159, 0.1314, 0.1371, 0.0967, 0.0926],
        [0.2743, 0.1412, 0.1418, 0.0875, 0.0997, 0.0895, 0.1046, 0.0614],
        [0.2937, 0.1337, 0.2494, 0.0633, 0.0746, 0.0692, 0.0610, 0.0550],
        [0.1951, 0.0913, 0.4312, 0.0493, 0.0602, 0.0511, 0.0705, 0.0512]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.2210, 0.1033, 0.1493, 0.0986, 0.1259, 0.0808, 0.0843, 0.1367],
        [0.1280, 0.0788, 0.1393, 0.1288, 0.1273, 0.1359, 0.1463, 0.1157]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1687, 0.0913, 0.0934, 0.1300, 0.1315, 0.1815, 0.1196, 0.0839],
        [0.1275, 0.0797, 0.1097, 0.1215, 0.1282, 0.1550, 0.0995, 0.1789],
        [0.1784, 0.1023, 0.1198, 0.0758, 0.0834, 0.1626, 0.1423, 0.1353]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1980, 0.1053, 0.0973, 0.1125, 0.0978, 0.0898, 0.1768, 0.1226],
        [0.1286, 0.0851, 0.0872, 0.1092, 0.1939, 0.1632, 0.1081, 0.1245],
        [0.1430, 0.0855, 0.0962, 0.1141, 0.1152, 0.1527, 0.1784, 0.1149],
        [0.2242, 0.1133, 0.1202, 0.0703, 0.0847, 0.0791, 0.1790, 0.1293]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1907, 0.1110, 0.1563, 0.0861, 0.0937, 0.1048, 0.1618, 0.0956],
        [0.0974, 0.0808, 0.1537, 0.0961, 0.1158, 0.1304, 0.1522, 0.1737],
        [0.1819, 0.1298, 0.1291, 0.0914, 0.1338, 0.1021, 0.1029, 0.1290],
        [0.1614, 0.1044, 0.1010, 0.1603, 0.0882, 0.1534, 0.1254, 0.1059],
        [0.1642, 0.1002, 0.0993, 0.1354, 0.1354, 0.1085, 0.0910, 0.1661]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/23 08:45:41 AM | size of train loader is 391
07/23 08:45:48 AM | Train: [44/50] Step 000/390 Loss 0.008 Prec@(1,5) (20.3%, 50.0%)
07/23 08:49:18 AM | Train: [44/50] Step 050/390 Loss 0.008 Prec@(1,5) (22.2%, 50.2%)
07/23 08:52:50 AM | Train: [44/50] Step 100/390 Loss 0.008 Prec@(1,5) (21.1%, 49.1%)
07/23 08:56:22 AM | Train: [44/50] Step 150/390 Loss 0.009 Prec@(1,5) (21.2%, 49.2%)
07/23 08:59:55 AM | Train: [44/50] Step 200/390 Loss 0.009 Prec@(1,5) (21.1%, 49.0%)
07/23 09:03:29 AM | Train: [44/50] Step 250/390 Loss 0.008 Prec@(1,5) (21.4%, 49.4%)
07/23 09:06:59 AM | Train: [44/50] Step 300/390 Loss 0.008 Prec@(1,5) (21.5%, 49.6%)
07/23 09:10:34 AM | Train: [44/50] Step 350/390 Loss 0.008 Prec@(1,5) (21.4%, 49.6%)
07/23 09:13:23 AM | Train: [44/50] Step 390/390 Loss 0.008 Prec@(1,5) (21.4%, 49.6%)
07/23 09:13:23 AM | Train: [44/50] Final Prec@1 21.4440%
07/23 09:13:25 AM | Valid: [44/50] Step 000/390 Loss 3.504 Prec@(1,5) (18.8%, 42.2%)
07/23 09:13:46 AM | Valid: [44/50] Step 050/390 Loss 3.246 Prec@(1,5) (20.8%, 50.0%)
07/23 09:14:08 AM | Valid: [44/50] Step 100/390 Loss 3.287 Prec@(1,5) (19.9%, 48.7%)
07/23 09:14:29 AM | Valid: [44/50] Step 150/390 Loss 3.299 Prec@(1,5) (20.0%, 48.4%)
07/23 09:14:51 AM | Valid: [44/50] Step 200/390 Loss 3.296 Prec@(1,5) (20.4%, 48.4%)
07/23 09:15:13 AM | Valid: [44/50] Step 250/390 Loss 3.293 Prec@(1,5) (20.7%, 48.6%)
07/23 09:15:34 AM | Valid: [44/50] Step 300/390 Loss 3.294 Prec@(1,5) (20.6%, 48.6%)
07/23 09:15:55 AM | Valid: [44/50] Step 350/390 Loss 3.299 Prec@(1,5) (20.4%, 48.5%)
07/23 09:16:11 AM | Valid: [44/50] Step 390/390 Loss 3.298 Prec@(1,5) (20.4%, 48.5%)
07/23 09:16:11 AM | Valid: [44/50] Final Prec@1 20.3880%
07/23 09:16:11 AM | genotype = Genotype(normal=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)], [('max_pool_3x3', 0), ('skip_connect', 3)], [('skip_connect', 4), ('max_pool_3x3', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_5x5', 1)], [('max_pool_3x3', 2), ('dil_conv_3x3', 0)], [('max_pool_3x3', 3), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.4050, 0.0802, 0.0963, 0.0870, 0.0892, 0.0923, 0.1005, 0.0495],
        [0.3789, 0.0795, 0.0975, 0.0761, 0.0936, 0.0952, 0.0827, 0.0964]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.3765, 0.0967, 0.1111, 0.1319, 0.0877, 0.0790, 0.0659, 0.0512],
        [0.3011, 0.0945, 0.1295, 0.1085, 0.0898, 0.0683, 0.1246, 0.0838],
        [0.3351, 0.1303, 0.1589, 0.0824, 0.0603, 0.0799, 0.1024, 0.0508]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.4066, 0.1188, 0.1565, 0.0649, 0.0746, 0.0555, 0.0649, 0.0582],
        [0.2830, 0.1177, 0.1448, 0.0955, 0.1403, 0.0728, 0.0715, 0.0744],
        [0.3090, 0.1590, 0.1691, 0.0547, 0.0762, 0.0987, 0.0799, 0.0534],
        [0.2396, 0.1263, 0.3276, 0.0882, 0.0507, 0.0645, 0.0644, 0.0388]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.3351, 0.1531, 0.1700, 0.0593, 0.0640, 0.0514, 0.1145, 0.0525],
        [0.2058, 0.1075, 0.1216, 0.1140, 0.1312, 0.1342, 0.0957, 0.0901],
        [0.2741, 0.1438, 0.1450, 0.0862, 0.0988, 0.0877, 0.1042, 0.0602],
        [0.2894, 0.1347, 0.2591, 0.0626, 0.0732, 0.0673, 0.0601, 0.0536],
        [0.1820, 0.0872, 0.4545, 0.0486, 0.0584, 0.0504, 0.0692, 0.0497]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.2213, 0.1032, 0.1499, 0.0987, 0.1263, 0.0801, 0.0839, 0.1367],
        [0.1285, 0.0787, 0.1396, 0.1292, 0.1263, 0.1360, 0.1451, 0.1165]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1679, 0.0912, 0.0941, 0.1300, 0.1320, 0.1817, 0.1184, 0.0847],
        [0.1274, 0.0796, 0.1091, 0.1238, 0.1274, 0.1562, 0.0990, 0.1776],
        [0.1783, 0.1027, 0.1190, 0.0768, 0.0838, 0.1631, 0.1420, 0.1342]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1975, 0.1051, 0.0972, 0.1119, 0.0977, 0.0898, 0.1769, 0.1237],
        [0.1290, 0.0855, 0.0881, 0.1098, 0.1908, 0.1624, 0.1084, 0.1260],
        [0.1418, 0.0855, 0.0958, 0.1141, 0.1155, 0.1525, 0.1803, 0.1147],
        [0.2231, 0.1135, 0.1197, 0.0710, 0.0840, 0.0792, 0.1801, 0.1294]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1898, 0.1106, 0.1539, 0.0869, 0.0935, 0.1064, 0.1619, 0.0970],
        [0.0979, 0.0809, 0.1538, 0.0971, 0.1159, 0.1302, 0.1516, 0.1726],
        [0.1831, 0.1301, 0.1287, 0.0915, 0.1334, 0.1019, 0.1029, 0.1284],
        [0.1625, 0.1049, 0.1016, 0.1605, 0.0868, 0.1532, 0.1252, 0.1052],
        [0.1640, 0.0997, 0.0987, 0.1356, 0.1379, 0.1093, 0.0911, 0.1637]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/23 09:16:12 AM | size of train loader is 391
07/23 09:16:18 AM | Train: [45/50] Step 000/390 Loss 0.008 Prec@(1,5) (25.0%, 57.8%)
07/23 09:19:51 AM | Train: [45/50] Step 050/390 Loss 0.008 Prec@(1,5) (22.3%, 51.2%)
07/23 09:23:19 AM | Train: [45/50] Step 100/390 Loss 0.008 Prec@(1,5) (21.0%, 49.9%)
07/23 09:26:51 AM | Train: [45/50] Step 150/390 Loss 0.008 Prec@(1,5) (21.6%, 49.9%)
07/23 09:30:25 AM | Train: [45/50] Step 200/390 Loss 0.008 Prec@(1,5) (21.3%, 49.8%)
07/23 09:33:59 AM | Train: [45/50] Step 250/390 Loss 0.008 Prec@(1,5) (21.3%, 49.7%)
07/23 09:37:29 AM | Train: [45/50] Step 300/390 Loss 0.008 Prec@(1,5) (21.2%, 49.4%)
07/23 09:41:01 AM | Train: [45/50] Step 350/390 Loss 0.008 Prec@(1,5) (21.2%, 49.4%)
07/23 09:43:49 AM | Train: [45/50] Step 390/390 Loss 0.008 Prec@(1,5) (21.4%, 49.7%)
07/23 09:43:49 AM | Train: [45/50] Final Prec@1 21.4080%
07/23 09:43:51 AM | Valid: [45/50] Step 000/390 Loss 3.428 Prec@(1,5) (18.8%, 42.2%)
07/23 09:44:12 AM | Valid: [45/50] Step 050/390 Loss 3.275 Prec@(1,5) (21.2%, 48.7%)
07/23 09:44:33 AM | Valid: [45/50] Step 100/390 Loss 3.262 Prec@(1,5) (21.9%, 49.6%)
07/23 09:44:55 AM | Valid: [45/50] Step 150/390 Loss 3.266 Prec@(1,5) (21.4%, 49.2%)
07/23 09:45:17 AM | Valid: [45/50] Step 200/390 Loss 3.273 Prec@(1,5) (21.2%, 49.0%)
07/23 09:45:39 AM | Valid: [45/50] Step 250/390 Loss 3.282 Prec@(1,5) (21.1%, 48.8%)
07/23 09:46:00 AM | Valid: [45/50] Step 300/390 Loss 3.281 Prec@(1,5) (21.0%, 48.9%)
07/23 09:46:21 AM | Valid: [45/50] Step 350/390 Loss 3.279 Prec@(1,5) (20.9%, 49.1%)
07/23 09:46:35 AM | Valid: [45/50] Step 390/390 Loss 3.278 Prec@(1,5) (20.9%, 49.0%)
07/23 09:46:36 AM | Valid: [45/50] Final Prec@1 20.8560%
07/23 09:46:36 AM | genotype = Genotype(normal=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)], [('max_pool_3x3', 0), ('skip_connect', 3)], [('skip_connect', 4), ('max_pool_3x3', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_5x5', 1)], [('max_pool_3x3', 2), ('dil_conv_3x3', 0)], [('max_pool_3x3', 3), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.4077, 0.0817, 0.0977, 0.0861, 0.0880, 0.0907, 0.0991, 0.0489],
        [0.3832, 0.0807, 0.0995, 0.0752, 0.0920, 0.0932, 0.0816, 0.0946]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.3770, 0.0988, 0.1124, 0.1295, 0.0876, 0.0786, 0.0652, 0.0508],
        [0.3062, 0.0960, 0.1312, 0.1072, 0.0885, 0.0674, 0.1217, 0.0817],
        [0.3355, 0.1327, 0.1616, 0.0819, 0.0593, 0.0774, 0.1016, 0.0500]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.4076, 0.1212, 0.1586, 0.0633, 0.0734, 0.0551, 0.0640, 0.0568],
        [0.2859, 0.1193, 0.1468, 0.0938, 0.1390, 0.0714, 0.0708, 0.0729],
        [0.3091, 0.1620, 0.1739, 0.0535, 0.0749, 0.0968, 0.0774, 0.0524],
        [0.2320, 0.1243, 0.3410, 0.0867, 0.0498, 0.0641, 0.0636, 0.0384]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.3351, 0.1547, 0.1704, 0.0590, 0.0639, 0.0512, 0.1132, 0.0525],
        [0.2070, 0.1097, 0.1241, 0.1119, 0.1317, 0.1336, 0.0940, 0.0879],
        [0.2730, 0.1450, 0.1476, 0.0858, 0.0983, 0.0867, 0.1039, 0.0597],
        [0.2848, 0.1338, 0.2667, 0.0622, 0.0729, 0.0666, 0.0600, 0.0529],
        [0.1711, 0.0836, 0.4741, 0.0483, 0.0568, 0.0498, 0.0678, 0.0486]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.2216, 0.1024, 0.1492, 0.0994, 0.1280, 0.0799, 0.0832, 0.1363],
        [0.1288, 0.0784, 0.1402, 0.1297, 0.1251, 0.1357, 0.1450, 0.1170]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1674, 0.0909, 0.0949, 0.1308, 0.1320, 0.1812, 0.1181, 0.0848],
        [0.1275, 0.0795, 0.1091, 0.1247, 0.1270, 0.1563, 0.0986, 0.1773],
        [0.1795, 0.1035, 0.1192, 0.0778, 0.0835, 0.1620, 0.1413, 0.1332]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1969, 0.1047, 0.0977, 0.1125, 0.0976, 0.0893, 0.1770, 0.1244],
        [0.1284, 0.0853, 0.0895, 0.1103, 0.1912, 0.1610, 0.1080, 0.1262],
        [0.1411, 0.0854, 0.0958, 0.1134, 0.1157, 0.1520, 0.1822, 0.1143],
        [0.2220, 0.1133, 0.1196, 0.0717, 0.0831, 0.0796, 0.1809, 0.1299]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1892, 0.1100, 0.1531, 0.0871, 0.0934, 0.1078, 0.1612, 0.0981],
        [0.0975, 0.0801, 0.1543, 0.0978, 0.1172, 0.1304, 0.1503, 0.1723],
        [0.1840, 0.1301, 0.1291, 0.0908, 0.1325, 0.1014, 0.1032, 0.1290],
        [0.1617, 0.1042, 0.1010, 0.1615, 0.0863, 0.1543, 0.1257, 0.1052],
        [0.1630, 0.0988, 0.0979, 0.1363, 0.1395, 0.1099, 0.0916, 0.1630]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/23 09:46:37 AM | size of train loader is 391
07/23 09:46:43 AM | Train: [46/50] Step 000/390 Loss 0.009 Prec@(1,5) (18.8%, 48.4%)
07/23 09:50:11 AM | Train: [46/50] Step 050/390 Loss 0.008 Prec@(1,5) (22.3%, 49.6%)
07/23 09:53:42 AM | Train: [46/50] Step 100/390 Loss 0.008 Prec@(1,5) (21.7%, 49.5%)
07/23 09:57:16 AM | Train: [46/50] Step 150/390 Loss 0.008 Prec@(1,5) (22.2%, 50.0%)
07/23 10:00:48 AM | Train: [46/50] Step 200/390 Loss 0.008 Prec@(1,5) (21.9%, 49.8%)
07/23 10:04:23 AM | Train: [46/50] Step 250/390 Loss 0.008 Prec@(1,5) (22.2%, 50.0%)
07/23 10:07:55 AM | Train: [46/50] Step 300/390 Loss 0.008 Prec@(1,5) (22.0%, 50.1%)
07/23 10:11:21 AM | Train: [46/50] Step 350/390 Loss 0.008 Prec@(1,5) (22.0%, 50.1%)
07/23 10:13:55 AM | Train: [46/50] Step 390/390 Loss 0.008 Prec@(1,5) (21.8%, 50.0%)
07/23 10:13:56 AM | Train: [46/50] Final Prec@1 21.8440%
07/23 10:13:57 AM | Valid: [46/50] Step 000/390 Loss 3.566 Prec@(1,5) (18.8%, 40.6%)
07/23 10:14:18 AM | Valid: [46/50] Step 050/390 Loss 3.315 Prec@(1,5) (20.3%, 48.0%)
07/23 10:14:38 AM | Valid: [46/50] Step 100/390 Loss 3.313 Prec@(1,5) (20.1%, 48.4%)
07/23 10:14:58 AM | Valid: [46/50] Step 150/390 Loss 3.310 Prec@(1,5) (20.2%, 48.3%)
07/23 10:15:19 AM | Valid: [46/50] Step 200/390 Loss 3.300 Prec@(1,5) (20.4%, 48.7%)
07/23 10:15:39 AM | Valid: [46/50] Step 250/390 Loss 3.293 Prec@(1,5) (20.6%, 48.9%)
07/23 10:15:59 AM | Valid: [46/50] Step 300/390 Loss 3.288 Prec@(1,5) (20.7%, 49.0%)
07/23 10:16:21 AM | Valid: [46/50] Step 350/390 Loss 3.291 Prec@(1,5) (20.6%, 48.8%)
07/23 10:16:36 AM | Valid: [46/50] Step 390/390 Loss 3.286 Prec@(1,5) (20.8%, 49.0%)
07/23 10:16:37 AM | Valid: [46/50] Final Prec@1 20.7840%
07/23 10:16:37 AM | genotype = Genotype(normal=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)], [('max_pool_3x3', 0), ('skip_connect', 3)], [('skip_connect', 4), ('max_pool_3x3', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_5x5', 1)], [('max_pool_3x3', 2), ('dil_conv_3x3', 0)], [('max_pool_3x3', 3), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.4100, 0.0829, 0.0988, 0.0852, 0.0872, 0.0890, 0.0983, 0.0486],
        [0.3853, 0.0814, 0.1005, 0.0750, 0.0914, 0.0920, 0.0813, 0.0932]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.3794, 0.1008, 0.1137, 0.1272, 0.0866, 0.0777, 0.0643, 0.0504],
        [0.3106, 0.0973, 0.1325, 0.1062, 0.0879, 0.0667, 0.1194, 0.0795],
        [0.3369, 0.1351, 0.1654, 0.0805, 0.0584, 0.0748, 0.0999, 0.0490]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.4083, 0.1238, 0.1621, 0.0613, 0.0723, 0.0541, 0.0626, 0.0555],
        [0.2891, 0.1211, 0.1491, 0.0922, 0.1371, 0.0696, 0.0704, 0.0715],
        [0.3091, 0.1656, 0.1789, 0.0519, 0.0729, 0.0950, 0.0756, 0.0510],
        [0.2240, 0.1227, 0.3561, 0.0847, 0.0487, 0.0631, 0.0627, 0.0379]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.3351, 0.1561, 0.1707, 0.0589, 0.0640, 0.0514, 0.1112, 0.0527],
        [0.2084, 0.1119, 0.1264, 0.1106, 0.1312, 0.1319, 0.0929, 0.0867],
        [0.2719, 0.1462, 0.1498, 0.0850, 0.0976, 0.0861, 0.1043, 0.0593],
        [0.2806, 0.1336, 0.2745, 0.0619, 0.0722, 0.0654, 0.0596, 0.0524],
        [0.1609, 0.0804, 0.4913, 0.0480, 0.0557, 0.0491, 0.0670, 0.0476]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.2225, 0.1024, 0.1497, 0.0990, 0.1277, 0.0792, 0.0830, 0.1366],
        [0.1298, 0.0788, 0.1397, 0.1297, 0.1254, 0.1358, 0.1439, 0.1170]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1668, 0.0908, 0.0954, 0.1308, 0.1316, 0.1821, 0.1168, 0.0856],
        [0.1282, 0.0799, 0.1084, 0.1252, 0.1271, 0.1563, 0.0987, 0.1763],
        [0.1791, 0.1039, 0.1190, 0.0788, 0.0842, 0.1608, 0.1414, 0.1328]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1961, 0.1043, 0.0986, 0.1132, 0.0977, 0.0893, 0.1768, 0.1240],
        [0.1282, 0.0853, 0.0904, 0.1111, 0.1901, 0.1602, 0.1075, 0.1272],
        [0.1400, 0.0852, 0.0955, 0.1121, 0.1166, 0.1518, 0.1837, 0.1151],
        [0.2220, 0.1130, 0.1189, 0.0722, 0.0824, 0.0795, 0.1824, 0.1295]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1882, 0.1090, 0.1518, 0.0882, 0.0935, 0.1090, 0.1603, 0.1000],
        [0.0979, 0.0802, 0.1559, 0.0985, 0.1174, 0.1311, 0.1495, 0.1695],
        [0.1836, 0.1292, 0.1287, 0.0907, 0.1330, 0.1015, 0.1038, 0.1296],
        [0.1616, 0.1039, 0.1008, 0.1623, 0.0868, 0.1547, 0.1248, 0.1050],
        [0.1621, 0.0984, 0.0977, 0.1363, 0.1414, 0.1106, 0.0919, 0.1617]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/23 10:16:38 AM | size of train loader is 391
07/23 10:16:44 AM | Train: [47/50] Step 000/390 Loss 0.009 Prec@(1,5) (15.6%, 46.9%)
07/23 10:20:06 AM | Train: [47/50] Step 050/390 Loss 0.008 Prec@(1,5) (21.4%, 50.8%)
07/23 10:23:28 AM | Train: [47/50] Step 100/390 Loss 0.008 Prec@(1,5) (21.4%, 49.9%)
07/23 10:26:50 AM | Train: [47/50] Step 150/390 Loss 0.008 Prec@(1,5) (21.4%, 49.6%)
07/23 10:30:16 AM | Train: [47/50] Step 200/390 Loss 0.008 Prec@(1,5) (21.5%, 49.9%)
07/23 10:33:39 AM | Train: [47/50] Step 250/390 Loss 0.008 Prec@(1,5) (21.6%, 49.9%)
07/23 10:37:02 AM | Train: [47/50] Step 300/390 Loss 0.008 Prec@(1,5) (21.6%, 50.1%)
07/23 10:40:22 AM | Train: [47/50] Step 350/390 Loss 0.008 Prec@(1,5) (21.7%, 50.1%)
07/23 10:43:04 AM | Train: [47/50] Step 390/390 Loss 0.008 Prec@(1,5) (21.7%, 50.4%)
07/23 10:43:04 AM | Train: [47/50] Final Prec@1 21.7440%
07/23 10:43:05 AM | Valid: [47/50] Step 000/390 Loss 3.499 Prec@(1,5) (14.1%, 48.4%)
07/23 10:43:26 AM | Valid: [47/50] Step 050/390 Loss 3.255 Prec@(1,5) (20.6%, 49.8%)
07/23 10:43:46 AM | Valid: [47/50] Step 100/390 Loss 3.281 Prec@(1,5) (20.3%, 49.1%)
07/23 10:44:06 AM | Valid: [47/50] Step 150/390 Loss 3.276 Prec@(1,5) (20.7%, 49.3%)
07/23 10:44:27 AM | Valid: [47/50] Step 200/390 Loss 3.284 Prec@(1,5) (20.5%, 49.1%)
07/23 10:44:47 AM | Valid: [47/50] Step 250/390 Loss 3.278 Prec@(1,5) (20.8%, 49.2%)
07/23 10:45:07 AM | Valid: [47/50] Step 300/390 Loss 3.285 Prec@(1,5) (20.9%, 49.0%)
07/23 10:45:27 AM | Valid: [47/50] Step 350/390 Loss 3.286 Prec@(1,5) (20.8%, 49.0%)
07/23 10:45:41 AM | Valid: [47/50] Step 390/390 Loss 3.287 Prec@(1,5) (20.8%, 49.0%)
07/23 10:45:41 AM | Valid: [47/50] Final Prec@1 20.8240%
07/23 10:45:41 AM | genotype = Genotype(normal=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)], [('max_pool_3x3', 0), ('skip_connect', 3)], [('skip_connect', 4), ('max_pool_3x3', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_5x5', 1)], [('max_pool_3x3', 2), ('dil_conv_3x3', 0)], [('max_pool_3x3', 3), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.4114, 0.0842, 0.0997, 0.0840, 0.0866, 0.0882, 0.0977, 0.0483],
        [0.3859, 0.0821, 0.1015, 0.0746, 0.0913, 0.0916, 0.0812, 0.0919]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.3811, 0.1028, 0.1154, 0.1243, 0.0856, 0.0772, 0.0637, 0.0499],
        [0.3146, 0.0984, 0.1339, 0.1053, 0.0871, 0.0654, 0.1177, 0.0776],
        [0.3388, 0.1379, 0.1687, 0.0794, 0.0574, 0.0723, 0.0975, 0.0480]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.4089, 0.1260, 0.1637, 0.0600, 0.0717, 0.0536, 0.0616, 0.0544],
        [0.2903, 0.1225, 0.1516, 0.0910, 0.1362, 0.0681, 0.0702, 0.0701],
        [0.3094, 0.1684, 0.1821, 0.0511, 0.0719, 0.0930, 0.0743, 0.0499],
        [0.2162, 0.1204, 0.3710, 0.0829, 0.0481, 0.0623, 0.0618, 0.0374]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.3334, 0.1569, 0.1716, 0.0589, 0.0643, 0.0515, 0.1104, 0.0530],
        [0.2100, 0.1142, 0.1285, 0.1093, 0.1307, 0.1301, 0.0917, 0.0856],
        [0.2712, 0.1474, 0.1516, 0.0843, 0.0969, 0.0847, 0.1048, 0.0591],
        [0.2760, 0.1328, 0.2824, 0.0614, 0.0714, 0.0648, 0.0594, 0.0519],
        [0.1517, 0.0773, 0.5079, 0.0477, 0.0543, 0.0486, 0.0659, 0.0467]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.2233, 0.1021, 0.1492, 0.0988, 0.1286, 0.0786, 0.0828, 0.1367],
        [0.1295, 0.0785, 0.1403, 0.1297, 0.1253, 0.1361, 0.1434, 0.1173]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1660, 0.0906, 0.0963, 0.1305, 0.1314, 0.1836, 0.1156, 0.0860],
        [0.1281, 0.0800, 0.1082, 0.1256, 0.1278, 0.1560, 0.0990, 0.1753],
        [0.1796, 0.1042, 0.1190, 0.0796, 0.0843, 0.1593, 0.1417, 0.1324]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1962, 0.1040, 0.0987, 0.1135, 0.0983, 0.0900, 0.1751, 0.1241],
        [0.1271, 0.0847, 0.0919, 0.1120, 0.1895, 0.1595, 0.1075, 0.1278],
        [0.1384, 0.0848, 0.0951, 0.1117, 0.1165, 0.1520, 0.1863, 0.1151],
        [0.2219, 0.1127, 0.1184, 0.0730, 0.0820, 0.0801, 0.1822, 0.1295]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1882, 0.1087, 0.1513, 0.0886, 0.0930, 0.1097, 0.1595, 0.1010],
        [0.0977, 0.0798, 0.1569, 0.0988, 0.1184, 0.1319, 0.1490, 0.1676],
        [0.1842, 0.1285, 0.1284, 0.0904, 0.1330, 0.1017, 0.1031, 0.1307],
        [0.1622, 0.1040, 0.1012, 0.1624, 0.0864, 0.1543, 0.1243, 0.1051],
        [0.1623, 0.0978, 0.0974, 0.1375, 0.1414, 0.1113, 0.0925, 0.1597]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/23 10:45:43 AM | size of train loader is 391
07/23 10:45:49 AM | Train: [48/50] Step 000/390 Loss 0.009 Prec@(1,5) (25.0%, 51.6%)
07/23 10:49:08 AM | Train: [48/50] Step 050/390 Loss 0.007 Prec@(1,5) (22.5%, 51.6%)
07/23 10:52:31 AM | Train: [48/50] Step 100/390 Loss 0.008 Prec@(1,5) (22.2%, 50.7%)
07/23 10:55:53 AM | Train: [48/50] Step 150/390 Loss 0.008 Prec@(1,5) (22.0%, 50.7%)
07/23 10:59:18 AM | Train: [48/50] Step 200/390 Loss 0.008 Prec@(1,5) (21.8%, 50.8%)
07/23 11:02:43 AM | Train: [48/50] Step 250/390 Loss 0.008 Prec@(1,5) (21.8%, 50.6%)
07/23 11:06:03 AM | Train: [48/50] Step 300/390 Loss 0.008 Prec@(1,5) (21.9%, 50.6%)
07/23 11:09:28 AM | Train: [48/50] Step 350/390 Loss 0.008 Prec@(1,5) (21.9%, 50.5%)
07/23 11:12:06 AM | Train: [48/50] Step 390/390 Loss 0.008 Prec@(1,5) (21.8%, 50.4%)
07/23 11:12:06 AM | Train: [48/50] Final Prec@1 21.7920%
07/23 11:12:08 AM | Valid: [48/50] Step 000/390 Loss 3.350 Prec@(1,5) (23.4%, 50.0%)
07/23 11:12:29 AM | Valid: [48/50] Step 050/390 Loss 3.253 Prec@(1,5) (21.3%, 49.6%)
07/23 11:12:49 AM | Valid: [48/50] Step 100/390 Loss 3.274 Prec@(1,5) (20.6%, 48.9%)
07/23 11:13:09 AM | Valid: [48/50] Step 150/390 Loss 3.278 Prec@(1,5) (20.7%, 49.1%)
07/23 11:13:29 AM | Valid: [48/50] Step 200/390 Loss 3.276 Prec@(1,5) (20.9%, 49.0%)
07/23 11:13:49 AM | Valid: [48/50] Step 250/390 Loss 3.268 Prec@(1,5) (21.0%, 49.2%)
07/23 11:14:09 AM | Valid: [48/50] Step 300/390 Loss 3.267 Prec@(1,5) (21.0%, 49.1%)
07/23 11:14:29 AM | Valid: [48/50] Step 350/390 Loss 3.269 Prec@(1,5) (21.1%, 49.0%)
07/23 11:14:43 AM | Valid: [48/50] Step 390/390 Loss 3.270 Prec@(1,5) (21.1%, 48.9%)
07/23 11:14:43 AM | Valid: [48/50] Final Prec@1 21.1200%
07/23 11:14:43 AM | genotype = Genotype(normal=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)], [('max_pool_3x3', 0), ('skip_connect', 3)], [('skip_connect', 4), ('max_pool_3x3', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_5x5', 1)], [('max_pool_3x3', 2), ('dil_conv_3x3', 0)], [('max_pool_3x3', 3), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.4146, 0.0856, 0.1014, 0.0823, 0.0852, 0.0867, 0.0966, 0.0476],
        [0.3862, 0.0827, 0.1025, 0.0751, 0.0904, 0.0906, 0.0814, 0.0912]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.3827, 0.1041, 0.1169, 0.1219, 0.0851, 0.0768, 0.0629, 0.0495],
        [0.3178, 0.0986, 0.1342, 0.1051, 0.0865, 0.0647, 0.1168, 0.0763],
        [0.3384, 0.1397, 0.1718, 0.0789, 0.0569, 0.0701, 0.0966, 0.0477]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.4100, 0.1280, 0.1659, 0.0585, 0.0711, 0.0526, 0.0606, 0.0534],
        [0.2913, 0.1232, 0.1530, 0.0901, 0.1358, 0.0672, 0.0701, 0.0693],
        [0.3082, 0.1711, 0.1864, 0.0503, 0.0709, 0.0911, 0.0726, 0.0494],
        [0.2082, 0.1180, 0.3855, 0.0814, 0.0470, 0.0614, 0.0612, 0.0373]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.3320, 0.1587, 0.1732, 0.0588, 0.0642, 0.0514, 0.1086, 0.0531],
        [0.2095, 0.1154, 0.1296, 0.1081, 0.1313, 0.1295, 0.0915, 0.0850],
        [0.2692, 0.1482, 0.1533, 0.0839, 0.0963, 0.0840, 0.1057, 0.0593],
        [0.2699, 0.1317, 0.2895, 0.0615, 0.0715, 0.0645, 0.0596, 0.0518],
        [0.1433, 0.0751, 0.5218, 0.0474, 0.0534, 0.0482, 0.0647, 0.0460]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.2250, 0.1020, 0.1493, 0.0984, 0.1288, 0.0779, 0.0823, 0.1363],
        [0.1302, 0.0784, 0.1404, 0.1302, 0.1240, 0.1355, 0.1431, 0.1183]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1662, 0.0906, 0.0965, 0.1305, 0.1306, 0.1840, 0.1153, 0.0863],
        [0.1284, 0.0800, 0.1076, 0.1251, 0.1281, 0.1570, 0.0987, 0.1751],
        [0.1804, 0.1042, 0.1175, 0.0809, 0.0844, 0.1607, 0.1412, 0.1306]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1967, 0.1043, 0.0984, 0.1141, 0.0973, 0.0898, 0.1755, 0.1240],
        [0.1271, 0.0849, 0.0935, 0.1116, 0.1881, 0.1587, 0.1076, 0.1285],
        [0.1383, 0.0850, 0.0955, 0.1105, 0.1179, 0.1518, 0.1865, 0.1145],
        [0.2232, 0.1130, 0.1184, 0.0738, 0.0814, 0.0801, 0.1816, 0.1284]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1880, 0.1086, 0.1508, 0.0894, 0.0929, 0.1097, 0.1583, 0.1023],
        [0.0981, 0.0798, 0.1572, 0.0996, 0.1190, 0.1321, 0.1469, 0.1672],
        [0.1835, 0.1279, 0.1289, 0.0897, 0.1334, 0.1016, 0.1033, 0.1318],
        [0.1622, 0.1037, 0.1011, 0.1632, 0.0859, 0.1536, 0.1244, 0.1058],
        [0.1612, 0.0968, 0.0965, 0.1387, 0.1434, 0.1119, 0.0925, 0.1590]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/23 11:14:44 AM | size of train loader is 391
07/23 11:14:50 AM | Train: [49/50] Step 000/390 Loss 0.009 Prec@(1,5) (29.7%, 56.2%)
07/23 11:18:13 AM | Train: [49/50] Step 050/390 Loss 0.007 Prec@(1,5) (22.0%, 50.0%)
07/23 11:21:27 AM | Train: [49/50] Step 100/390 Loss 0.008 Prec@(1,5) (21.9%, 50.8%)
07/23 11:24:43 AM | Train: [49/50] Step 150/390 Loss 0.008 Prec@(1,5) (22.1%, 50.7%)
07/23 11:28:09 AM | Train: [49/50] Step 200/390 Loss 0.008 Prec@(1,5) (22.0%, 50.8%)
07/23 11:31:38 AM | Train: [49/50] Step 250/390 Loss 0.008 Prec@(1,5) (22.1%, 50.9%)
07/23 11:35:03 AM | Train: [49/50] Step 300/390 Loss 0.008 Prec@(1,5) (21.9%, 50.7%)
07/23 11:38:33 AM | Train: [49/50] Step 350/390 Loss 0.008 Prec@(1,5) (21.9%, 50.7%)
07/23 11:41:14 AM | Train: [49/50] Step 390/390 Loss 0.008 Prec@(1,5) (21.9%, 50.6%)
07/23 11:41:14 AM | Train: [49/50] Final Prec@1 21.9320%
07/23 11:41:15 AM | Valid: [49/50] Step 000/390 Loss 3.290 Prec@(1,5) (15.6%, 48.4%)
07/23 11:41:36 AM | Valid: [49/50] Step 050/390 Loss 3.245 Prec@(1,5) (21.4%, 49.7%)
07/23 11:41:56 AM | Valid: [49/50] Step 100/390 Loss 3.252 Prec@(1,5) (21.1%, 50.0%)
07/23 11:42:16 AM | Valid: [49/50] Step 150/390 Loss 3.259 Prec@(1,5) (21.1%, 49.6%)
07/23 11:42:36 AM | Valid: [49/50] Step 200/390 Loss 3.259 Prec@(1,5) (21.0%, 49.5%)
07/23 11:42:56 AM | Valid: [49/50] Step 250/390 Loss 3.256 Prec@(1,5) (21.1%, 49.6%)
07/23 11:43:16 AM | Valid: [49/50] Step 300/390 Loss 3.258 Prec@(1,5) (21.2%, 49.6%)
07/23 11:43:36 AM | Valid: [49/50] Step 350/390 Loss 3.256 Prec@(1,5) (21.1%, 49.4%)
07/23 11:43:50 AM | Valid: [49/50] Step 390/390 Loss 3.254 Prec@(1,5) (21.2%, 49.4%)
07/23 11:43:51 AM | Valid: [49/50] Final Prec@1 21.1600%
07/23 11:43:51 AM | genotype = Genotype(normal=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)], [('max_pool_3x3', 0), ('skip_connect', 3)], [('skip_connect', 4), ('max_pool_3x3', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_5x5', 1)], [('max_pool_3x3', 2), ('dil_conv_3x3', 0)], [('max_pool_3x3', 3), ('max_pool_3x3', 0)], [('max_pool_3x3', 2), ('max_pool_3x3', 0)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.4147, 0.0872, 0.1029, 0.0816, 0.0841, 0.0860, 0.0960, 0.0476],
        [0.3865, 0.0837, 0.1039, 0.0748, 0.0900, 0.0897, 0.0813, 0.0900]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.3813, 0.1059, 0.1181, 0.1208, 0.0849, 0.0768, 0.0626, 0.0495],
        [0.3195, 0.0994, 0.1348, 0.1048, 0.0867, 0.0641, 0.1159, 0.0747],
        [0.3365, 0.1421, 0.1748, 0.0787, 0.0561, 0.0684, 0.0958, 0.0475]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.4065, 0.1302, 0.1691, 0.0579, 0.0709, 0.0523, 0.0600, 0.0531],
        [0.2921, 0.1248, 0.1553, 0.0893, 0.1350, 0.0659, 0.0696, 0.0680],
        [0.3056, 0.1746, 0.1901, 0.0496, 0.0702, 0.0889, 0.0721, 0.0489],
        [0.1997, 0.1158, 0.4004, 0.0798, 0.0464, 0.0605, 0.0604, 0.0371]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.3302, 0.1601, 0.1740, 0.0585, 0.0643, 0.0519, 0.1073, 0.0537],
        [0.2092, 0.1169, 0.1315, 0.1071, 0.1313, 0.1285, 0.0912, 0.0843],
        [0.2670, 0.1492, 0.1549, 0.0831, 0.0964, 0.0833, 0.1063, 0.0599],
        [0.2644, 0.1308, 0.2955, 0.0614, 0.0718, 0.0643, 0.0599, 0.0518],
        [0.1358, 0.0732, 0.5347, 0.0470, 0.0521, 0.0478, 0.0639, 0.0455]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.2247, 0.1019, 0.1497, 0.0982, 0.1296, 0.0776, 0.0819, 0.1364],
        [0.1301, 0.0782, 0.1404, 0.1301, 0.1245, 0.1353, 0.1426, 0.1187]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1660, 0.0910, 0.0965, 0.1310, 0.1302, 0.1843, 0.1145, 0.0865],
        [0.1283, 0.0800, 0.1080, 0.1250, 0.1282, 0.1573, 0.0984, 0.1748],
        [0.1809, 0.1050, 0.1175, 0.0817, 0.0840, 0.1604, 0.1409, 0.1296]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1957, 0.1041, 0.0986, 0.1140, 0.0975, 0.0899, 0.1764, 0.1239],
        [0.1264, 0.0847, 0.0949, 0.1117, 0.1882, 0.1576, 0.1077, 0.1287],
        [0.1376, 0.0849, 0.0955, 0.1109, 0.1179, 0.1517, 0.1866, 0.1149],
        [0.2223, 0.1129, 0.1178, 0.0739, 0.0814, 0.0800, 0.1828, 0.1290]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1876, 0.1083, 0.1506, 0.0899, 0.0926, 0.1103, 0.1576, 0.1030],
        [0.0984, 0.0800, 0.1590, 0.0994, 0.1192, 0.1322, 0.1450, 0.1666],
        [0.1842, 0.1281, 0.1295, 0.0892, 0.1321, 0.1015, 0.1036, 0.1318],
        [0.1634, 0.1044, 0.1020, 0.1618, 0.0858, 0.1525, 0.1240, 0.1061],
        [0.1611, 0.0969, 0.0967, 0.1382, 0.1436, 0.1133, 0.0935, 0.1568]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/23 11:43:52 AM | size of train loader is 391
07/23 11:43:58 AM | Train: [50/50] Step 000/390 Loss 0.009 Prec@(1,5) (25.0%, 54.7%)
07/23 11:46:56 AM | Train: [50/50] Step 050/390 Loss 0.007 Prec@(1,5) (23.0%, 50.6%)
07/23 11:50:02 AM | Train: [50/50] Step 100/390 Loss 0.008 Prec@(1,5) (22.6%, 51.1%)
07/23 11:53:21 AM | Train: [50/50] Step 150/390 Loss 0.008 Prec@(1,5) (22.4%, 50.7%)
07/23 11:56:45 AM | Train: [50/50] Step 200/390 Loss 0.008 Prec@(1,5) (21.9%, 50.0%)
07/23 12:00:14 PM | Train: [50/50] Step 250/390 Loss 0.008 Prec@(1,5) (21.9%, 50.3%)
07/23 12:03:45 PM | Train: [50/50] Step 300/390 Loss 0.008 Prec@(1,5) (22.0%, 50.5%)
07/23 12:07:12 PM | Train: [50/50] Step 350/390 Loss 0.008 Prec@(1,5) (21.9%, 50.4%)
07/23 12:09:58 PM | Train: [50/50] Step 390/390 Loss 0.008 Prec@(1,5) (21.9%, 50.5%)
07/23 12:09:58 PM | Train: [50/50] Final Prec@1 21.9240%
07/23 12:09:59 PM | Valid: [50/50] Step 000/390 Loss 3.229 Prec@(1,5) (23.4%, 48.4%)
07/23 12:10:21 PM | Valid: [50/50] Step 050/390 Loss 3.227 Prec@(1,5) (22.0%, 50.3%)
07/23 12:10:43 PM | Valid: [50/50] Step 100/390 Loss 3.246 Prec@(1,5) (21.9%, 49.8%)
07/23 12:11:04 PM | Valid: [50/50] Step 150/390 Loss 3.248 Prec@(1,5) (21.6%, 50.0%)
07/23 12:11:24 PM | Valid: [50/50] Step 200/390 Loss 3.244 Prec@(1,5) (21.6%, 50.0%)
07/23 12:11:44 PM | Valid: [50/50] Step 250/390 Loss 3.237 Prec@(1,5) (21.8%, 50.3%)
07/23 12:12:05 PM | Valid: [50/50] Step 300/390 Loss 3.241 Prec@(1,5) (21.7%, 50.1%)
07/23 12:12:27 PM | Valid: [50/50] Step 350/390 Loss 3.239 Prec@(1,5) (21.7%, 50.1%)
07/23 12:12:42 PM | Valid: [50/50] Step 390/390 Loss 3.242 Prec@(1,5) (21.7%, 50.1%)
07/23 12:12:42 PM | Valid: [50/50] Final Prec@1 21.6600%
07/23 12:12:42 PM | genotype = Genotype(normal=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)], [('skip_connect', 3), ('max_pool_3x3', 0)], [('skip_connect', 4), ('max_pool_3x3', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_5x5', 1)], [('max_pool_3x3', 2), ('dil_conv_3x3', 0)], [('max_pool_3x3', 3), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)]], reduce_concat=range(2, 6))
07/23 12:12:43 PM | Final best Prec@1 = 21.6600%
07/23 12:12:43 PM | Best Genotype = Genotype(normal=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)], [('skip_connect', 3), ('max_pool_3x3', 0)], [('skip_connect', 4), ('max_pool_3x3', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_5x5', 1)], [('max_pool_3x3', 2), ('dil_conv_3x3', 0)], [('max_pool_3x3', 3), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)]], reduce_concat=range(2, 6))
