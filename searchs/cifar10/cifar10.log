07/17 02:34:54 PM | 
07/17 02:34:54 PM | Parameters:
07/17 02:34:54 PM | ALPHA_LR=0.0003
07/17 02:34:54 PM | ALPHA_WEIGHT_DECAY=0.001
07/17 02:34:54 PM | BATCH_SIZE=64
07/17 02:34:54 PM | DATA_PATH=./data/
07/17 02:34:54 PM | DATASET=cifar10
07/17 02:34:54 PM | EPOCHS=50
07/17 02:34:54 PM | GPUS=[0]
07/17 02:34:54 PM | INIT_CHANNELS=16
07/17 02:34:54 PM | LAYERS=8
07/17 02:34:54 PM | NAME=cifar10
07/17 02:34:54 PM | PATH=searchs/cifar10
07/17 02:34:54 PM | PLOT_PATH=searchs/cifar10/plots
07/17 02:34:54 PM | PRINT_FREQ=50
07/17 02:34:54 PM | SEED=2
07/17 02:34:54 PM | W_GRAD_CLIP=5.0
07/17 02:34:54 PM | W_LR=0.025
07/17 02:34:54 PM | W_LR_MIN=0.001
07/17 02:34:54 PM | W_MOMENTUM=0.9
07/17 02:34:54 PM | W_WEIGHT_DECAY=0.0003
07/17 02:34:54 PM | WORKERS=4
07/17 02:34:54 PM | 
07/17 02:34:54 PM | Logger is set - training start
07/17 02:37:10 PM | 
07/17 02:37:10 PM | Parameters:
07/17 02:37:10 PM | ALPHA_LR=0.0003
07/17 02:37:10 PM | ALPHA_WEIGHT_DECAY=0.001
07/17 02:37:10 PM | BATCH_SIZE=64
07/17 02:37:10 PM | DATA_PATH=./data/
07/17 02:37:10 PM | DATASET=cifar10
07/17 02:37:10 PM | EPOCHS=50
07/17 02:37:10 PM | GPUS=[0]
07/17 02:37:10 PM | INIT_CHANNELS=16
07/17 02:37:10 PM | LAYERS=8
07/17 02:37:10 PM | NAME=cifar10
07/17 02:37:10 PM | PATH=searchs/cifar10
07/17 02:37:10 PM | PLOT_PATH=searchs/cifar10/plots
07/17 02:37:10 PM | PRINT_FREQ=50
07/17 02:37:10 PM | SEED=2
07/17 02:37:10 PM | W_GRAD_CLIP=5.0
07/17 02:37:10 PM | W_LR=0.025
07/17 02:37:10 PM | W_LR_MIN=0.001
07/17 02:37:10 PM | W_MOMENTUM=0.9
07/17 02:37:10 PM | W_WEIGHT_DECAY=0.0003
07/17 02:37:10 PM | WORKERS=4
07/17 02:37:10 PM | 
07/17 02:37:10 PM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/17 02:37:22 PM | Train: [ 1/50] Step 000/390 Loss 2.369 Prec@(1,5) (12.5%, 51.6%)
07/17 02:40:25 PM | Train: [ 1/50] Step 050/390 Loss 1.985 Prec@(1,5) (27.5%, 80.2%)
07/17 02:43:25 PM | Train: [ 1/50] Step 100/390 Loss 1.866 Prec@(1,5) (30.9%, 83.8%)
07/17 02:46:28 PM | Train: [ 1/50] Step 150/390 Loss 1.796 Prec@(1,5) (33.3%, 85.8%)
