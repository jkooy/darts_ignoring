07/23 11:52:23 PM | 
07/23 11:52:23 PM | Parameters:
07/23 11:52:23 PM | ALPHA_LR=0.0003
07/23 11:52:23 PM | ALPHA_WEIGHT_DECAY=0.001
07/23 11:52:23 PM | BATCH_SIZE=64
07/23 11:52:23 PM | DATA_PATH=./data/
07/23 11:52:23 PM | DATASET=cifar100
07/23 11:52:23 PM | EPOCHS=50
07/23 11:52:23 PM | GPUS=[0]
07/23 11:52:23 PM | INIT_CHANNELS=16
07/23 11:52:23 PM | LAYERS=8
07/23 11:52:23 PM | NAME=cifar100-2e
07/23 11:52:23 PM | PATH=searchs/cifar100-2e
07/23 11:52:23 PM | PLOT_PATH=searchs/cifar100-2e/plots
07/23 11:52:23 PM | PRINT_FREQ=50
07/23 11:52:23 PM | SEED=2
07/23 11:52:23 PM | W_GRAD_CLIP=5.0
07/23 11:52:23 PM | W_LR=0.025
07/23 11:52:23 PM | W_LR_MIN=0.001
07/23 11:52:23 PM | W_MOMENTUM=0.9
07/23 11:52:23 PM | W_WEIGHT_DECAY=0.0003
07/23 11:52:23 PM | WORKERS=4
07/23 11:52:23 PM | 
07/23 11:52:23 PM | Logger is set - training start
07/24 12:15:41 AM | 
07/24 12:15:41 AM | Parameters:
07/24 12:15:41 AM | ALPHA_LR=0.0003
07/24 12:15:41 AM | ALPHA_WEIGHT_DECAY=0.001
07/24 12:15:41 AM | BATCH_SIZE=64
07/24 12:15:41 AM | DATA_PATH=./data/
07/24 12:15:41 AM | DATASET=cifar100
07/24 12:15:41 AM | EPOCHS=50
07/24 12:15:41 AM | GPUS=[0]
07/24 12:15:41 AM | INIT_CHANNELS=16
07/24 12:15:41 AM | LAYERS=8
07/24 12:15:41 AM | NAME=cifar100-2e
07/24 12:15:41 AM | PATH=searchs/cifar100-2e
07/24 12:15:41 AM | PLOT_PATH=searchs/cifar100-2e/plots
07/24 12:15:41 AM | PRINT_FREQ=50
07/24 12:15:41 AM | SEED=2
07/24 12:15:41 AM | W_GRAD_CLIP=5.0
07/24 12:15:41 AM | W_LR=0.025
07/24 12:15:41 AM | W_LR_MIN=0.001
07/24 12:15:41 AM | W_MOMENTUM=0.9
07/24 12:15:41 AM | W_WEIGHT_DECAY=0.0003
07/24 12:15:41 AM | WORKERS=4
07/24 12:15:41 AM | 
07/24 12:15:41 AM | Logger is set - training start
07/24 12:28:09 AM | 
07/24 12:28:09 AM | Parameters:
07/24 12:28:09 AM | ALPHA_LR=0.0003
07/24 12:28:09 AM | ALPHA_WEIGHT_DECAY=0.001
07/24 12:28:09 AM | BATCH_SIZE=64
07/24 12:28:09 AM | DATA_PATH=./data/
07/24 12:28:09 AM | DATASET=cifar100
07/24 12:28:09 AM | EPOCHS=50
07/24 12:28:09 AM | GPUS=[0]
07/24 12:28:09 AM | INIT_CHANNELS=16
07/24 12:28:09 AM | LAYERS=8
07/24 12:28:09 AM | NAME=cifar100-2e
07/24 12:28:09 AM | PATH=searchs/cifar100-2e
07/24 12:28:09 AM | PLOT_PATH=searchs/cifar100-2e/plots
07/24 12:28:09 AM | PRINT_FREQ=50
07/24 12:28:09 AM | SEED=2
07/24 12:28:09 AM | W_GRAD_CLIP=5.0
07/24 12:28:09 AM | W_LR=0.025
07/24 12:28:09 AM | W_LR_MIN=0.001
07/24 12:28:09 AM | W_MOMENTUM=0.9
07/24 12:28:09 AM | W_WEIGHT_DECAY=0.0003
07/24 12:28:09 AM | WORKERS=4
07/24 12:28:09 AM | 
07/24 12:28:09 AM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/24 12:28:32 AM | Train: [ 1/50] Step 000/390 Loss 4.726 Prec@(1,5) (1.6%, 9.4%)
07/24 12:33:15 AM | Train: [ 1/50] Step 050/390 Loss 4.423 Prec@(1,5) (3.7%, 14.8%)
07/24 12:37:54 AM | Train: [ 1/50] Step 100/390 Loss 4.270 Prec@(1,5) (5.1%, 19.5%)
07/24 12:42:32 AM | Train: [ 1/50] Step 150/390 Loss 4.177 Prec@(1,5) (6.0%, 22.2%)
07/24 12:47:15 AM | Train: [ 1/50] Step 200/390 Loss 4.085 Prec@(1,5) (7.3%, 25.0%)
07/24 12:51:57 AM | Train: [ 1/50] Step 250/390 Loss 4.013 Prec@(1,5) (8.1%, 27.0%)
07/24 12:56:48 AM | Train: [ 1/50] Step 300/390 Loss 3.953 Prec@(1,5) (9.1%, 29.1%)
07/24 01:01:34 AM | Train: [ 1/50] Step 350/390 Loss 3.895 Prec@(1,5) (9.9%, 30.7%)
07/24 01:05:26 AM | Train: [ 1/50] Step 390/390 Loss 3.855 Prec@(1,5) (10.6%, 32.0%)
07/24 01:05:26 AM | Train: [ 1/50] Final Prec@1 10.5720%
07/24 01:05:27 AM | Valid: [ 1/50] Step 000/390 Loss 3.654 Prec@(1,5) (10.9%, 35.9%)
07/24 01:05:43 AM | Valid: [ 1/50] Step 050/390 Loss 3.509 Prec@(1,5) (16.0%, 43.2%)
07/24 01:06:00 AM | Valid: [ 1/50] Step 100/390 Loss 3.495 Prec@(1,5) (16.6%, 43.8%)
07/24 01:06:16 AM | Valid: [ 1/50] Step 150/390 Loss 3.481 Prec@(1,5) (16.7%, 43.8%)
07/24 01:06:32 AM | Valid: [ 1/50] Step 200/390 Loss 3.477 Prec@(1,5) (17.0%, 43.7%)
07/24 01:06:49 AM | Valid: [ 1/50] Step 250/390 Loss 3.476 Prec@(1,5) (17.0%, 43.7%)
07/24 01:07:05 AM | Valid: [ 1/50] Step 300/390 Loss 3.480 Prec@(1,5) (16.7%, 43.7%)
07/24 01:07:22 AM | Valid: [ 1/50] Step 350/390 Loss 3.486 Prec@(1,5) (16.5%, 43.5%)
07/24 01:07:35 AM | Valid: [ 1/50] Step 390/390 Loss 3.488 Prec@(1,5) (16.5%, 43.5%)
07/24 01:07:35 AM | Valid: [ 1/50] Final Prec@1 16.5320%
07/24 01:07:35 AM | genotype = Genotype(normal=[[('dil_conv_5x5', 1), ('max_pool_3x3', 0)], [('sep_conv_5x5', 1), ('sep_conv_3x3', 2)], [('dil_conv_5x5', 3), ('sep_conv_5x5', 0)], [('dil_conv_3x3', 1), ('dil_conv_5x5', 2)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_3x3', 1)], [('max_pool_3x3', 0), ('sep_conv_5x5', 2)], [('dil_conv_5x5', 3), ('dil_conv_5x5', 2)], [('max_pool_3x3', 0), ('dil_conv_5x5', 4)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1280, 0.1215, 0.1246, 0.1235, 0.1263, 0.1255, 0.1260, 0.1246],
        [0.1230, 0.1189, 0.1204, 0.1233, 0.1291, 0.1270, 0.1293, 0.1291]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1270, 0.1207, 0.1234, 0.1252, 0.1261, 0.1259, 0.1271, 0.1247],
        [0.1234, 0.1186, 0.1205, 0.1250, 0.1285, 0.1271, 0.1272, 0.1297],
        [0.1231, 0.1191, 0.1214, 0.1281, 0.1274, 0.1256, 0.1267, 0.1287]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1263, 0.1198, 0.1220, 0.1264, 0.1282, 0.1253, 0.1256, 0.1265],
        [0.1222, 0.1177, 0.1195, 0.1277, 0.1279, 0.1280, 0.1272, 0.1299],
        [0.1224, 0.1188, 0.1215, 0.1258, 0.1262, 0.1282, 0.1271, 0.1300],
        [0.1218, 0.1185, 0.1200, 0.1264, 0.1281, 0.1265, 0.1291, 0.1296]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1259, 0.1204, 0.1226, 0.1262, 0.1267, 0.1246, 0.1264, 0.1271],
        [0.1223, 0.1184, 0.1200, 0.1254, 0.1273, 0.1290, 0.1274, 0.1301],
        [0.1215, 0.1183, 0.1200, 0.1288, 0.1275, 0.1264, 0.1288, 0.1286],
        [0.1213, 0.1184, 0.1200, 0.1274, 0.1275, 0.1282, 0.1277, 0.1296],
        [0.1209, 0.1187, 0.1197, 0.1283, 0.1267, 0.1279, 0.1286, 0.1292]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1281, 0.1251, 0.1230, 0.1256, 0.1249, 0.1248, 0.1249, 0.1238],
        [0.1261, 0.1235, 0.1250, 0.1239, 0.1249, 0.1261, 0.1245, 0.1260]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1284, 0.1247, 0.1267, 0.1244, 0.1238, 0.1237, 0.1242, 0.1240],
        [0.1265, 0.1232, 0.1255, 0.1257, 0.1241, 0.1231, 0.1269, 0.1251],
        [0.1255, 0.1206, 0.1236, 0.1256, 0.1278, 0.1243, 0.1272, 0.1254]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1272, 0.1241, 0.1259, 0.1241, 0.1256, 0.1255, 0.1242, 0.1233],
        [0.1253, 0.1221, 0.1258, 0.1271, 0.1251, 0.1252, 0.1237, 0.1257],
        [0.1248, 0.1208, 0.1239, 0.1244, 0.1275, 0.1257, 0.1275, 0.1254],
        [0.1229, 0.1194, 0.1234, 0.1266, 0.1267, 0.1251, 0.1274, 0.1283]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1283, 0.1246, 0.1254, 0.1234, 0.1253, 0.1238, 0.1251, 0.1240],
        [0.1272, 0.1232, 0.1264, 0.1225, 0.1238, 0.1252, 0.1260, 0.1257],
        [0.1257, 0.1202, 0.1246, 0.1254, 0.1271, 0.1242, 0.1270, 0.1258],
        [0.1246, 0.1212, 0.1247, 0.1261, 0.1263, 0.1257, 0.1259, 0.1255],
        [0.1240, 0.1209, 0.1241, 0.1264, 0.1271, 0.1245, 0.1276, 0.1254]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/24 01:07:43 AM | Train: [ 2/50] Step 000/390 Loss 3.499 Prec@(1,5) (21.9%, 45.3%)
07/24 01:12:29 AM | Train: [ 2/50] Step 050/390 Loss 3.354 Prec@(1,5) (19.8%, 46.4%)
07/24 01:17:19 AM | Train: [ 2/50] Step 100/390 Loss 3.319 Prec@(1,5) (20.1%, 47.6%)
07/24 01:22:04 AM | Train: [ 2/50] Step 150/390 Loss 3.303 Prec@(1,5) (19.9%, 47.7%)
07/24 01:26:49 AM | Train: [ 2/50] Step 200/390 Loss 3.281 Prec@(1,5) (20.0%, 48.3%)
07/24 01:31:37 AM | Train: [ 2/50] Step 250/390 Loss 3.260 Prec@(1,5) (20.3%, 49.0%)
07/24 01:36:25 AM | Train: [ 2/50] Step 300/390 Loss 3.226 Prec@(1,5) (20.9%, 50.0%)
07/24 01:41:08 AM | Train: [ 2/50] Step 350/390 Loss 3.201 Prec@(1,5) (21.2%, 50.7%)
07/24 01:45:00 AM | Train: [ 2/50] Step 390/390 Loss 3.187 Prec@(1,5) (21.4%, 51.0%)
07/24 01:45:00 AM | Train: [ 2/50] Final Prec@1 21.4480%
07/24 01:45:01 AM | Valid: [ 2/50] Step 000/390 Loss 2.614 Prec@(1,5) (34.4%, 70.3%)
07/24 01:45:17 AM | Valid: [ 2/50] Step 050/390 Loss 2.982 Prec@(1,5) (24.4%, 56.9%)
07/24 01:45:33 AM | Valid: [ 2/50] Step 100/390 Loss 2.987 Prec@(1,5) (24.4%, 56.6%)
07/24 01:45:50 AM | Valid: [ 2/50] Step 150/390 Loss 3.003 Prec@(1,5) (24.3%, 56.2%)
07/24 01:46:06 AM | Valid: [ 2/50] Step 200/390 Loss 3.014 Prec@(1,5) (24.2%, 56.0%)
07/24 01:46:22 AM | Valid: [ 2/50] Step 250/390 Loss 3.028 Prec@(1,5) (23.9%, 55.7%)
07/24 01:46:38 AM | Valid: [ 2/50] Step 300/390 Loss 3.018 Prec@(1,5) (24.1%, 55.9%)
07/24 01:46:54 AM | Valid: [ 2/50] Step 350/390 Loss 3.020 Prec@(1,5) (24.1%, 55.7%)
07/24 01:47:08 AM | Valid: [ 2/50] Step 390/390 Loss 3.019 Prec@(1,5) (24.1%, 55.8%)
07/24 01:47:08 AM | Valid: [ 2/50] Final Prec@1 24.0840%
07/24 01:47:08 AM | genotype = Genotype(normal=[[('sep_conv_5x5', 1), ('max_pool_3x3', 0)], [('sep_conv_5x5', 1), ('max_pool_3x3', 0)], [('sep_conv_5x5', 3), ('dil_conv_3x3', 1)], [('dil_conv_3x3', 1), ('sep_conv_3x3', 2)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('sep_conv_5x5', 1)], [('max_pool_3x3', 0), ('dil_conv_5x5', 2)], [('max_pool_3x3', 0), ('dil_conv_5x5', 2)], [('max_pool_3x3', 0), ('dil_conv_5x5', 4)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1301, 0.1182, 0.1244, 0.1224, 0.1284, 0.1277, 0.1247, 0.1241],
        [0.1233, 0.1151, 0.1191, 0.1229, 0.1322, 0.1262, 0.1312, 0.1300]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1304, 0.1183, 0.1236, 0.1245, 0.1258, 0.1269, 0.1265, 0.1239],
        [0.1233, 0.1141, 0.1188, 0.1248, 0.1301, 0.1284, 0.1283, 0.1322],
        [0.1239, 0.1161, 0.1211, 0.1293, 0.1279, 0.1261, 0.1264, 0.1292]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1296, 0.1172, 0.1221, 0.1263, 0.1291, 0.1238, 0.1270, 0.1248],
        [0.1219, 0.1131, 0.1172, 0.1273, 0.1290, 0.1308, 0.1292, 0.1314],
        [0.1225, 0.1153, 0.1200, 0.1248, 0.1265, 0.1305, 0.1303, 0.1301],
        [0.1209, 0.1146, 0.1178, 0.1277, 0.1320, 0.1260, 0.1303, 0.1307]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1295, 0.1185, 0.1234, 0.1257, 0.1278, 0.1242, 0.1254, 0.1256],
        [0.1234, 0.1146, 0.1188, 0.1250, 0.1282, 0.1309, 0.1278, 0.1313],
        [0.1228, 0.1160, 0.1194, 0.1305, 0.1277, 0.1258, 0.1294, 0.1284],
        [0.1215, 0.1156, 0.1189, 0.1273, 0.1269, 0.1301, 0.1294, 0.1304],
        [0.1217, 0.1165, 0.1191, 0.1303, 0.1251, 0.1282, 0.1301, 0.1290]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1335, 0.1272, 0.1204, 0.1234, 0.1262, 0.1243, 0.1243, 0.1208],
        [0.1258, 0.1212, 0.1238, 0.1226, 0.1271, 0.1257, 0.1245, 0.1294]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1335, 0.1260, 0.1275, 0.1238, 0.1239, 0.1213, 0.1224, 0.1216],
        [0.1261, 0.1203, 0.1263, 0.1256, 0.1241, 0.1222, 0.1276, 0.1279],
        [0.1254, 0.1163, 0.1223, 0.1275, 0.1284, 0.1235, 0.1309, 0.1257]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1305, 0.1243, 0.1250, 0.1233, 0.1262, 0.1259, 0.1228, 0.1220],
        [0.1239, 0.1182, 0.1272, 0.1274, 0.1271, 0.1261, 0.1238, 0.1263],
        [0.1233, 0.1157, 0.1221, 0.1265, 0.1279, 0.1269, 0.1300, 0.1275],
        [0.1210, 0.1134, 0.1201, 0.1292, 0.1296, 0.1261, 0.1302, 0.1304]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1335, 0.1253, 0.1235, 0.1237, 0.1247, 0.1232, 0.1234, 0.1227],
        [0.1270, 0.1201, 0.1275, 0.1241, 0.1235, 0.1246, 0.1266, 0.1265],
        [0.1266, 0.1161, 0.1235, 0.1259, 0.1295, 0.1243, 0.1279, 0.1261],
        [0.1249, 0.1173, 0.1234, 0.1287, 0.1291, 0.1250, 0.1259, 0.1256],
        [0.1236, 0.1169, 0.1227, 0.1251, 0.1310, 0.1245, 0.1321, 0.1242]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/24 01:47:15 AM | Train: [ 3/50] Step 000/390 Loss 2.981 Prec@(1,5) (21.9%, 60.9%)
07/24 01:52:02 AM | Train: [ 3/50] Step 050/390 Loss 2.871 Prec@(1,5) (26.7%, 59.7%)
07/24 01:56:51 AM | Train: [ 3/50] Step 100/390 Loss 2.877 Prec@(1,5) (26.7%, 59.2%)
07/24 02:01:38 AM | Train: [ 3/50] Step 150/390 Loss 2.863 Prec@(1,5) (27.0%, 59.5%)
07/24 02:06:24 AM | Train: [ 3/50] Step 200/390 Loss 2.841 Prec@(1,5) (27.3%, 59.7%)
07/24 02:11:10 AM | Train: [ 3/50] Step 250/390 Loss 2.821 Prec@(1,5) (27.8%, 60.1%)
07/24 02:15:53 AM | Train: [ 3/50] Step 300/390 Loss 2.815 Prec@(1,5) (28.0%, 60.2%)
07/24 02:20:37 AM | Train: [ 3/50] Step 350/390 Loss 2.800 Prec@(1,5) (28.4%, 60.6%)
07/24 02:24:21 AM | Train: [ 3/50] Step 390/390 Loss 2.784 Prec@(1,5) (28.7%, 61.0%)
07/24 02:24:21 AM | Train: [ 3/50] Final Prec@1 28.6840%
07/24 02:24:22 AM | Valid: [ 3/50] Step 000/390 Loss 2.744 Prec@(1,5) (32.8%, 64.1%)
07/24 02:24:38 AM | Valid: [ 3/50] Step 050/390 Loss 2.776 Prec@(1,5) (29.7%, 61.9%)
07/24 02:24:55 AM | Valid: [ 3/50] Step 100/390 Loss 2.788 Prec@(1,5) (28.9%, 61.5%)
07/24 02:25:11 AM | Valid: [ 3/50] Step 150/390 Loss 2.782 Prec@(1,5) (28.8%, 61.6%)
07/24 02:25:28 AM | Valid: [ 3/50] Step 200/390 Loss 2.781 Prec@(1,5) (28.7%, 61.4%)
07/24 02:25:44 AM | Valid: [ 3/50] Step 250/390 Loss 2.781 Prec@(1,5) (28.7%, 61.3%)
07/24 02:26:01 AM | Valid: [ 3/50] Step 300/390 Loss 2.777 Prec@(1,5) (28.9%, 61.5%)
07/24 02:26:19 AM | Valid: [ 3/50] Step 350/390 Loss 2.773 Prec@(1,5) (29.2%, 61.6%)
07/24 02:26:32 AM | Valid: [ 3/50] Step 390/390 Loss 2.771 Prec@(1,5) (29.2%, 61.6%)
07/24 02:26:32 AM | Valid: [ 3/50] Final Prec@1 29.2200%
07/24 02:26:32 AM | genotype = Genotype(normal=[[('sep_conv_5x5', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('sep_conv_5x5', 1)], [('sep_conv_5x5', 3), ('dil_conv_3x3', 1)], [('dil_conv_3x3', 1), ('dil_conv_5x5', 4)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('sep_conv_5x5', 1)], [('max_pool_3x3', 0), ('dil_conv_5x5', 2)], [('max_pool_3x3', 0), ('sep_conv_3x3', 3)], [('max_pool_3x3', 0), ('dil_conv_5x5', 4)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1321, 0.1155, 0.1243, 0.1209, 0.1309, 0.1288, 0.1252, 0.1222],
        [0.1219, 0.1104, 0.1166, 0.1237, 0.1346, 0.1268, 0.1334, 0.1326]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1340, 0.1166, 0.1248, 0.1251, 0.1251, 0.1266, 0.1247, 0.1232],
        [0.1221, 0.1096, 0.1166, 0.1249, 0.1333, 0.1288, 0.1294, 0.1353],
        [0.1240, 0.1125, 0.1211, 0.1320, 0.1280, 0.1271, 0.1250, 0.1302]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1328, 0.1156, 0.1227, 0.1254, 0.1294, 0.1225, 0.1284, 0.1233],
        [0.1194, 0.1079, 0.1138, 0.1301, 0.1301, 0.1344, 0.1303, 0.1339],
        [0.1221, 0.1117, 0.1189, 0.1243, 0.1275, 0.1330, 0.1312, 0.1312],
        [0.1182, 0.1098, 0.1146, 0.1298, 0.1360, 0.1278, 0.1311, 0.1327]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1326, 0.1169, 0.1237, 0.1266, 0.1288, 0.1235, 0.1238, 0.1242],
        [0.1222, 0.1100, 0.1162, 0.1248, 0.1304, 0.1325, 0.1297, 0.1343],
        [0.1233, 0.1125, 0.1183, 0.1314, 0.1279, 0.1274, 0.1295, 0.1297],
        [0.1207, 0.1120, 0.1173, 0.1275, 0.1276, 0.1307, 0.1301, 0.1342],
        [0.1219, 0.1139, 0.1182, 0.1308, 0.1235, 0.1284, 0.1326, 0.1306]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1362, 0.1273, 0.1210, 0.1224, 0.1264, 0.1226, 0.1251, 0.1189],
        [0.1260, 0.1195, 0.1246, 0.1234, 0.1282, 0.1236, 0.1237, 0.1311]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1378, 0.1272, 0.1276, 0.1223, 0.1240, 0.1191, 0.1210, 0.1210],
        [0.1264, 0.1181, 0.1269, 0.1257, 0.1246, 0.1207, 0.1287, 0.1289],
        [0.1267, 0.1136, 0.1220, 0.1276, 0.1294, 0.1232, 0.1319, 0.1255]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1333, 0.1246, 0.1251, 0.1229, 0.1269, 0.1249, 0.1208, 0.1215],
        [0.1244, 0.1168, 0.1284, 0.1275, 0.1271, 0.1253, 0.1232, 0.1274],
        [0.1228, 0.1123, 0.1209, 0.1289, 0.1290, 0.1287, 0.1294, 0.1279],
        [0.1193, 0.1098, 0.1182, 0.1328, 0.1306, 0.1262, 0.1322, 0.1309]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1365, 0.1249, 0.1225, 0.1251, 0.1257, 0.1226, 0.1217, 0.1210],
        [0.1279, 0.1181, 0.1264, 0.1262, 0.1245, 0.1233, 0.1267, 0.1270],
        [0.1272, 0.1126, 0.1219, 0.1261, 0.1318, 0.1263, 0.1283, 0.1257],
        [0.1253, 0.1149, 0.1236, 0.1289, 0.1315, 0.1238, 0.1242, 0.1279],
        [0.1230, 0.1140, 0.1215, 0.1256, 0.1335, 0.1238, 0.1337, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/24 02:26:39 AM | Train: [ 4/50] Step 000/390 Loss 2.521 Prec@(1,5) (35.9%, 68.8%)
07/24 02:31:21 AM | Train: [ 4/50] Step 050/390 Loss 2.520 Prec@(1,5) (33.5%, 67.4%)
07/24 02:36:05 AM | Train: [ 4/50] Step 100/390 Loss 2.546 Prec@(1,5) (33.2%, 66.5%)
07/24 02:40:53 AM | Train: [ 4/50] Step 150/390 Loss 2.535 Prec@(1,5) (33.5%, 66.7%)
07/24 02:45:40 AM | Train: [ 4/50] Step 200/390 Loss 2.525 Prec@(1,5) (33.7%, 66.9%)
07/24 02:50:24 AM | Train: [ 4/50] Step 250/390 Loss 2.509 Prec@(1,5) (34.1%, 67.3%)
07/24 02:55:12 AM | Train: [ 4/50] Step 300/390 Loss 2.501 Prec@(1,5) (34.3%, 67.6%)
07/24 02:59:59 AM | Train: [ 4/50] Step 350/390 Loss 2.497 Prec@(1,5) (34.6%, 67.7%)
07/24 03:03:45 AM | Train: [ 4/50] Step 390/390 Loss 2.493 Prec@(1,5) (34.6%, 67.9%)
07/24 03:03:45 AM | Train: [ 4/50] Final Prec@1 34.5920%
07/24 03:03:46 AM | Valid: [ 4/50] Step 000/390 Loss 2.703 Prec@(1,5) (32.8%, 64.1%)
07/24 03:04:02 AM | Valid: [ 4/50] Step 050/390 Loss 2.605 Prec@(1,5) (34.1%, 65.1%)
07/24 03:04:19 AM | Valid: [ 4/50] Step 100/390 Loss 2.621 Prec@(1,5) (33.7%, 65.2%)
07/24 03:04:39 AM | Valid: [ 4/50] Step 150/390 Loss 2.638 Prec@(1,5) (33.2%, 64.9%)
07/24 03:04:56 AM | Valid: [ 4/50] Step 200/390 Loss 2.618 Prec@(1,5) (33.2%, 65.3%)
07/24 03:05:12 AM | Valid: [ 4/50] Step 250/390 Loss 2.621 Prec@(1,5) (33.2%, 65.3%)
07/24 03:05:28 AM | Valid: [ 4/50] Step 300/390 Loss 2.629 Prec@(1,5) (33.1%, 65.1%)
07/24 03:05:48 AM | Valid: [ 4/50] Step 350/390 Loss 2.627 Prec@(1,5) (33.1%, 65.1%)
07/24 03:06:05 AM | Valid: [ 4/50] Step 390/390 Loss 2.626 Prec@(1,5) (33.1%, 65.2%)
07/24 03:06:05 AM | Valid: [ 4/50] Final Prec@1 33.1200%
07/24 03:06:05 AM | genotype = Genotype(normal=[[('sep_conv_5x5', 1), ('sep_conv_5x5', 0)], [('max_pool_3x3', 0), ('sep_conv_5x5', 1)], [('sep_conv_5x5', 3), ('dil_conv_3x3', 1)], [('dil_conv_5x5', 4), ('dil_conv_3x3', 3)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('sep_conv_5x5', 1)], [('max_pool_3x3', 0), ('dil_conv_5x5', 2)], [('dil_conv_5x5', 3), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('dil_conv_5x5', 4)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1322, 0.1132, 0.1245, 0.1218, 0.1332, 0.1286, 0.1249, 0.1215],
        [0.1193, 0.1071, 0.1155, 0.1238, 0.1379, 0.1266, 0.1356, 0.1343]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1365, 0.1157, 0.1264, 0.1242, 0.1241, 0.1257, 0.1240, 0.1234],
        [0.1198, 0.1061, 0.1153, 0.1264, 0.1349, 0.1305, 0.1297, 0.1372],
        [0.1223, 0.1091, 0.1213, 0.1322, 0.1311, 0.1275, 0.1249, 0.1317]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1336, 0.1135, 0.1226, 0.1256, 0.1310, 0.1211, 0.1294, 0.1233],
        [0.1167, 0.1044, 0.1121, 0.1338, 0.1322, 0.1359, 0.1288, 0.1361],
        [0.1197, 0.1074, 0.1176, 0.1248, 0.1278, 0.1359, 0.1322, 0.1346],
        [0.1144, 0.1049, 0.1109, 0.1328, 0.1388, 0.1311, 0.1315, 0.1356]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1332, 0.1146, 0.1230, 0.1274, 0.1299, 0.1232, 0.1244, 0.1243],
        [0.1206, 0.1066, 0.1148, 0.1265, 0.1309, 0.1323, 0.1305, 0.1379],
        [0.1208, 0.1077, 0.1164, 0.1332, 0.1288, 0.1309, 0.1295, 0.1327],
        [0.1173, 0.1069, 0.1135, 0.1297, 0.1292, 0.1335, 0.1318, 0.1381],
        [0.1186, 0.1093, 0.1146, 0.1344, 0.1250, 0.1284, 0.1349, 0.1348]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1384, 0.1271, 0.1203, 0.1199, 0.1284, 0.1227, 0.1244, 0.1187],
        [0.1277, 0.1190, 0.1232, 0.1240, 0.1289, 0.1217, 0.1242, 0.1313]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1400, 0.1273, 0.1275, 0.1222, 0.1232, 0.1182, 0.1197, 0.1219],
        [0.1275, 0.1168, 0.1278, 0.1249, 0.1248, 0.1203, 0.1294, 0.1285],
        [0.1256, 0.1103, 0.1208, 0.1287, 0.1318, 0.1230, 0.1344, 0.1255]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1338, 0.1232, 0.1256, 0.1210, 0.1289, 0.1251, 0.1212, 0.1211],
        [0.1252, 0.1157, 0.1298, 0.1282, 0.1273, 0.1239, 0.1221, 0.1276],
        [0.1209, 0.1088, 0.1192, 0.1304, 0.1317, 0.1298, 0.1304, 0.1288],
        [0.1167, 0.1065, 0.1166, 0.1339, 0.1316, 0.1273, 0.1344, 0.1330]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1380, 0.1239, 0.1208, 0.1260, 0.1263, 0.1225, 0.1218, 0.1207],
        [0.1294, 0.1172, 0.1255, 0.1266, 0.1241, 0.1223, 0.1284, 0.1265],
        [0.1266, 0.1087, 0.1203, 0.1259, 0.1352, 0.1268, 0.1304, 0.1262],
        [0.1238, 0.1113, 0.1221, 0.1307, 0.1340, 0.1245, 0.1249, 0.1286],
        [0.1214, 0.1105, 0.1197, 0.1265, 0.1356, 0.1247, 0.1371, 0.1245]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/24 03:06:13 AM | Train: [ 5/50] Step 000/390 Loss 2.501 Prec@(1,5) (31.2%, 64.1%)
07/24 03:10:56 AM | Train: [ 5/50] Step 050/390 Loss 2.283 Prec@(1,5) (39.3%, 72.4%)
07/24 03:15:43 AM | Train: [ 5/50] Step 100/390 Loss 2.281 Prec@(1,5) (39.1%, 72.5%)
07/24 03:20:26 AM | Train: [ 5/50] Step 150/390 Loss 2.288 Prec@(1,5) (38.9%, 72.6%)
07/24 03:25:08 AM | Train: [ 5/50] Step 200/390 Loss 2.296 Prec@(1,5) (38.9%, 72.6%)
07/24 03:29:48 AM | Train: [ 5/50] Step 250/390 Loss 2.290 Prec@(1,5) (38.8%, 72.7%)
07/24 03:34:28 AM | Train: [ 5/50] Step 300/390 Loss 2.283 Prec@(1,5) (39.0%, 72.9%)
07/24 03:39:10 AM | Train: [ 5/50] Step 350/390 Loss 2.271 Prec@(1,5) (39.5%, 73.0%)
07/24 03:42:56 AM | Train: [ 5/50] Step 390/390 Loss 2.268 Prec@(1,5) (39.5%, 73.0%)
07/24 03:42:56 AM | Train: [ 5/50] Final Prec@1 39.5240%
07/24 03:42:57 AM | Valid: [ 5/50] Step 000/390 Loss 2.123 Prec@(1,5) (39.1%, 73.4%)
07/24 03:43:14 AM | Valid: [ 5/50] Step 050/390 Loss 2.429 Prec@(1,5) (37.3%, 69.4%)
07/24 03:43:30 AM | Valid: [ 5/50] Step 100/390 Loss 2.402 Prec@(1,5) (37.4%, 69.7%)
07/24 03:43:47 AM | Valid: [ 5/50] Step 150/390 Loss 2.404 Prec@(1,5) (37.7%, 69.7%)
07/24 03:44:03 AM | Valid: [ 5/50] Step 200/390 Loss 2.386 Prec@(1,5) (37.7%, 70.2%)
07/24 03:44:21 AM | Valid: [ 5/50] Step 250/390 Loss 2.383 Prec@(1,5) (38.1%, 70.1%)
07/24 03:44:37 AM | Valid: [ 5/50] Step 300/390 Loss 2.386 Prec@(1,5) (38.1%, 69.9%)
07/24 03:44:58 AM | Valid: [ 5/50] Step 350/390 Loss 2.395 Prec@(1,5) (37.7%, 69.8%)
07/24 03:45:14 AM | Valid: [ 5/50] Step 390/390 Loss 2.388 Prec@(1,5) (37.8%, 70.1%)
07/24 03:45:14 AM | Valid: [ 5/50] Final Prec@1 37.7600%
07/24 03:45:14 AM | genotype = Genotype(normal=[[('sep_conv_5x5', 1), ('sep_conv_5x5', 0)], [('sep_conv_5x5', 1), ('max_pool_3x3', 0)], [('sep_conv_5x5', 3), ('dil_conv_3x3', 2)], [('sep_conv_3x3', 4), ('sep_conv_3x3', 2)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('dil_conv_5x5', 2)], [('dil_conv_5x5', 3), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('sep_conv_5x5', 4)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1317, 0.1112, 0.1251, 0.1225, 0.1352, 0.1292, 0.1237, 0.1214],
        [0.1172, 0.1037, 0.1141, 0.1246, 0.1407, 0.1289, 0.1359, 0.1348]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1376, 0.1144, 0.1277, 0.1244, 0.1232, 0.1258, 0.1230, 0.1238],
        [0.1176, 0.1028, 0.1140, 0.1273, 0.1372, 0.1319, 0.1307, 0.1386],
        [0.1208, 0.1058, 0.1217, 0.1330, 0.1315, 0.1278, 0.1247, 0.1347]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1339, 0.1117, 0.1228, 0.1260, 0.1326, 0.1201, 0.1300, 0.1229],
        [0.1149, 0.1014, 0.1108, 0.1357, 0.1332, 0.1361, 0.1290, 0.1388],
        [0.1180, 0.1040, 0.1168, 0.1249, 0.1278, 0.1369, 0.1339, 0.1377],
        [0.1111, 0.1008, 0.1078, 0.1341, 0.1405, 0.1348, 0.1314, 0.1394]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1334, 0.1127, 0.1227, 0.1291, 0.1310, 0.1231, 0.1239, 0.1242],
        [0.1180, 0.1025, 0.1123, 0.1288, 0.1298, 0.1338, 0.1324, 0.1423],
        [0.1184, 0.1033, 0.1142, 0.1363, 0.1315, 0.1319, 0.1290, 0.1353],
        [0.1144, 0.1026, 0.1103, 0.1306, 0.1308, 0.1351, 0.1321, 0.1441],
        [0.1146, 0.1043, 0.1102, 0.1385, 0.1254, 0.1300, 0.1378, 0.1392]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1416, 0.1283, 0.1203, 0.1180, 0.1282, 0.1225, 0.1233, 0.1179],
        [0.1303, 0.1191, 0.1217, 0.1235, 0.1292, 0.1203, 0.1242, 0.1317]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1426, 0.1278, 0.1279, 0.1218, 0.1223, 0.1172, 0.1193, 0.1212],
        [0.1284, 0.1156, 0.1289, 0.1240, 0.1243, 0.1202, 0.1285, 0.1301],
        [0.1239, 0.1064, 0.1189, 0.1309, 0.1342, 0.1243, 0.1362, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1341, 0.1224, 0.1275, 0.1217, 0.1299, 0.1247, 0.1207, 0.1190],
        [0.1271, 0.1152, 0.1320, 0.1283, 0.1274, 0.1227, 0.1203, 0.1271],
        [0.1185, 0.1055, 0.1194, 0.1309, 0.1330, 0.1310, 0.1308, 0.1308],
        [0.1141, 0.1029, 0.1156, 0.1343, 0.1328, 0.1291, 0.1368, 0.1343]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1401, 0.1242, 0.1203, 0.1268, 0.1254, 0.1212, 0.1221, 0.1198],
        [0.1298, 0.1154, 0.1262, 0.1268, 0.1253, 0.1216, 0.1287, 0.1262],
        [0.1255, 0.1047, 0.1198, 0.1274, 0.1345, 0.1285, 0.1310, 0.1285],
        [0.1224, 0.1074, 0.1210, 0.1338, 0.1348, 0.1243, 0.1268, 0.1296],
        [0.1203, 0.1067, 0.1183, 0.1276, 0.1383, 0.1254, 0.1382, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/24 03:45:22 AM | Train: [ 6/50] Step 000/390 Loss 1.697 Prec@(1,5) (53.1%, 82.8%)
07/24 03:50:06 AM | Train: [ 6/50] Step 050/390 Loss 2.049 Prec@(1,5) (44.0%, 76.9%)
07/24 03:54:48 AM | Train: [ 6/50] Step 100/390 Loss 2.085 Prec@(1,5) (43.3%, 76.8%)
07/24 03:59:29 AM | Train: [ 6/50] Step 150/390 Loss 2.080 Prec@(1,5) (43.2%, 76.9%)
07/24 04:04:09 AM | Train: [ 6/50] Step 200/390 Loss 2.085 Prec@(1,5) (43.2%, 76.9%)
07/24 04:08:51 AM | Train: [ 6/50] Step 250/390 Loss 2.089 Prec@(1,5) (43.1%, 76.6%)
07/24 04:13:34 AM | Train: [ 6/50] Step 300/390 Loss 2.089 Prec@(1,5) (43.2%, 76.5%)
07/24 04:18:22 AM | Train: [ 6/50] Step 350/390 Loss 2.090 Prec@(1,5) (43.1%, 76.4%)
07/24 04:22:07 AM | Train: [ 6/50] Step 390/390 Loss 2.084 Prec@(1,5) (43.3%, 76.5%)
07/24 04:22:08 AM | Train: [ 6/50] Final Prec@1 43.3160%
07/24 04:22:08 AM | Valid: [ 6/50] Step 000/390 Loss 2.114 Prec@(1,5) (39.1%, 79.7%)
07/24 04:22:25 AM | Valid: [ 6/50] Step 050/390 Loss 2.179 Prec@(1,5) (42.0%, 74.1%)
07/24 04:22:44 AM | Valid: [ 6/50] Step 100/390 Loss 2.133 Prec@(1,5) (43.0%, 75.0%)
07/24 04:23:02 AM | Valid: [ 6/50] Step 150/390 Loss 2.145 Prec@(1,5) (42.5%, 74.8%)
07/24 04:23:18 AM | Valid: [ 6/50] Step 200/390 Loss 2.141 Prec@(1,5) (42.7%, 74.9%)
07/24 04:23:34 AM | Valid: [ 6/50] Step 250/390 Loss 2.146 Prec@(1,5) (42.9%, 74.8%)
07/24 04:23:51 AM | Valid: [ 6/50] Step 300/390 Loss 2.154 Prec@(1,5) (42.7%, 74.7%)
07/24 04:24:07 AM | Valid: [ 6/50] Step 350/390 Loss 2.148 Prec@(1,5) (42.9%, 74.8%)
07/24 04:24:20 AM | Valid: [ 6/50] Step 390/390 Loss 2.154 Prec@(1,5) (42.8%, 74.8%)
07/24 04:24:20 AM | Valid: [ 6/50] Final Prec@1 42.8440%
07/24 04:24:20 AM | genotype = Genotype(normal=[[('sep_conv_5x5', 1), ('sep_conv_5x5', 0)], [('sep_conv_5x5', 1), ('max_pool_3x3', 0)], [('sep_conv_5x5', 3), ('sep_conv_3x3', 1)], [('dil_conv_5x5', 4), ('sep_conv_3x3', 2)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('dil_conv_5x5', 2)], [('dil_conv_5x5', 3), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('dil_conv_5x5', 4)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1312, 0.1092, 0.1262, 0.1234, 0.1359, 0.1302, 0.1228, 0.1211],
        [0.1153, 0.1004, 0.1135, 0.1249, 0.1420, 0.1301, 0.1367, 0.1370]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1384, 0.1125, 0.1286, 0.1240, 0.1231, 0.1263, 0.1221, 0.1250],
        [0.1151, 0.0991, 0.1125, 0.1284, 0.1399, 0.1319, 0.1319, 0.1411],
        [0.1179, 0.1011, 0.1201, 0.1355, 0.1329, 0.1278, 0.1260, 0.1386]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1337, 0.1097, 0.1231, 0.1272, 0.1334, 0.1189, 0.1303, 0.1237],
        [0.1129, 0.0979, 0.1093, 0.1391, 0.1347, 0.1373, 0.1278, 0.1410],
        [0.1149, 0.0996, 0.1152, 0.1273, 0.1272, 0.1382, 0.1343, 0.1433],
        [0.1074, 0.0963, 0.1046, 0.1353, 0.1426, 0.1379, 0.1320, 0.1440]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1329, 0.1102, 0.1223, 0.1307, 0.1320, 0.1229, 0.1237, 0.1252],
        [0.1164, 0.0990, 0.1108, 0.1292, 0.1310, 0.1338, 0.1319, 0.1479],
        [0.1152, 0.0981, 0.1119, 0.1387, 0.1334, 0.1324, 0.1286, 0.1418],
        [0.1111, 0.0978, 0.1067, 0.1299, 0.1322, 0.1368, 0.1341, 0.1513],
        [0.1108, 0.0988, 0.1056, 0.1397, 0.1269, 0.1317, 0.1407, 0.1457]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1433, 0.1273, 0.1214, 0.1153, 0.1289, 0.1231, 0.1234, 0.1173],
        [0.1325, 0.1184, 0.1215, 0.1233, 0.1276, 0.1200, 0.1249, 0.1318]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1450, 0.1275, 0.1289, 0.1217, 0.1214, 0.1161, 0.1182, 0.1213],
        [0.1311, 0.1151, 0.1286, 0.1221, 0.1247, 0.1196, 0.1288, 0.1299],
        [0.1236, 0.1041, 0.1194, 0.1304, 0.1362, 0.1238, 0.1365, 0.1260]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1351, 0.1213, 0.1280, 0.1217, 0.1316, 0.1244, 0.1196, 0.1182],
        [0.1300, 0.1151, 0.1329, 0.1277, 0.1263, 0.1209, 0.1204, 0.1265],
        [0.1171, 0.1031, 0.1199, 0.1319, 0.1326, 0.1324, 0.1313, 0.1317],
        [0.1122, 0.0994, 0.1143, 0.1357, 0.1336, 0.1313, 0.1381, 0.1354]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1429, 0.1246, 0.1193, 0.1271, 0.1248, 0.1197, 0.1208, 0.1207],
        [0.1336, 0.1158, 0.1261, 0.1268, 0.1256, 0.1201, 0.1282, 0.1239],
        [0.1257, 0.1028, 0.1209, 0.1262, 0.1344, 0.1285, 0.1322, 0.1293],
        [0.1215, 0.1045, 0.1212, 0.1357, 0.1356, 0.1249, 0.1263, 0.1303],
        [0.1197, 0.1043, 0.1184, 0.1274, 0.1394, 0.1254, 0.1403, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/24 04:24:27 AM | Train: [ 7/50] Step 000/390 Loss 1.776 Prec@(1,5) (53.1%, 79.7%)
07/24 04:29:20 AM | Train: [ 7/50] Step 050/390 Loss 1.933 Prec@(1,5) (46.5%, 79.7%)
07/24 04:34:09 AM | Train: [ 7/50] Step 100/390 Loss 1.917 Prec@(1,5) (47.0%, 79.7%)
07/24 04:39:00 AM | Train: [ 7/50] Step 150/390 Loss 1.922 Prec@(1,5) (47.1%, 79.6%)
07/24 04:43:41 AM | Train: [ 7/50] Step 200/390 Loss 1.931 Prec@(1,5) (46.9%, 79.4%)
07/24 04:48:18 AM | Train: [ 7/50] Step 250/390 Loss 1.925 Prec@(1,5) (47.0%, 79.5%)
07/24 04:52:55 AM | Train: [ 7/50] Step 300/390 Loss 1.932 Prec@(1,5) (46.9%, 79.4%)
07/24 04:57:42 AM | Train: [ 7/50] Step 350/390 Loss 1.933 Prec@(1,5) (46.8%, 79.2%)
07/24 05:01:31 AM | Train: [ 7/50] Step 390/390 Loss 1.924 Prec@(1,5) (47.0%, 79.3%)
07/24 05:01:32 AM | Train: [ 7/50] Final Prec@1 46.9880%
07/24 05:01:32 AM | Valid: [ 7/50] Step 000/390 Loss 2.239 Prec@(1,5) (43.8%, 75.0%)
07/24 05:01:49 AM | Valid: [ 7/50] Step 050/390 Loss 2.117 Prec@(1,5) (43.8%, 75.2%)
07/24 05:02:05 AM | Valid: [ 7/50] Step 100/390 Loss 2.109 Prec@(1,5) (44.1%, 75.3%)
07/24 05:02:21 AM | Valid: [ 7/50] Step 150/390 Loss 2.095 Prec@(1,5) (44.2%, 75.7%)
07/24 05:02:38 AM | Valid: [ 7/50] Step 200/390 Loss 2.102 Prec@(1,5) (44.1%, 75.7%)
07/24 05:02:54 AM | Valid: [ 7/50] Step 250/390 Loss 2.103 Prec@(1,5) (44.0%, 75.8%)
07/24 05:03:11 AM | Valid: [ 7/50] Step 300/390 Loss 2.110 Prec@(1,5) (43.9%, 75.8%)
07/24 05:03:27 AM | Valid: [ 7/50] Step 350/390 Loss 2.117 Prec@(1,5) (43.7%, 75.9%)
07/24 05:03:40 AM | Valid: [ 7/50] Step 390/390 Loss 2.115 Prec@(1,5) (43.7%, 75.9%)
07/24 05:03:40 AM | Valid: [ 7/50] Final Prec@1 43.7320%
07/24 05:03:40 AM | genotype = Genotype(normal=[[('sep_conv_5x5', 1), ('sep_conv_5x5', 0)], [('sep_conv_5x5', 1), ('max_pool_3x3', 0)], [('sep_conv_3x3', 1), ('sep_conv_5x5', 3)], [('dil_conv_5x5', 4), ('sep_conv_3x3', 2)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('dil_conv_5x5', 2)], [('dil_conv_5x5', 3), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('sep_conv_5x5', 4)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1306, 0.1079, 0.1278, 0.1251, 0.1371, 0.1300, 0.1221, 0.1194],
        [0.1123, 0.0978, 0.1130, 0.1256, 0.1442, 0.1293, 0.1375, 0.1402]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1394, 0.1116, 0.1304, 0.1230, 0.1217, 0.1268, 0.1215, 0.1257],
        [0.1121, 0.0960, 0.1111, 0.1302, 0.1425, 0.1316, 0.1316, 0.1449],
        [0.1148, 0.0976, 0.1201, 0.1355, 0.1333, 0.1276, 0.1268, 0.1445]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1335, 0.1086, 0.1242, 0.1280, 0.1346, 0.1177, 0.1299, 0.1235],
        [0.1102, 0.0950, 0.1083, 0.1435, 0.1358, 0.1356, 0.1264, 0.1452],
        [0.1113, 0.0957, 0.1144, 0.1282, 0.1266, 0.1392, 0.1353, 0.1494],
        [0.1026, 0.0919, 0.1016, 0.1360, 0.1430, 0.1415, 0.1321, 0.1512]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1326, 0.1084, 0.1224, 0.1318, 0.1325, 0.1223, 0.1240, 0.1262],
        [0.1133, 0.0956, 0.1086, 0.1308, 0.1313, 0.1354, 0.1315, 0.1536],
        [0.1110, 0.0932, 0.1092, 0.1405, 0.1344, 0.1346, 0.1289, 0.1482],
        [0.1060, 0.0927, 0.1027, 0.1303, 0.1332, 0.1371, 0.1357, 0.1622],
        [0.1055, 0.0929, 0.1002, 0.1418, 0.1278, 0.1333, 0.1442, 0.1542]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1439, 0.1265, 0.1222, 0.1146, 0.1301, 0.1223, 0.1241, 0.1163],
        [0.1341, 0.1187, 0.1213, 0.1230, 0.1266, 0.1200, 0.1240, 0.1323]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1467, 0.1284, 0.1298, 0.1208, 0.1207, 0.1152, 0.1180, 0.1205],
        [0.1323, 0.1155, 0.1282, 0.1213, 0.1248, 0.1186, 0.1292, 0.1300],
        [0.1226, 0.1024, 0.1207, 0.1293, 0.1363, 0.1245, 0.1371, 0.1272]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1360, 0.1216, 0.1270, 0.1215, 0.1328, 0.1252, 0.1183, 0.1175],
        [0.1325, 0.1162, 0.1341, 0.1266, 0.1256, 0.1206, 0.1193, 0.1252],
        [0.1156, 0.1008, 0.1204, 0.1327, 0.1337, 0.1344, 0.1311, 0.1313],
        [0.1102, 0.0964, 0.1142, 0.1371, 0.1357, 0.1315, 0.1384, 0.1364]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1437, 0.1245, 0.1197, 0.1289, 0.1243, 0.1187, 0.1203, 0.1199],
        [0.1345, 0.1159, 0.1272, 0.1265, 0.1268, 0.1195, 0.1271, 0.1224],
        [0.1247, 0.1005, 0.1220, 0.1249, 0.1343, 0.1300, 0.1329, 0.1306],
        [0.1189, 0.1009, 0.1207, 0.1385, 0.1359, 0.1248, 0.1277, 0.1325],
        [0.1177, 0.1011, 0.1183, 0.1272, 0.1415, 0.1267, 0.1404, 0.1271]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/24 05:03:48 AM | Train: [ 8/50] Step 000/390 Loss 1.863 Prec@(1,5) (43.8%, 82.8%)
07/24 05:08:30 AM | Train: [ 8/50] Step 050/390 Loss 1.771 Prec@(1,5) (50.8%, 82.2%)
07/24 05:13:14 AM | Train: [ 8/50] Step 100/390 Loss 1.778 Prec@(1,5) (50.6%, 81.8%)
07/24 05:17:59 AM | Train: [ 8/50] Step 150/390 Loss 1.782 Prec@(1,5) (50.3%, 82.0%)
07/24 05:22:42 AM | Train: [ 8/50] Step 200/390 Loss 1.785 Prec@(1,5) (50.4%, 81.8%)
07/24 05:27:28 AM | Train: [ 8/50] Step 250/390 Loss 1.787 Prec@(1,5) (50.4%, 81.7%)
07/24 05:32:16 AM | Train: [ 8/50] Step 300/390 Loss 1.788 Prec@(1,5) (50.3%, 81.8%)
07/24 05:36:59 AM | Train: [ 8/50] Step 350/390 Loss 1.795 Prec@(1,5) (50.1%, 81.7%)
07/24 05:40:43 AM | Train: [ 8/50] Step 390/390 Loss 1.800 Prec@(1,5) (50.0%, 81.6%)
07/24 05:40:43 AM | Train: [ 8/50] Final Prec@1 49.9960%
07/24 05:40:44 AM | Valid: [ 8/50] Step 000/390 Loss 2.261 Prec@(1,5) (40.6%, 78.1%)
07/24 05:41:00 AM | Valid: [ 8/50] Step 050/390 Loss 2.105 Prec@(1,5) (44.6%, 76.8%)
07/24 05:41:17 AM | Valid: [ 8/50] Step 100/390 Loss 2.121 Prec@(1,5) (44.6%, 76.3%)
07/24 05:41:33 AM | Valid: [ 8/50] Step 150/390 Loss 2.104 Prec@(1,5) (44.7%, 76.4%)
07/24 05:41:49 AM | Valid: [ 8/50] Step 200/390 Loss 2.091 Prec@(1,5) (44.9%, 76.6%)
07/24 05:42:06 AM | Valid: [ 8/50] Step 250/390 Loss 2.092 Prec@(1,5) (45.1%, 76.5%)
07/24 05:42:22 AM | Valid: [ 8/50] Step 300/390 Loss 2.094 Prec@(1,5) (45.0%, 76.4%)
07/24 05:42:39 AM | Valid: [ 8/50] Step 350/390 Loss 2.095 Prec@(1,5) (45.2%, 76.3%)
07/24 05:42:52 AM | Valid: [ 8/50] Step 390/390 Loss 2.097 Prec@(1,5) (45.2%, 76.3%)
07/24 05:42:52 AM | Valid: [ 8/50] Final Prec@1 45.1680%
07/24 05:42:52 AM | genotype = Genotype(normal=[[('sep_conv_5x5', 1), ('sep_conv_5x5', 0)], [('sep_conv_5x5', 1), ('max_pool_3x3', 0)], [('sep_conv_3x3', 1), ('dil_conv_3x3', 3)], [('dil_conv_5x5', 4), ('sep_conv_3x3', 2)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('sep_conv_5x5', 2)], [('dil_conv_5x5', 3), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('sep_conv_5x5', 4)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1290, 0.1071, 0.1303, 0.1272, 0.1360, 0.1298, 0.1212, 0.1194],
        [0.1096, 0.0956, 0.1134, 0.1256, 0.1471, 0.1276, 0.1383, 0.1428]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1386, 0.1114, 0.1334, 0.1219, 0.1211, 0.1269, 0.1197, 0.1271],
        [0.1090, 0.0933, 0.1106, 0.1318, 0.1445, 0.1340, 0.1289, 0.1479],
        [0.1107, 0.0940, 0.1198, 0.1374, 0.1340, 0.1265, 0.1262, 0.1514]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1324, 0.1076, 0.1254, 0.1291, 0.1350, 0.1170, 0.1300, 0.1237],
        [0.1072, 0.0922, 0.1078, 0.1457, 0.1377, 0.1366, 0.1243, 0.1486],
        [0.1066, 0.0914, 0.1126, 0.1298, 0.1260, 0.1403, 0.1369, 0.1565],
        [0.0974, 0.0873, 0.0984, 0.1349, 0.1451, 0.1452, 0.1314, 0.1603]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1315, 0.1070, 0.1229, 0.1324, 0.1339, 0.1212, 0.1234, 0.1279],
        [0.1106, 0.0925, 0.1078, 0.1312, 0.1314, 0.1362, 0.1305, 0.1599],
        [0.1064, 0.0886, 0.1069, 0.1413, 0.1356, 0.1356, 0.1290, 0.1565],
        [0.1008, 0.0876, 0.0985, 0.1311, 0.1338, 0.1383, 0.1365, 0.1734],
        [0.1000, 0.0878, 0.0954, 0.1429, 0.1303, 0.1343, 0.1452, 0.1641]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1447, 0.1263, 0.1231, 0.1136, 0.1309, 0.1219, 0.1243, 0.1152],
        [0.1358, 0.1194, 0.1199, 0.1238, 0.1261, 0.1186, 0.1239, 0.1326]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1473, 0.1283, 0.1301, 0.1215, 0.1205, 0.1150, 0.1178, 0.1195],
        [0.1337, 0.1164, 0.1284, 0.1204, 0.1250, 0.1178, 0.1290, 0.1292],
        [0.1206, 0.1003, 0.1215, 0.1281, 0.1383, 0.1251, 0.1376, 0.1285]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1362, 0.1210, 0.1285, 0.1217, 0.1335, 0.1257, 0.1173, 0.1161],
        [0.1341, 0.1166, 0.1342, 0.1252, 0.1245, 0.1212, 0.1204, 0.1237],
        [0.1133, 0.0986, 0.1212, 0.1333, 0.1348, 0.1342, 0.1317, 0.1329],
        [0.1076, 0.0938, 0.1140, 0.1381, 0.1373, 0.1333, 0.1383, 0.1376]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1450, 0.1250, 0.1216, 0.1287, 0.1234, 0.1180, 0.1191, 0.1192],
        [0.1368, 0.1169, 0.1287, 0.1250, 0.1260, 0.1193, 0.1255, 0.1217],
        [0.1227, 0.0984, 0.1238, 0.1244, 0.1336, 0.1308, 0.1333, 0.1329],
        [0.1160, 0.0986, 0.1217, 0.1399, 0.1360, 0.1246, 0.1286, 0.1347],
        [0.1151, 0.0982, 0.1177, 0.1278, 0.1445, 0.1282, 0.1404, 0.1280]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/24 05:43:00 AM | Train: [ 9/50] Step 000/390 Loss 1.641 Prec@(1,5) (51.6%, 89.1%)
07/24 05:47:41 AM | Train: [ 9/50] Step 050/390 Loss 1.661 Prec@(1,5) (53.7%, 84.3%)
07/24 05:52:32 AM | Train: [ 9/50] Step 100/390 Loss 1.661 Prec@(1,5) (53.0%, 84.2%)
07/24 05:57:16 AM | Train: [ 9/50] Step 150/390 Loss 1.675 Prec@(1,5) (52.9%, 83.6%)
07/24 06:02:02 AM | Train: [ 9/50] Step 200/390 Loss 1.682 Prec@(1,5) (52.5%, 83.5%)
07/24 06:06:49 AM | Train: [ 9/50] Step 250/390 Loss 1.685 Prec@(1,5) (52.6%, 83.4%)
07/24 06:11:35 AM | Train: [ 9/50] Step 300/390 Loss 1.682 Prec@(1,5) (52.8%, 83.6%)
07/24 06:16:18 AM | Train: [ 9/50] Step 350/390 Loss 1.680 Prec@(1,5) (52.9%, 83.6%)
07/24 06:20:05 AM | Train: [ 9/50] Step 390/390 Loss 1.683 Prec@(1,5) (52.7%, 83.6%)
07/24 06:20:05 AM | Train: [ 9/50] Final Prec@1 52.7360%
07/24 06:20:06 AM | Valid: [ 9/50] Step 000/390 Loss 1.873 Prec@(1,5) (50.0%, 78.1%)
07/24 06:20:23 AM | Valid: [ 9/50] Step 050/390 Loss 1.892 Prec@(1,5) (49.0%, 79.3%)
07/24 06:20:39 AM | Valid: [ 9/50] Step 100/390 Loss 1.932 Prec@(1,5) (48.4%, 78.7%)
07/24 06:20:55 AM | Valid: [ 9/50] Step 150/390 Loss 1.919 Prec@(1,5) (48.2%, 79.2%)
07/24 06:21:11 AM | Valid: [ 9/50] Step 200/390 Loss 1.925 Prec@(1,5) (48.1%, 79.3%)
07/24 06:21:28 AM | Valid: [ 9/50] Step 250/390 Loss 1.920 Prec@(1,5) (48.2%, 79.4%)
07/24 06:21:44 AM | Valid: [ 9/50] Step 300/390 Loss 1.922 Prec@(1,5) (48.3%, 79.4%)
07/24 06:22:01 AM | Valid: [ 9/50] Step 350/390 Loss 1.926 Prec@(1,5) (48.2%, 79.4%)
07/24 06:22:14 AM | Valid: [ 9/50] Step 390/390 Loss 1.928 Prec@(1,5) (48.2%, 79.3%)
07/24 06:22:14 AM | Valid: [ 9/50] Final Prec@1 48.1720%
07/24 06:22:14 AM | genotype = Genotype(normal=[[('sep_conv_5x5', 1), ('sep_conv_5x5', 0)], [('sep_conv_5x5', 1), ('sep_conv_3x3', 2)], [('sep_conv_3x3', 1), ('dil_conv_3x3', 3)], [('dil_conv_5x5', 4), ('sep_conv_3x3', 2)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('dil_conv_5x5', 2)], [('sep_conv_5x5', 3), ('sep_conv_5x5', 2)], [('sep_conv_5x5', 4), ('max_pool_3x3', 0)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1257, 0.1057, 0.1318, 0.1307, 0.1363, 0.1302, 0.1203, 0.1194],
        [0.1049, 0.0924, 0.1124, 0.1261, 0.1504, 0.1270, 0.1414, 0.1453]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1358, 0.1099, 0.1345, 0.1230, 0.1219, 0.1275, 0.1191, 0.1284],
        [0.1049, 0.0898, 0.1088, 0.1326, 0.1471, 0.1362, 0.1292, 0.1514],
        [0.1066, 0.0908, 0.1195, 0.1380, 0.1344, 0.1271, 0.1258, 0.1578]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1303, 0.1064, 0.1264, 0.1298, 0.1371, 0.1162, 0.1288, 0.1249],
        [0.1027, 0.0886, 0.1058, 0.1482, 0.1397, 0.1372, 0.1252, 0.1527],
        [0.1029, 0.0884, 0.1125, 0.1301, 0.1248, 0.1410, 0.1362, 0.1641],
        [0.0928, 0.0833, 0.0956, 0.1365, 0.1445, 0.1468, 0.1303, 0.1702]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1289, 0.1050, 0.1229, 0.1336, 0.1356, 0.1205, 0.1233, 0.1302],
        [0.1062, 0.0888, 0.1058, 0.1324, 0.1306, 0.1363, 0.1318, 0.1681],
        [0.1016, 0.0847, 0.1051, 0.1412, 0.1371, 0.1369, 0.1284, 0.1650],
        [0.0957, 0.0831, 0.0951, 0.1305, 0.1341, 0.1378, 0.1373, 0.1864],
        [0.0942, 0.0827, 0.0902, 0.1446, 0.1296, 0.1341, 0.1489, 0.1758]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1451, 0.1251, 0.1247, 0.1123, 0.1316, 0.1214, 0.1245, 0.1153],
        [0.1379, 0.1199, 0.1187, 0.1235, 0.1268, 0.1174, 0.1241, 0.1318]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1475, 0.1274, 0.1311, 0.1206, 0.1224, 0.1151, 0.1163, 0.1197],
        [0.1351, 0.1167, 0.1300, 0.1194, 0.1249, 0.1168, 0.1287, 0.1285],
        [0.1186, 0.0984, 0.1220, 0.1279, 0.1388, 0.1248, 0.1395, 0.1299]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1355, 0.1199, 0.1280, 0.1217, 0.1359, 0.1269, 0.1167, 0.1156],
        [0.1361, 0.1171, 0.1350, 0.1240, 0.1249, 0.1222, 0.1197, 0.1210],
        [0.1110, 0.0968, 0.1216, 0.1339, 0.1363, 0.1344, 0.1330, 0.1329],
        [0.1055, 0.0915, 0.1143, 0.1379, 0.1386, 0.1348, 0.1379, 0.1394]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1452, 0.1249, 0.1227, 0.1297, 0.1224, 0.1173, 0.1181, 0.1197],
        [0.1382, 0.1174, 0.1301, 0.1248, 0.1254, 0.1201, 0.1245, 0.1194],
        [0.1203, 0.0967, 0.1258, 0.1240, 0.1346, 0.1305, 0.1330, 0.1351],
        [0.1128, 0.0956, 0.1213, 0.1407, 0.1373, 0.1258, 0.1310, 0.1355],
        [0.1130, 0.0956, 0.1173, 0.1279, 0.1467, 0.1293, 0.1406, 0.1296]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/24 06:22:21 AM | Train: [10/50] Step 000/390 Loss 1.436 Prec@(1,5) (57.8%, 89.1%)
07/24 06:27:07 AM | Train: [10/50] Step 050/390 Loss 1.547 Prec@(1,5) (55.4%, 86.1%)
07/24 06:31:52 AM | Train: [10/50] Step 100/390 Loss 1.534 Prec@(1,5) (56.0%, 85.9%)
07/24 06:36:37 AM | Train: [10/50] Step 150/390 Loss 1.543 Prec@(1,5) (56.0%, 85.8%)
07/24 06:41:21 AM | Train: [10/50] Step 200/390 Loss 1.554 Prec@(1,5) (55.8%, 85.6%)
07/24 06:46:03 AM | Train: [10/50] Step 250/390 Loss 1.558 Prec@(1,5) (55.6%, 85.5%)
07/24 06:50:45 AM | Train: [10/50] Step 300/390 Loss 1.570 Prec@(1,5) (55.3%, 85.5%)
07/24 06:55:27 AM | Train: [10/50] Step 350/390 Loss 1.575 Prec@(1,5) (55.1%, 85.4%)
07/24 06:59:12 AM | Train: [10/50] Step 390/390 Loss 1.580 Prec@(1,5) (54.9%, 85.3%)
07/24 06:59:12 AM | Train: [10/50] Final Prec@1 54.9040%
07/24 06:59:13 AM | Valid: [10/50] Step 000/390 Loss 2.135 Prec@(1,5) (40.6%, 79.7%)
07/24 06:59:29 AM | Valid: [10/50] Step 050/390 Loss 1.903 Prec@(1,5) (48.1%, 79.8%)
07/24 06:59:46 AM | Valid: [10/50] Step 100/390 Loss 1.931 Prec@(1,5) (48.1%, 79.3%)
07/24 07:00:03 AM | Valid: [10/50] Step 150/390 Loss 1.931 Prec@(1,5) (48.3%, 79.3%)
07/24 07:00:20 AM | Valid: [10/50] Step 200/390 Loss 1.919 Prec@(1,5) (48.6%, 79.7%)
07/24 07:00:37 AM | Valid: [10/50] Step 250/390 Loss 1.911 Prec@(1,5) (48.7%, 79.7%)
07/24 07:00:53 AM | Valid: [10/50] Step 300/390 Loss 1.922 Prec@(1,5) (48.3%, 79.6%)
07/24 07:01:09 AM | Valid: [10/50] Step 350/390 Loss 1.921 Prec@(1,5) (48.3%, 79.7%)
07/24 07:01:23 AM | Valid: [10/50] Step 390/390 Loss 1.921 Prec@(1,5) (48.3%, 79.6%)
07/24 07:01:23 AM | Valid: [10/50] Final Prec@1 48.3120%
07/24 07:01:23 AM | genotype = Genotype(normal=[[('sep_conv_5x5', 1), ('sep_conv_5x5', 0)], [('sep_conv_5x5', 1), ('sep_conv_3x3', 2)], [('sep_conv_3x3', 1), ('dil_conv_3x3', 3)], [('dil_conv_5x5', 4), ('sep_conv_3x3', 2)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('dil_conv_5x5', 2)], [('sep_conv_5x5', 3), ('sep_conv_5x5', 2)], [('sep_conv_5x5', 4), ('max_pool_3x3', 0)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1226, 0.1041, 0.1335, 0.1324, 0.1362, 0.1306, 0.1209, 0.1196],
        [0.1012, 0.0899, 0.1116, 0.1269, 0.1529, 0.1261, 0.1429, 0.1485]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1336, 0.1086, 0.1363, 0.1241, 0.1221, 0.1279, 0.1172, 0.1302],
        [0.1005, 0.0864, 0.1069, 0.1326, 0.1479, 0.1394, 0.1306, 0.1556],
        [0.1018, 0.0869, 0.1186, 0.1376, 0.1353, 0.1281, 0.1262, 0.1654]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1287, 0.1046, 0.1278, 0.1295, 0.1377, 0.1158, 0.1285, 0.1274],
        [0.0992, 0.0858, 0.1051, 0.1495, 0.1418, 0.1365, 0.1229, 0.1592],
        [0.0990, 0.0853, 0.1125, 0.1287, 0.1238, 0.1410, 0.1363, 0.1735],
        [0.0881, 0.0792, 0.0924, 0.1362, 0.1451, 0.1478, 0.1308, 0.1805]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1261, 0.1024, 0.1225, 0.1343, 0.1378, 0.1198, 0.1236, 0.1336],
        [0.1021, 0.0855, 0.1044, 0.1337, 0.1290, 0.1365, 0.1318, 0.1770],
        [0.0971, 0.0806, 0.1036, 0.1405, 0.1373, 0.1380, 0.1277, 0.1752],
        [0.0907, 0.0785, 0.0912, 0.1300, 0.1330, 0.1383, 0.1380, 0.2002],
        [0.0883, 0.0773, 0.0847, 0.1450, 0.1287, 0.1348, 0.1521, 0.1891]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1459, 0.1249, 0.1252, 0.1115, 0.1309, 0.1230, 0.1243, 0.1143],
        [0.1385, 0.1196, 0.1185, 0.1254, 0.1271, 0.1166, 0.1231, 0.1312]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1480, 0.1279, 0.1303, 0.1204, 0.1234, 0.1149, 0.1161, 0.1189],
        [0.1350, 0.1157, 0.1299, 0.1187, 0.1253, 0.1172, 0.1285, 0.1295],
        [0.1166, 0.0968, 0.1226, 0.1286, 0.1390, 0.1254, 0.1409, 0.1303]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1363, 0.1214, 0.1262, 0.1216, 0.1360, 0.1285, 0.1145, 0.1155],
        [0.1366, 0.1167, 0.1354, 0.1238, 0.1250, 0.1214, 0.1207, 0.1205],
        [0.1091, 0.0957, 0.1229, 0.1345, 0.1368, 0.1350, 0.1337, 0.1324],
        [0.1030, 0.0897, 0.1147, 0.1380, 0.1389, 0.1367, 0.1385, 0.1404]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1468, 0.1270, 0.1237, 0.1302, 0.1212, 0.1155, 0.1172, 0.1184],
        [0.1378, 0.1173, 0.1307, 0.1247, 0.1239, 0.1209, 0.1248, 0.1200],
        [0.1185, 0.0955, 0.1284, 0.1243, 0.1349, 0.1297, 0.1327, 0.1359],
        [0.1096, 0.0936, 0.1220, 0.1432, 0.1370, 0.1263, 0.1321, 0.1362],
        [0.1109, 0.0939, 0.1180, 0.1285, 0.1483, 0.1295, 0.1403, 0.1306]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/24 07:01:30 AM | Train: [11/50] Step 000/390 Loss 1.275 Prec@(1,5) (67.2%, 87.5%)
07/24 07:06:11 AM | Train: [11/50] Step 050/390 Loss 1.435 Prec@(1,5) (59.0%, 87.3%)
07/24 07:10:56 AM | Train: [11/50] Step 100/390 Loss 1.469 Prec@(1,5) (57.8%, 86.8%)
07/24 07:15:39 AM | Train: [11/50] Step 150/390 Loss 1.465 Prec@(1,5) (57.7%, 87.1%)
07/24 07:20:25 AM | Train: [11/50] Step 200/390 Loss 1.480 Prec@(1,5) (57.3%, 87.0%)
07/24 07:25:13 AM | Train: [11/50] Step 250/390 Loss 1.490 Prec@(1,5) (57.1%, 86.8%)
07/24 07:29:56 AM | Train: [11/50] Step 300/390 Loss 1.493 Prec@(1,5) (56.9%, 86.9%)
07/24 07:34:41 AM | Train: [11/50] Step 350/390 Loss 1.495 Prec@(1,5) (56.8%, 86.8%)
07/24 07:38:31 AM | Train: [11/50] Step 390/390 Loss 1.499 Prec@(1,5) (56.8%, 86.7%)
07/24 07:38:31 AM | Train: [11/50] Final Prec@1 56.7720%
07/24 07:38:32 AM | Valid: [11/50] Step 000/390 Loss 1.736 Prec@(1,5) (45.3%, 82.8%)
07/24 07:38:48 AM | Valid: [11/50] Step 050/390 Loss 1.911 Prec@(1,5) (47.9%, 80.2%)
07/24 07:39:04 AM | Valid: [11/50] Step 100/390 Loss 1.904 Prec@(1,5) (48.4%, 79.9%)
07/24 07:39:20 AM | Valid: [11/50] Step 150/390 Loss 1.899 Prec@(1,5) (48.4%, 79.9%)
07/24 07:39:37 AM | Valid: [11/50] Step 200/390 Loss 1.906 Prec@(1,5) (48.2%, 79.6%)
07/24 07:39:54 AM | Valid: [11/50] Step 250/390 Loss 1.908 Prec@(1,5) (48.4%, 79.7%)
07/24 07:40:10 AM | Valid: [11/50] Step 300/390 Loss 1.908 Prec@(1,5) (48.4%, 79.8%)
07/24 07:40:27 AM | Valid: [11/50] Step 350/390 Loss 1.909 Prec@(1,5) (48.6%, 79.8%)
07/24 07:40:40 AM | Valid: [11/50] Step 390/390 Loss 1.905 Prec@(1,5) (48.8%, 79.8%)
07/24 07:40:41 AM | Valid: [11/50] Final Prec@1 48.7640%
07/24 07:40:41 AM | genotype = Genotype(normal=[[('sep_conv_5x5', 1), ('skip_connect', 0)], [('sep_conv_5x5', 1), ('skip_connect', 0)], [('dil_conv_3x3', 3), ('sep_conv_3x3', 1)], [('dil_conv_5x5', 4), ('dil_conv_3x3', 3)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('dil_conv_5x5', 2)], [('max_pool_3x3', 1), ('dil_conv_5x5', 3)], [('max_pool_3x3', 0), ('sep_conv_5x5', 4)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1200, 0.1028, 0.1365, 0.1338, 0.1360, 0.1299, 0.1210, 0.1199],
        [0.0987, 0.0878, 0.1122, 0.1270, 0.1536, 0.1259, 0.1438, 0.1510]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1315, 0.1076, 0.1395, 0.1244, 0.1216, 0.1289, 0.1147, 0.1318],
        [0.0980, 0.0841, 0.1073, 0.1327, 0.1471, 0.1419, 0.1294, 0.1596],
        [0.0974, 0.0835, 0.1187, 0.1366, 0.1357, 0.1297, 0.1250, 0.1734]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1268, 0.1029, 0.1296, 0.1297, 0.1371, 0.1154, 0.1287, 0.1299],
        [0.0973, 0.0833, 0.1054, 0.1495, 0.1421, 0.1347, 0.1217, 0.1660],
        [0.0953, 0.0817, 0.1123, 0.1275, 0.1223, 0.1433, 0.1349, 0.1827],
        [0.0838, 0.0752, 0.0899, 0.1338, 0.1450, 0.1483, 0.1300, 0.1941]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1237, 0.1000, 0.1234, 0.1341, 0.1374, 0.1195, 0.1239, 0.1379],
        [0.0995, 0.0825, 0.1040, 0.1337, 0.1256, 0.1350, 0.1317, 0.1880],
        [0.0925, 0.0762, 0.1020, 0.1398, 0.1367, 0.1384, 0.1261, 0.1883],
        [0.0856, 0.0737, 0.0874, 0.1282, 0.1304, 0.1383, 0.1382, 0.2182],
        [0.0825, 0.0718, 0.0793, 0.1443, 0.1275, 0.1341, 0.1548, 0.2056]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1478, 0.1257, 0.1252, 0.1101, 0.1303, 0.1229, 0.1239, 0.1141],
        [0.1408, 0.1202, 0.1174, 0.1262, 0.1268, 0.1161, 0.1225, 0.1300]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1501, 0.1284, 0.1312, 0.1210, 0.1224, 0.1147, 0.1149, 0.1175],
        [0.1377, 0.1169, 0.1295, 0.1185, 0.1240, 0.1169, 0.1278, 0.1287],
        [0.1157, 0.0954, 0.1243, 0.1276, 0.1387, 0.1256, 0.1405, 0.1321]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1383, 0.1226, 0.1260, 0.1205, 0.1355, 0.1285, 0.1125, 0.1160],
        [0.1405, 0.1185, 0.1355, 0.1231, 0.1229, 0.1204, 0.1195, 0.1197],
        [0.1083, 0.0944, 0.1244, 0.1340, 0.1368, 0.1347, 0.1348, 0.1326],
        [0.1016, 0.0883, 0.1158, 0.1376, 0.1374, 0.1377, 0.1395, 0.1421]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1486, 0.1276, 0.1244, 0.1298, 0.1206, 0.1135, 0.1172, 0.1182],
        [0.1407, 0.1187, 0.1314, 0.1228, 0.1233, 0.1208, 0.1238, 0.1185],
        [0.1174, 0.0941, 0.1307, 0.1243, 0.1349, 0.1297, 0.1327, 0.1362],
        [0.1079, 0.0918, 0.1231, 0.1452, 0.1360, 0.1261, 0.1338, 0.1361],
        [0.1088, 0.0919, 0.1186, 0.1278, 0.1495, 0.1302, 0.1403, 0.1330]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/24 07:40:49 AM | Train: [12/50] Step 000/390 Loss 1.371 Prec@(1,5) (57.8%, 84.4%)
07/24 07:45:39 AM | Train: [12/50] Step 050/390 Loss 1.342 Prec@(1,5) (61.7%, 88.3%)
07/24 07:50:24 AM | Train: [12/50] Step 100/390 Loss 1.359 Prec@(1,5) (61.1%, 88.6%)
07/24 07:55:12 AM | Train: [12/50] Step 150/390 Loss 1.390 Prec@(1,5) (60.1%, 88.1%)
07/24 07:59:58 AM | Train: [12/50] Step 200/390 Loss 1.398 Prec@(1,5) (59.8%, 88.1%)
07/24 08:04:39 AM | Train: [12/50] Step 250/390 Loss 1.414 Prec@(1,5) (59.4%, 87.9%)
07/24 08:09:27 AM | Train: [12/50] Step 300/390 Loss 1.413 Prec@(1,5) (59.2%, 87.9%)
07/24 08:14:12 AM | Train: [12/50] Step 350/390 Loss 1.413 Prec@(1,5) (59.2%, 87.9%)
07/24 08:17:58 AM | Train: [12/50] Step 390/390 Loss 1.416 Prec@(1,5) (59.2%, 87.8%)
07/24 08:17:58 AM | Train: [12/50] Final Prec@1 59.2240%
07/24 08:17:59 AM | Valid: [12/50] Step 000/390 Loss 1.669 Prec@(1,5) (51.6%, 85.9%)
07/24 08:18:15 AM | Valid: [12/50] Step 050/390 Loss 1.867 Prec@(1,5) (49.9%, 80.8%)
07/24 08:18:34 AM | Valid: [12/50] Step 100/390 Loss 1.843 Prec@(1,5) (50.5%, 81.1%)
07/24 08:18:50 AM | Valid: [12/50] Step 150/390 Loss 1.835 Prec@(1,5) (50.7%, 81.3%)
07/24 08:19:07 AM | Valid: [12/50] Step 200/390 Loss 1.824 Prec@(1,5) (51.1%, 81.5%)
07/24 08:19:23 AM | Valid: [12/50] Step 250/390 Loss 1.837 Prec@(1,5) (50.6%, 81.2%)
07/24 08:19:39 AM | Valid: [12/50] Step 300/390 Loss 1.838 Prec@(1,5) (50.6%, 81.1%)
07/24 08:19:55 AM | Valid: [12/50] Step 350/390 Loss 1.830 Prec@(1,5) (50.8%, 81.2%)
07/24 08:20:08 AM | Valid: [12/50] Step 390/390 Loss 1.838 Prec@(1,5) (50.7%, 81.0%)
07/24 08:20:08 AM | Valid: [12/50] Final Prec@1 50.7360%
07/24 08:20:08 AM | genotype = Genotype(normal=[[('sep_conv_5x5', 1), ('skip_connect', 0)], [('sep_conv_5x5', 1), ('skip_connect', 0)], [('dil_conv_3x3', 3), ('sep_conv_3x3', 1)], [('dil_conv_5x5', 4), ('dil_conv_5x5', 3)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('dil_conv_5x5', 2)], [('dil_conv_5x5', 3), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('sep_conv_5x5', 4)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1172, 0.1021, 0.1404, 0.1356, 0.1360, 0.1294, 0.1191, 0.1202],
        [0.0961, 0.0860, 0.1130, 0.1270, 0.1541, 0.1248, 0.1443, 0.1547]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1292, 0.1068, 0.1428, 0.1242, 0.1218, 0.1288, 0.1133, 0.1331],
        [0.0952, 0.0819, 0.1075, 0.1319, 0.1471, 0.1424, 0.1292, 0.1649],
        [0.0923, 0.0796, 0.1178, 0.1362, 0.1349, 0.1317, 0.1233, 0.1842]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1244, 0.1018, 0.1318, 0.1295, 0.1359, 0.1148, 0.1290, 0.1327],
        [0.0942, 0.0808, 0.1048, 0.1493, 0.1428, 0.1345, 0.1203, 0.1732],
        [0.0909, 0.0780, 0.1113, 0.1275, 0.1177, 0.1442, 0.1352, 0.1954],
        [0.0791, 0.0711, 0.0869, 0.1326, 0.1427, 0.1483, 0.1286, 0.2107]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1210, 0.0980, 0.1241, 0.1349, 0.1361, 0.1173, 0.1257, 0.1429],
        [0.0963, 0.0798, 0.1031, 0.1326, 0.1238, 0.1345, 0.1315, 0.1983],
        [0.0874, 0.0719, 0.0997, 0.1385, 0.1361, 0.1390, 0.1255, 0.2020],
        [0.0803, 0.0690, 0.0832, 0.1258, 0.1294, 0.1367, 0.1383, 0.2373],
        [0.0771, 0.0668, 0.0744, 0.1433, 0.1250, 0.1334, 0.1572, 0.2229]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1497, 0.1262, 0.1260, 0.1097, 0.1294, 0.1230, 0.1242, 0.1117],
        [0.1426, 0.1210, 0.1159, 0.1263, 0.1251, 0.1162, 0.1220, 0.1310]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1519, 0.1295, 0.1311, 0.1194, 0.1226, 0.1150, 0.1142, 0.1163],
        [0.1384, 0.1168, 0.1291, 0.1190, 0.1231, 0.1176, 0.1283, 0.1278],
        [0.1134, 0.0937, 0.1250, 0.1272, 0.1401, 0.1256, 0.1426, 0.1324]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1406, 0.1247, 0.1237, 0.1210, 0.1350, 0.1285, 0.1111, 0.1155],
        [0.1409, 0.1187, 0.1354, 0.1229, 0.1213, 0.1207, 0.1204, 0.1196],
        [0.1058, 0.0928, 0.1259, 0.1352, 0.1379, 0.1340, 0.1358, 0.1327],
        [0.0994, 0.0870, 0.1175, 0.1366, 0.1382, 0.1375, 0.1407, 0.1431]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1511, 0.1303, 0.1243, 0.1298, 0.1197, 0.1121, 0.1157, 0.1170],
        [0.1415, 0.1197, 0.1313, 0.1218, 0.1233, 0.1208, 0.1228, 0.1189],
        [0.1153, 0.0927, 0.1338, 0.1245, 0.1364, 0.1290, 0.1314, 0.1368],
        [0.1060, 0.0907, 0.1257, 0.1450, 0.1346, 0.1259, 0.1349, 0.1372],
        [0.1062, 0.0897, 0.1193, 0.1274, 0.1499, 0.1305, 0.1411, 0.1359]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/24 08:20:16 AM | Train: [13/50] Step 000/390 Loss 1.048 Prec@(1,5) (67.2%, 89.1%)
07/24 08:24:57 AM | Train: [13/50] Step 050/390 Loss 1.298 Prec@(1,5) (61.6%, 90.0%)
07/24 08:29:42 AM | Train: [13/50] Step 100/390 Loss 1.308 Prec@(1,5) (61.5%, 89.9%)
07/24 08:34:26 AM | Train: [13/50] Step 150/390 Loss 1.314 Prec@(1,5) (61.5%, 89.8%)
07/24 08:39:05 AM | Train: [13/50] Step 200/390 Loss 1.318 Prec@(1,5) (61.3%, 89.5%)
07/24 08:43:48 AM | Train: [13/50] Step 250/390 Loss 1.323 Prec@(1,5) (61.2%, 89.5%)
07/24 08:48:32 AM | Train: [13/50] Step 300/390 Loss 1.330 Prec@(1,5) (61.2%, 89.3%)
07/24 08:53:16 AM | Train: [13/50] Step 350/390 Loss 1.335 Prec@(1,5) (61.1%, 89.1%)
07/24 08:57:03 AM | Train: [13/50] Step 390/390 Loss 1.336 Prec@(1,5) (61.0%, 89.0%)
07/24 08:57:03 AM | Train: [13/50] Final Prec@1 61.0440%
07/24 08:57:04 AM | Valid: [13/50] Step 000/390 Loss 1.592 Prec@(1,5) (54.7%, 85.9%)
07/24 08:57:20 AM | Valid: [13/50] Step 050/390 Loss 1.848 Prec@(1,5) (50.7%, 81.5%)
07/24 08:57:36 AM | Valid: [13/50] Step 100/390 Loss 1.844 Prec@(1,5) (50.9%, 81.3%)
07/24 08:57:52 AM | Valid: [13/50] Step 150/390 Loss 1.871 Prec@(1,5) (50.6%, 80.6%)
07/24 08:58:08 AM | Valid: [13/50] Step 200/390 Loss 1.867 Prec@(1,5) (50.7%, 80.8%)
07/24 08:58:25 AM | Valid: [13/50] Step 250/390 Loss 1.866 Prec@(1,5) (50.8%, 80.7%)
07/24 08:58:41 AM | Valid: [13/50] Step 300/390 Loss 1.871 Prec@(1,5) (50.6%, 80.7%)
07/24 08:58:57 AM | Valid: [13/50] Step 350/390 Loss 1.864 Prec@(1,5) (50.8%, 80.8%)
07/24 08:59:10 AM | Valid: [13/50] Step 390/390 Loss 1.864 Prec@(1,5) (50.8%, 80.8%)
07/24 08:59:10 AM | Valid: [13/50] Final Prec@1 50.7840%
07/24 08:59:10 AM | genotype = Genotype(normal=[[('sep_conv_5x5', 1), ('skip_connect', 0)], [('sep_conv_5x5', 1), ('skip_connect', 0)], [('dil_conv_3x3', 3), ('sep_conv_3x3', 1)], [('dil_conv_5x5', 4), ('dil_conv_5x5', 3)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('dil_conv_5x5', 2)], [('max_pool_3x3', 1), ('dil_conv_5x5', 3)], [('max_pool_3x3', 0), ('sep_conv_5x5', 4)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1154, 0.1012, 0.1440, 0.1372, 0.1357, 0.1285, 0.1190, 0.1190],
        [0.0930, 0.0838, 0.1130, 0.1278, 0.1538, 0.1234, 0.1447, 0.1604]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1281, 0.1063, 0.1468, 0.1248, 0.1211, 0.1279, 0.1117, 0.1333],
        [0.0915, 0.0792, 0.1066, 0.1329, 0.1444, 0.1438, 0.1285, 0.1732],
        [0.0876, 0.0758, 0.1164, 0.1359, 0.1343, 0.1330, 0.1224, 0.1946]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1239, 0.1010, 0.1336, 0.1292, 0.1361, 0.1130, 0.1289, 0.1343],
        [0.0912, 0.0784, 0.1048, 0.1484, 0.1422, 0.1337, 0.1179, 0.1835],
        [0.0869, 0.0749, 0.1112, 0.1260, 0.1141, 0.1435, 0.1340, 0.2092],
        [0.0748, 0.0675, 0.0841, 0.1300, 0.1412, 0.1478, 0.1270, 0.2277]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1191, 0.0960, 0.1249, 0.1352, 0.1339, 0.1156, 0.1274, 0.1479],
        [0.0924, 0.0771, 0.1024, 0.1310, 0.1215, 0.1335, 0.1305, 0.2116],
        [0.0827, 0.0680, 0.0979, 0.1372, 0.1328, 0.1387, 0.1233, 0.2194],
        [0.0749, 0.0644, 0.0791, 0.1229, 0.1268, 0.1337, 0.1392, 0.2591],
        [0.0717, 0.0622, 0.0695, 0.1412, 0.1234, 0.1320, 0.1560, 0.2438]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1517, 0.1271, 0.1262, 0.1094, 0.1298, 0.1216, 0.1240, 0.1101],
        [0.1433, 0.1219, 0.1143, 0.1274, 0.1249, 0.1148, 0.1217, 0.1317]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1536, 0.1312, 0.1302, 0.1183, 0.1236, 0.1150, 0.1129, 0.1152],
        [0.1403, 0.1184, 0.1288, 0.1178, 0.1232, 0.1164, 0.1287, 0.1264],
        [0.1122, 0.0929, 0.1268, 0.1267, 0.1399, 0.1259, 0.1429, 0.1328]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1416, 0.1269, 0.1214, 0.1215, 0.1346, 0.1295, 0.1100, 0.1145],
        [0.1425, 0.1203, 0.1354, 0.1223, 0.1203, 0.1209, 0.1196, 0.1187],
        [0.1037, 0.0917, 0.1282, 0.1347, 0.1382, 0.1345, 0.1354, 0.1336],
        [0.0971, 0.0854, 0.1189, 0.1358, 0.1382, 0.1383, 0.1411, 0.1451]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1512, 0.1307, 0.1253, 0.1312, 0.1200, 0.1112, 0.1151, 0.1153],
        [0.1426, 0.1213, 0.1312, 0.1213, 0.1221, 0.1216, 0.1218, 0.1181],
        [0.1132, 0.0919, 0.1362, 0.1225, 0.1365, 0.1302, 0.1312, 0.1383],
        [0.1037, 0.0890, 0.1270, 0.1467, 0.1340, 0.1241, 0.1368, 0.1388],
        [0.1039, 0.0881, 0.1203, 0.1265, 0.1491, 0.1324, 0.1403, 0.1394]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/24 08:59:18 AM | Train: [14/50] Step 000/390 Loss 1.534 Prec@(1,5) (59.4%, 89.1%)
07/24 09:03:57 AM | Train: [14/50] Step 050/390 Loss 1.212 Prec@(1,5) (64.2%, 90.8%)
07/24 09:08:47 AM | Train: [14/50] Step 100/390 Loss 1.217 Prec@(1,5) (64.4%, 90.8%)
07/24 09:13:29 AM | Train: [14/50] Step 150/390 Loss 1.232 Prec@(1,5) (64.2%, 90.6%)
07/24 09:18:09 AM | Train: [14/50] Step 200/390 Loss 1.248 Prec@(1,5) (64.0%, 90.2%)
07/24 09:22:55 AM | Train: [14/50] Step 250/390 Loss 1.247 Prec@(1,5) (63.9%, 90.3%)
07/24 09:27:39 AM | Train: [14/50] Step 300/390 Loss 1.254 Prec@(1,5) (63.6%, 90.2%)
07/24 09:32:27 AM | Train: [14/50] Step 350/390 Loss 1.267 Prec@(1,5) (63.2%, 90.0%)
07/24 09:36:16 AM | Train: [14/50] Step 390/390 Loss 1.275 Prec@(1,5) (63.1%, 89.9%)
07/24 09:36:16 AM | Train: [14/50] Final Prec@1 63.0520%
07/24 09:36:16 AM | Valid: [14/50] Step 000/390 Loss 1.545 Prec@(1,5) (60.9%, 82.8%)
07/24 09:36:33 AM | Valid: [14/50] Step 050/390 Loss 1.766 Prec@(1,5) (53.6%, 82.5%)
07/24 09:36:49 AM | Valid: [14/50] Step 100/390 Loss 1.770 Prec@(1,5) (53.2%, 82.0%)
07/24 09:37:05 AM | Valid: [14/50] Step 150/390 Loss 1.772 Prec@(1,5) (52.7%, 82.1%)
07/24 09:37:22 AM | Valid: [14/50] Step 200/390 Loss 1.770 Prec@(1,5) (52.7%, 82.3%)
07/24 09:37:38 AM | Valid: [14/50] Step 250/390 Loss 1.762 Prec@(1,5) (52.8%, 82.5%)
07/24 09:37:54 AM | Valid: [14/50] Step 300/390 Loss 1.760 Prec@(1,5) (52.8%, 82.5%)
07/24 09:38:11 AM | Valid: [14/50] Step 350/390 Loss 1.763 Prec@(1,5) (52.6%, 82.5%)
07/24 09:38:24 AM | Valid: [14/50] Step 390/390 Loss 1.768 Prec@(1,5) (52.7%, 82.3%)
07/24 09:38:24 AM | Valid: [14/50] Final Prec@1 52.7200%
07/24 09:38:24 AM | genotype = Genotype(normal=[[('sep_conv_5x5', 1), ('skip_connect', 0)], [('skip_connect', 0), ('dil_conv_3x3', 1)], [('dil_conv_3x3', 3), ('sep_conv_3x3', 1)], [('dil_conv_5x5', 4), ('dil_conv_5x5', 3)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('dil_conv_5x5', 2)], [('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('sep_conv_3x3', 3)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1136, 0.1005, 0.1480, 0.1403, 0.1352, 0.1271, 0.1177, 0.1175],
        [0.0902, 0.0815, 0.1129, 0.1270, 0.1528, 0.1228, 0.1463, 0.1665]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1276, 0.1060, 0.1512, 0.1251, 0.1193, 0.1271, 0.1096, 0.1339],
        [0.0889, 0.0769, 0.1065, 0.1342, 0.1429, 0.1433, 0.1261, 0.1812],
        [0.0839, 0.0728, 0.1162, 0.1359, 0.1340, 0.1321, 0.1202, 0.2049]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1228, 0.0999, 0.1355, 0.1290, 0.1370, 0.1121, 0.1278, 0.1359],
        [0.0888, 0.0762, 0.1047, 0.1469, 0.1401, 0.1346, 0.1164, 0.1923],
        [0.0834, 0.0719, 0.1119, 0.1241, 0.1104, 0.1413, 0.1314, 0.2257],
        [0.0704, 0.0635, 0.0809, 0.1272, 0.1382, 0.1479, 0.1246, 0.2473]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1176, 0.0940, 0.1256, 0.1387, 0.1331, 0.1117, 0.1271, 0.1522],
        [0.0896, 0.0748, 0.1019, 0.1295, 0.1180, 0.1318, 0.1287, 0.2256],
        [0.0788, 0.0646, 0.0969, 0.1355, 0.1292, 0.1379, 0.1195, 0.2377],
        [0.0702, 0.0602, 0.0753, 0.1189, 0.1242, 0.1312, 0.1368, 0.2832],
        [0.0668, 0.0579, 0.0649, 0.1387, 0.1203, 0.1309, 0.1538, 0.2667]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1545, 0.1289, 0.1260, 0.1086, 0.1292, 0.1219, 0.1219, 0.1089],
        [0.1454, 0.1231, 0.1141, 0.1277, 0.1234, 0.1141, 0.1210, 0.1313]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1553, 0.1320, 0.1304, 0.1182, 0.1225, 0.1158, 0.1119, 0.1140],
        [0.1427, 0.1200, 0.1289, 0.1183, 0.1214, 0.1159, 0.1286, 0.1242],
        [0.1103, 0.0918, 0.1291, 0.1259, 0.1381, 0.1265, 0.1440, 0.1343]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1435, 0.1284, 0.1205, 0.1210, 0.1328, 0.1309, 0.1093, 0.1136],
        [0.1448, 0.1214, 0.1344, 0.1218, 0.1188, 0.1208, 0.1207, 0.1173],
        [0.1017, 0.0904, 0.1305, 0.1346, 0.1375, 0.1342, 0.1369, 0.1341],
        [0.0953, 0.0841, 0.1213, 0.1345, 0.1372, 0.1380, 0.1409, 0.1486]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1532, 0.1329, 0.1259, 0.1316, 0.1187, 0.1091, 0.1146, 0.1140],
        [0.1458, 0.1231, 0.1311, 0.1195, 0.1224, 0.1215, 0.1204, 0.1162],
        [0.1115, 0.0906, 0.1396, 0.1218, 0.1365, 0.1307, 0.1304, 0.1388],
        [0.1021, 0.0875, 0.1300, 0.1470, 0.1320, 0.1235, 0.1365, 0.1412],
        [0.1017, 0.0867, 0.1222, 0.1264, 0.1477, 0.1325, 0.1397, 0.1430]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/24 09:38:31 AM | Train: [15/50] Step 000/390 Loss 1.454 Prec@(1,5) (64.1%, 87.5%)
07/24 09:43:17 AM | Train: [15/50] Step 050/390 Loss 1.150 Prec@(1,5) (66.6%, 91.6%)
07/24 09:48:02 AM | Train: [15/50] Step 100/390 Loss 1.153 Prec@(1,5) (66.5%, 91.6%)
07/24 09:52:48 AM | Train: [15/50] Step 150/390 Loss 1.146 Prec@(1,5) (66.8%, 91.8%)
07/24 09:57:29 AM | Train: [15/50] Step 200/390 Loss 1.159 Prec@(1,5) (66.1%, 91.7%)
07/24 10:02:09 AM | Train: [15/50] Step 250/390 Loss 1.164 Prec@(1,5) (65.8%, 91.6%)
07/24 10:06:53 AM | Train: [15/50] Step 300/390 Loss 1.172 Prec@(1,5) (65.5%, 91.5%)
07/24 10:11:39 AM | Train: [15/50] Step 350/390 Loss 1.185 Prec@(1,5) (65.3%, 91.3%)
07/24 10:15:28 AM | Train: [15/50] Step 390/390 Loss 1.193 Prec@(1,5) (64.9%, 91.2%)
07/24 10:15:28 AM | Train: [15/50] Final Prec@1 64.9360%
07/24 10:15:29 AM | Valid: [15/50] Step 000/390 Loss 1.962 Prec@(1,5) (54.7%, 81.2%)
07/24 10:15:48 AM | Valid: [15/50] Step 050/390 Loss 1.836 Prec@(1,5) (51.8%, 81.3%)
07/24 10:16:05 AM | Valid: [15/50] Step 100/390 Loss 1.809 Prec@(1,5) (52.5%, 81.4%)
07/24 10:16:21 AM | Valid: [15/50] Step 150/390 Loss 1.810 Prec@(1,5) (52.2%, 81.2%)
07/24 10:16:37 AM | Valid: [15/50] Step 200/390 Loss 1.801 Prec@(1,5) (52.6%, 81.6%)
07/24 10:16:54 AM | Valid: [15/50] Step 250/390 Loss 1.805 Prec@(1,5) (52.4%, 81.6%)
07/24 10:17:11 AM | Valid: [15/50] Step 300/390 Loss 1.807 Prec@(1,5) (52.6%, 81.7%)
07/24 10:17:27 AM | Valid: [15/50] Step 350/390 Loss 1.802 Prec@(1,5) (52.9%, 81.7%)
07/24 10:17:42 AM | Valid: [15/50] Step 390/390 Loss 1.797 Prec@(1,5) (53.1%, 81.8%)
07/24 10:17:42 AM | Valid: [15/50] Final Prec@1 53.0600%
07/24 10:17:42 AM | genotype = Genotype(normal=[[('sep_conv_5x5', 1), ('skip_connect', 0)], [('skip_connect', 0), ('dil_conv_3x3', 1)], [('dil_conv_3x3', 3), ('sep_conv_3x3', 1)], [('dil_conv_5x5', 4), ('dil_conv_5x5', 3)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('sep_conv_3x3', 3)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1112, 0.0990, 0.1513, 0.1431, 0.1338, 0.1273, 0.1169, 0.1173],
        [0.0877, 0.0795, 0.1134, 0.1265, 0.1532, 0.1221, 0.1466, 0.1710]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1253, 0.1046, 0.1549, 0.1244, 0.1191, 0.1274, 0.1086, 0.1356],
        [0.0866, 0.0750, 0.1075, 0.1336, 0.1396, 0.1436, 0.1249, 0.1892],
        [0.0803, 0.0696, 0.1161, 0.1340, 0.1322, 0.1314, 0.1191, 0.2173]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1217, 0.0989, 0.1380, 0.1282, 0.1366, 0.1110, 0.1277, 0.1379],
        [0.0867, 0.0742, 0.1050, 0.1467, 0.1387, 0.1317, 0.1155, 0.2016],
        [0.0802, 0.0691, 0.1121, 0.1231, 0.1067, 0.1399, 0.1283, 0.2406],
        [0.0662, 0.0600, 0.0782, 0.1255, 0.1346, 0.1447, 0.1224, 0.2683]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1153, 0.0918, 0.1269, 0.1377, 0.1326, 0.1097, 0.1280, 0.1579],
        [0.0866, 0.0724, 0.1017, 0.1271, 0.1156, 0.1285, 0.1281, 0.2399],
        [0.0752, 0.0616, 0.0969, 0.1327, 0.1237, 0.1345, 0.1153, 0.2599],
        [0.0658, 0.0564, 0.0722, 0.1144, 0.1199, 0.1268, 0.1340, 0.3105],
        [0.0621, 0.0539, 0.0607, 0.1357, 0.1170, 0.1285, 0.1513, 0.2908]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1562, 0.1300, 0.1272, 0.1084, 0.1277, 0.1214, 0.1209, 0.1082],
        [0.1467, 0.1238, 0.1135, 0.1279, 0.1225, 0.1136, 0.1214, 0.1307]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1563, 0.1334, 0.1311, 0.1186, 0.1222, 0.1154, 0.1108, 0.1122],
        [0.1452, 0.1220, 0.1288, 0.1182, 0.1208, 0.1148, 0.1275, 0.1226],
        [0.1085, 0.0908, 0.1319, 0.1251, 0.1368, 0.1268, 0.1440, 0.1362]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1438, 0.1295, 0.1179, 0.1213, 0.1332, 0.1324, 0.1088, 0.1130],
        [0.1470, 0.1227, 0.1338, 0.1211, 0.1178, 0.1205, 0.1207, 0.1164],
        [0.0999, 0.0891, 0.1323, 0.1352, 0.1379, 0.1331, 0.1379, 0.1346],
        [0.0929, 0.0819, 0.1219, 0.1367, 0.1363, 0.1388, 0.1418, 0.1496]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1548, 0.1355, 0.1257, 0.1319, 0.1174, 0.1081, 0.1131, 0.1135],
        [0.1472, 0.1245, 0.1320, 0.1182, 0.1227, 0.1209, 0.1190, 0.1155],
        [0.1097, 0.0898, 0.1443, 0.1203, 0.1366, 0.1292, 0.1300, 0.1401],
        [0.0999, 0.0858, 0.1321, 0.1468, 0.1307, 0.1242, 0.1383, 0.1421],
        [0.0996, 0.0855, 0.1242, 0.1252, 0.1462, 0.1334, 0.1404, 0.1457]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/24 10:17:50 AM | Train: [16/50] Step 000/390 Loss 1.066 Prec@(1,5) (71.9%, 92.2%)
07/24 10:22:32 AM | Train: [16/50] Step 050/390 Loss 1.078 Prec@(1,5) (68.0%, 93.0%)
07/24 10:27:18 AM | Train: [16/50] Step 100/390 Loss 1.103 Prec@(1,5) (66.9%, 92.6%)
07/24 10:32:03 AM | Train: [16/50] Step 150/390 Loss 1.098 Prec@(1,5) (66.7%, 92.7%)
07/24 10:36:46 AM | Train: [16/50] Step 200/390 Loss 1.100 Prec@(1,5) (66.6%, 92.6%)
07/24 10:41:32 AM | Train: [16/50] Step 250/390 Loss 1.113 Prec@(1,5) (66.5%, 92.4%)
07/24 10:46:17 AM | Train: [16/50] Step 300/390 Loss 1.113 Prec@(1,5) (66.7%, 92.2%)
07/24 10:51:03 AM | Train: [16/50] Step 350/390 Loss 1.125 Prec@(1,5) (66.4%, 92.0%)
07/24 10:54:49 AM | Train: [16/50] Step 390/390 Loss 1.127 Prec@(1,5) (66.4%, 92.0%)
07/24 10:54:49 AM | Train: [16/50] Final Prec@1 66.3920%
07/24 10:54:50 AM | Valid: [16/50] Step 000/390 Loss 1.954 Prec@(1,5) (46.9%, 81.2%)
07/24 10:55:06 AM | Valid: [16/50] Step 050/390 Loss 1.754 Prec@(1,5) (53.1%, 82.1%)
07/24 10:55:22 AM | Valid: [16/50] Step 100/390 Loss 1.750 Prec@(1,5) (54.3%, 82.2%)
07/24 10:55:40 AM | Valid: [16/50] Step 150/390 Loss 1.749 Prec@(1,5) (53.8%, 82.2%)
07/24 10:56:00 AM | Valid: [16/50] Step 200/390 Loss 1.749 Prec@(1,5) (54.0%, 82.2%)
07/24 10:56:17 AM | Valid: [16/50] Step 250/390 Loss 1.746 Prec@(1,5) (53.9%, 82.5%)
07/24 10:56:36 AM | Valid: [16/50] Step 300/390 Loss 1.748 Prec@(1,5) (53.8%, 82.6%)
07/24 10:56:53 AM | Valid: [16/50] Step 350/390 Loss 1.740 Prec@(1,5) (53.9%, 82.7%)
07/24 10:57:07 AM | Valid: [16/50] Step 390/390 Loss 1.747 Prec@(1,5) (53.8%, 82.6%)
07/24 10:57:07 AM | Valid: [16/50] Final Prec@1 53.7520%
07/24 10:57:07 AM | genotype = Genotype(normal=[[('skip_connect', 0), ('sep_conv_5x5', 1)], [('skip_connect', 0), ('dil_conv_3x3', 1)], [('dil_conv_3x3', 3), ('sep_conv_3x3', 1)], [('dil_conv_5x5', 4), ('dil_conv_5x5', 3)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('skip_connect', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1100, 0.0995, 0.1571, 0.1449, 0.1317, 0.1259, 0.1155, 0.1154],
        [0.0846, 0.0772, 0.1132, 0.1255, 0.1516, 0.1216, 0.1473, 0.1790]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1239, 0.1049, 0.1604, 0.1245, 0.1174, 0.1268, 0.1068, 0.1353],
        [0.0836, 0.0730, 0.1077, 0.1331, 0.1373, 0.1433, 0.1221, 0.2000],
        [0.0762, 0.0665, 0.1155, 0.1332, 0.1295, 0.1325, 0.1161, 0.2305]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1209, 0.0989, 0.1423, 0.1270, 0.1354, 0.1104, 0.1252, 0.1399],
        [0.0839, 0.0721, 0.1052, 0.1452, 0.1366, 0.1295, 0.1137, 0.2137],
        [0.0763, 0.0662, 0.1123, 0.1214, 0.1034, 0.1373, 0.1247, 0.2584],
        [0.0620, 0.0564, 0.0750, 0.1231, 0.1312, 0.1429, 0.1193, 0.2900]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1133, 0.0905, 0.1286, 0.1381, 0.1324, 0.1079, 0.1268, 0.1624],
        [0.0831, 0.0698, 0.1012, 0.1251, 0.1112, 0.1258, 0.1260, 0.2577],
        [0.0709, 0.0583, 0.0954, 0.1298, 0.1192, 0.1321, 0.1124, 0.2819],
        [0.0609, 0.0524, 0.0682, 0.1093, 0.1152, 0.1229, 0.1301, 0.3411],
        [0.0572, 0.0497, 0.0562, 0.1321, 0.1145, 0.1249, 0.1470, 0.3183]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1589, 0.1317, 0.1285, 0.1073, 0.1263, 0.1213, 0.1189, 0.1071],
        [0.1491, 0.1263, 0.1124, 0.1276, 0.1218, 0.1124, 0.1206, 0.1299]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1588, 0.1354, 0.1311, 0.1174, 0.1220, 0.1154, 0.1102, 0.1097],
        [0.1473, 0.1240, 0.1289, 0.1175, 0.1196, 0.1149, 0.1263, 0.1216],
        [0.1064, 0.0903, 0.1346, 0.1241, 0.1359, 0.1261, 0.1448, 0.1377]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1454, 0.1311, 0.1175, 0.1212, 0.1320, 0.1341, 0.1076, 0.1111],
        [0.1495, 0.1252, 0.1338, 0.1197, 0.1165, 0.1198, 0.1205, 0.1150],
        [0.0978, 0.0883, 0.1350, 0.1347, 0.1373, 0.1321, 0.1383, 0.1365],
        [0.0909, 0.0809, 0.1239, 0.1357, 0.1352, 0.1390, 0.1417, 0.1526]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1571, 0.1387, 0.1269, 0.1313, 0.1148, 0.1072, 0.1122, 0.1120],
        [0.1489, 0.1273, 0.1327, 0.1179, 0.1213, 0.1203, 0.1177, 0.1141],
        [0.1072, 0.0893, 0.1482, 0.1198, 0.1355, 0.1286, 0.1296, 0.1419],
        [0.0979, 0.0848, 0.1353, 0.1471, 0.1291, 0.1225, 0.1384, 0.1450],
        [0.0973, 0.0843, 0.1264, 0.1258, 0.1435, 0.1332, 0.1399, 0.1496]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/24 10:57:15 AM | Train: [17/50] Step 000/390 Loss 0.917 Prec@(1,5) (71.9%, 93.8%)
07/24 11:01:57 AM | Train: [17/50] Step 050/390 Loss 0.992 Prec@(1,5) (70.0%, 94.0%)
07/24 11:06:38 AM | Train: [17/50] Step 100/390 Loss 1.012 Prec@(1,5) (69.8%, 93.7%)
07/24 11:11:22 AM | Train: [17/50] Step 150/390 Loss 1.017 Prec@(1,5) (69.6%, 93.7%)
07/24 11:16:09 AM | Train: [17/50] Step 200/390 Loss 1.024 Prec@(1,5) (69.3%, 93.5%)
07/24 11:21:00 AM | Train: [17/50] Step 250/390 Loss 1.033 Prec@(1,5) (69.1%, 93.3%)
07/24 11:25:48 AM | Train: [17/50] Step 300/390 Loss 1.045 Prec@(1,5) (68.8%, 93.1%)
07/24 11:30:32 AM | Train: [17/50] Step 350/390 Loss 1.059 Prec@(1,5) (68.6%, 92.9%)
07/24 11:34:22 AM | Train: [17/50] Step 390/390 Loss 1.063 Prec@(1,5) (68.5%, 92.8%)
07/24 11:34:22 AM | Train: [17/50] Final Prec@1 68.4720%
07/24 11:34:23 AM | Valid: [17/50] Step 000/390 Loss 1.655 Prec@(1,5) (57.8%, 79.7%)
07/24 11:34:40 AM | Valid: [17/50] Step 050/390 Loss 1.795 Prec@(1,5) (53.2%, 82.5%)
07/24 11:34:57 AM | Valid: [17/50] Step 100/390 Loss 1.795 Prec@(1,5) (53.1%, 82.1%)
07/24 11:35:14 AM | Valid: [17/50] Step 150/390 Loss 1.799 Prec@(1,5) (53.2%, 81.7%)
07/24 11:35:32 AM | Valid: [17/50] Step 200/390 Loss 1.781 Prec@(1,5) (53.6%, 82.0%)
07/24 11:35:49 AM | Valid: [17/50] Step 250/390 Loss 1.774 Prec@(1,5) (53.4%, 82.2%)
07/24 11:36:05 AM | Valid: [17/50] Step 300/390 Loss 1.767 Prec@(1,5) (53.6%, 82.2%)
07/24 11:36:22 AM | Valid: [17/50] Step 350/390 Loss 1.773 Prec@(1,5) (53.4%, 82.2%)
07/24 11:36:35 AM | Valid: [17/50] Step 390/390 Loss 1.780 Prec@(1,5) (53.2%, 82.1%)
07/24 11:36:35 AM | Valid: [17/50] Final Prec@1 53.2040%
07/24 11:36:35 AM | genotype = Genotype(normal=[[('skip_connect', 0), ('sep_conv_5x5', 1)], [('skip_connect', 0), ('dil_conv_3x3', 1)], [('dil_conv_3x3', 3), ('sep_conv_3x3', 1)], [('dil_conv_5x5', 4), ('dil_conv_5x5', 3)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('skip_connect', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1075, 0.0985, 0.1611, 0.1474, 0.1308, 0.1253, 0.1155, 0.1139],
        [0.0813, 0.0743, 0.1122, 0.1244, 0.1508, 0.1208, 0.1484, 0.1878]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1209, 0.1037, 0.1646, 0.1248, 0.1164, 0.1268, 0.1062, 0.1366],
        [0.0805, 0.0708, 0.1077, 0.1307, 0.1349, 0.1447, 0.1210, 0.2097],
        [0.0721, 0.0635, 0.1149, 0.1320, 0.1269, 0.1318, 0.1139, 0.2450]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1189, 0.0979, 0.1452, 0.1262, 0.1342, 0.1097, 0.1249, 0.1431],
        [0.0807, 0.0694, 0.1045, 0.1437, 0.1343, 0.1277, 0.1116, 0.2281],
        [0.0718, 0.0629, 0.1110, 0.1202, 0.0986, 0.1363, 0.1227, 0.2765],
        [0.0576, 0.0526, 0.0714, 0.1199, 0.1283, 0.1399, 0.1158, 0.3145]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1100, 0.0882, 0.1289, 0.1372, 0.1328, 0.1065, 0.1271, 0.1693],
        [0.0795, 0.0669, 0.1000, 0.1225, 0.1079, 0.1233, 0.1239, 0.2760],
        [0.0667, 0.0550, 0.0950, 0.1258, 0.1131, 0.1277, 0.1085, 0.3083],
        [0.0564, 0.0486, 0.0648, 0.1031, 0.1096, 0.1167, 0.1266, 0.3742],
        [0.0527, 0.0459, 0.0522, 0.1273, 0.1103, 0.1216, 0.1421, 0.3479]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1608, 0.1328, 0.1281, 0.1069, 0.1261, 0.1204, 0.1188, 0.1061],
        [0.1499, 0.1268, 0.1115, 0.1270, 0.1228, 0.1128, 0.1192, 0.1300]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1598, 0.1363, 0.1314, 0.1165, 0.1220, 0.1162, 0.1098, 0.1079],
        [0.1491, 0.1256, 0.1294, 0.1165, 0.1185, 0.1146, 0.1260, 0.1202],
        [0.1042, 0.0890, 0.1370, 0.1231, 0.1355, 0.1265, 0.1452, 0.1395]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1472, 0.1330, 0.1156, 0.1217, 0.1318, 0.1336, 0.1068, 0.1104],
        [0.1516, 0.1268, 0.1342, 0.1190, 0.1152, 0.1192, 0.1205, 0.1135],
        [0.0963, 0.0875, 0.1379, 0.1341, 0.1361, 0.1321, 0.1383, 0.1377],
        [0.0895, 0.0802, 0.1266, 0.1342, 0.1350, 0.1381, 0.1421, 0.1543]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1582, 0.1403, 0.1280, 0.1310, 0.1140, 0.1056, 0.1119, 0.1109],
        [0.1505, 0.1288, 0.1329, 0.1173, 0.1213, 0.1194, 0.1171, 0.1128],
        [0.1049, 0.0882, 0.1515, 0.1191, 0.1357, 0.1289, 0.1291, 0.1427],
        [0.0954, 0.0836, 0.1382, 0.1463, 0.1270, 0.1232, 0.1391, 0.1470],
        [0.0951, 0.0826, 0.1281, 0.1261, 0.1409, 0.1346, 0.1400, 0.1525]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/24 11:36:43 AM | Train: [18/50] Step 000/390 Loss 1.070 Prec@(1,5) (68.8%, 92.2%)
07/24 11:41:25 AM | Train: [18/50] Step 050/390 Loss 0.930 Prec@(1,5) (72.1%, 94.6%)
07/24 11:46:07 AM | Train: [18/50] Step 100/390 Loss 0.939 Prec@(1,5) (71.7%, 94.4%)
07/24 11:50:51 AM | Train: [18/50] Step 150/390 Loss 0.946 Prec@(1,5) (71.3%, 94.3%)
07/24 11:55:37 AM | Train: [18/50] Step 200/390 Loss 0.957 Prec@(1,5) (71.2%, 94.2%)
07/24 12:00:23 PM | Train: [18/50] Step 250/390 Loss 0.966 Prec@(1,5) (70.8%, 94.1%)
07/24 12:05:07 PM | Train: [18/50] Step 300/390 Loss 0.984 Prec@(1,5) (70.5%, 93.9%)
07/24 12:09:51 PM | Train: [18/50] Step 350/390 Loss 1.001 Prec@(1,5) (69.9%, 93.7%)
07/24 12:13:40 PM | Train: [18/50] Step 390/390 Loss 1.009 Prec@(1,5) (69.7%, 93.6%)
07/24 12:13:41 PM | Train: [18/50] Final Prec@1 69.7360%
07/24 12:13:41 PM | Valid: [18/50] Step 000/390 Loss 1.413 Prec@(1,5) (60.9%, 87.5%)
07/24 12:13:57 PM | Valid: [18/50] Step 050/390 Loss 1.674 Prec@(1,5) (55.8%, 83.8%)
07/24 12:14:13 PM | Valid: [18/50] Step 100/390 Loss 1.693 Prec@(1,5) (55.4%, 83.5%)
07/24 12:14:30 PM | Valid: [18/50] Step 150/390 Loss 1.680 Prec@(1,5) (55.7%, 83.9%)
07/24 12:14:46 PM | Valid: [18/50] Step 200/390 Loss 1.680 Prec@(1,5) (55.6%, 83.7%)
07/24 12:15:03 PM | Valid: [18/50] Step 250/390 Loss 1.684 Prec@(1,5) (55.5%, 83.7%)
07/24 12:15:20 PM | Valid: [18/50] Step 300/390 Loss 1.686 Prec@(1,5) (55.5%, 83.6%)
07/24 12:15:36 PM | Valid: [18/50] Step 350/390 Loss 1.694 Prec@(1,5) (55.3%, 83.5%)
07/24 12:15:49 PM | Valid: [18/50] Step 390/390 Loss 1.694 Prec@(1,5) (55.4%, 83.5%)
07/24 12:15:49 PM | Valid: [18/50] Final Prec@1 55.4240%
07/24 12:15:49 PM | genotype = Genotype(normal=[[('skip_connect', 0), ('sep_conv_5x5', 1)], [('skip_connect', 0), ('dil_conv_3x3', 1)], [('dil_conv_3x3', 3), ('dil_conv_3x3', 2)], [('dil_conv_5x5', 4), ('dil_conv_5x5', 3)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('skip_connect', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1045, 0.0974, 0.1653, 0.1496, 0.1298, 0.1251, 0.1159, 0.1123],
        [0.0785, 0.0722, 0.1122, 0.1239, 0.1495, 0.1194, 0.1480, 0.1962]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1174, 0.1026, 0.1686, 0.1250, 0.1149, 0.1278, 0.1060, 0.1377],
        [0.0781, 0.0692, 0.1086, 0.1301, 0.1328, 0.1438, 0.1178, 0.2196],
        [0.0683, 0.0605, 0.1145, 0.1291, 0.1235, 0.1305, 0.1114, 0.2622]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1160, 0.0969, 0.1482, 0.1259, 0.1321, 0.1094, 0.1241, 0.1474],
        [0.0783, 0.0676, 0.1049, 0.1405, 0.1312, 0.1255, 0.1092, 0.2427],
        [0.0675, 0.0596, 0.1098, 0.1172, 0.0937, 0.1344, 0.1193, 0.2984],
        [0.0533, 0.0492, 0.0679, 0.1159, 0.1244, 0.1368, 0.1117, 0.3407]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1071, 0.0868, 0.1310, 0.1347, 0.1315, 0.1045, 0.1268, 0.1775],
        [0.0765, 0.0646, 0.0997, 0.1188, 0.1048, 0.1203, 0.1208, 0.2944],
        [0.0627, 0.0521, 0.0938, 0.1214, 0.1083, 0.1239, 0.1038, 0.3340],
        [0.0519, 0.0448, 0.0607, 0.0976, 0.1042, 0.1124, 0.1214, 0.4070],
        [0.0484, 0.0424, 0.0483, 0.1219, 0.1052, 0.1170, 0.1371, 0.3797]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1629, 0.1345, 0.1290, 0.1059, 0.1255, 0.1202, 0.1175, 0.1045],
        [0.1524, 0.1295, 0.1103, 0.1263, 0.1216, 0.1118, 0.1182, 0.1298]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1617, 0.1382, 0.1317, 0.1152, 0.1212, 0.1167, 0.1095, 0.1058],
        [0.1522, 0.1289, 0.1296, 0.1151, 0.1170, 0.1136, 0.1247, 0.1188],
        [0.1027, 0.0883, 0.1405, 0.1227, 0.1344, 0.1262, 0.1440, 0.1413]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1483, 0.1350, 0.1149, 0.1214, 0.1307, 0.1348, 0.1063, 0.1087],
        [0.1548, 0.1295, 0.1324, 0.1181, 0.1156, 0.1172, 0.1197, 0.1126],
        [0.0951, 0.0871, 0.1415, 0.1311, 0.1360, 0.1325, 0.1378, 0.1388],
        [0.0874, 0.0788, 0.1279, 0.1331, 0.1347, 0.1396, 0.1418, 0.1569]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1617, 0.1440, 0.1273, 0.1297, 0.1125, 0.1045, 0.1106, 0.1097],
        [0.1536, 0.1327, 0.1327, 0.1162, 0.1203, 0.1178, 0.1156, 0.1111],
        [0.1038, 0.0879, 0.1568, 0.1183, 0.1341, 0.1284, 0.1273, 0.1434],
        [0.0939, 0.0828, 0.1420, 0.1455, 0.1248, 0.1228, 0.1382, 0.1500],
        [0.0939, 0.0820, 0.1319, 0.1245, 0.1378, 0.1345, 0.1394, 0.1562]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/24 12:15:57 PM | Train: [19/50] Step 000/390 Loss 1.029 Prec@(1,5) (73.4%, 92.2%)
07/24 12:20:39 PM | Train: [19/50] Step 050/390 Loss 0.888 Prec@(1,5) (73.7%, 95.3%)
07/24 12:25:25 PM | Train: [19/50] Step 100/390 Loss 0.886 Prec@(1,5) (73.5%, 95.3%)
07/24 12:30:09 PM | Train: [19/50] Step 150/390 Loss 0.894 Prec@(1,5) (73.2%, 95.1%)
07/24 12:34:53 PM | Train: [19/50] Step 200/390 Loss 0.902 Prec@(1,5) (72.9%, 95.0%)
07/24 12:39:36 PM | Train: [19/50] Step 250/390 Loss 0.914 Prec@(1,5) (72.4%, 94.7%)
07/24 12:44:17 PM | Train: [19/50] Step 300/390 Loss 0.921 Prec@(1,5) (72.2%, 94.5%)
07/24 12:49:00 PM | Train: [19/50] Step 350/390 Loss 0.928 Prec@(1,5) (72.1%, 94.5%)
07/24 12:52:45 PM | Train: [19/50] Step 390/390 Loss 0.932 Prec@(1,5) (71.9%, 94.4%)
07/24 12:52:45 PM | Train: [19/50] Final Prec@1 71.9280%
07/24 12:52:46 PM | Valid: [19/50] Step 000/390 Loss 1.618 Prec@(1,5) (59.4%, 78.1%)
07/24 12:53:02 PM | Valid: [19/50] Step 050/390 Loss 1.651 Prec@(1,5) (56.8%, 84.6%)
07/24 12:53:19 PM | Valid: [19/50] Step 100/390 Loss 1.658 Prec@(1,5) (56.6%, 84.5%)
07/24 12:53:38 PM | Valid: [19/50] Step 150/390 Loss 1.686 Prec@(1,5) (56.1%, 84.1%)
07/24 12:53:58 PM | Valid: [19/50] Step 200/390 Loss 1.692 Prec@(1,5) (56.1%, 83.8%)
07/24 12:54:17 PM | Valid: [19/50] Step 250/390 Loss 1.695 Prec@(1,5) (56.0%, 83.7%)
07/24 12:54:33 PM | Valid: [19/50] Step 300/390 Loss 1.702 Prec@(1,5) (55.9%, 83.6%)
07/24 12:54:50 PM | Valid: [19/50] Step 350/390 Loss 1.702 Prec@(1,5) (55.9%, 83.6%)
07/24 12:55:03 PM | Valid: [19/50] Step 390/390 Loss 1.708 Prec@(1,5) (55.7%, 83.5%)
07/24 12:55:03 PM | Valid: [19/50] Final Prec@1 55.6640%
07/24 12:55:03 PM | genotype = Genotype(normal=[[('skip_connect', 0), ('dil_conv_5x5', 1)], [('skip_connect', 0), ('dil_conv_3x3', 1)], [('dil_conv_3x3', 3), ('skip_connect', 0)], [('dil_conv_5x5', 4), ('dil_conv_5x5', 3)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('skip_connect', 2), ('max_pool_3x3', 0)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1029, 0.0971, 0.1703, 0.1526, 0.1283, 0.1244, 0.1137, 0.1107],
        [0.0758, 0.0703, 0.1126, 0.1232, 0.1471, 0.1178, 0.1476, 0.2055]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1157, 0.1026, 0.1746, 0.1252, 0.1131, 0.1267, 0.1040, 0.1381],
        [0.0756, 0.0675, 0.1091, 0.1274, 0.1287, 0.1438, 0.1158, 0.2320],
        [0.0649, 0.0576, 0.1138, 0.1277, 0.1197, 0.1286, 0.1082, 0.2794]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1151, 0.0967, 0.1525, 0.1246, 0.1309, 0.1079, 0.1234, 0.1489],
        [0.0753, 0.0654, 0.1043, 0.1394, 0.1269, 0.1242, 0.1072, 0.2572],
        [0.0639, 0.0570, 0.1094, 0.1141, 0.0893, 0.1306, 0.1147, 0.3211],
        [0.0497, 0.0462, 0.0650, 0.1105, 0.1201, 0.1322, 0.1072, 0.3692]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1054, 0.0856, 0.1335, 0.1332, 0.1306, 0.1023, 0.1251, 0.1843],
        [0.0731, 0.0624, 0.0989, 0.1159, 0.1018, 0.1159, 0.1166, 0.3154],
        [0.0586, 0.0489, 0.0922, 0.1164, 0.1024, 0.1188, 0.0984, 0.3642],
        [0.0479, 0.0415, 0.0571, 0.0920, 0.0973, 0.1048, 0.1165, 0.4430],
        [0.0447, 0.0394, 0.0450, 0.1155, 0.1014, 0.1126, 0.1306, 0.4108]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1639, 0.1363, 0.1294, 0.1050, 0.1252, 0.1196, 0.1171, 0.1035],
        [0.1533, 0.1313, 0.1095, 0.1266, 0.1225, 0.1112, 0.1168, 0.1287]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1629, 0.1403, 0.1308, 0.1141, 0.1207, 0.1178, 0.1087, 0.1046],
        [0.1539, 0.1317, 0.1303, 0.1146, 0.1162, 0.1128, 0.1237, 0.1169],
        [0.1014, 0.0878, 0.1444, 0.1201, 0.1332, 0.1268, 0.1434, 0.1428]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1492, 0.1375, 0.1128, 0.1203, 0.1306, 0.1355, 0.1060, 0.1081],
        [0.1565, 0.1324, 0.1319, 0.1173, 0.1150, 0.1168, 0.1187, 0.1113],
        [0.0936, 0.0863, 0.1451, 0.1287, 0.1351, 0.1321, 0.1383, 0.1408],
        [0.0847, 0.0774, 0.1298, 0.1337, 0.1349, 0.1379, 0.1424, 0.1592]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1629, 0.1474, 0.1278, 0.1288, 0.1111, 0.1033, 0.1101, 0.1086],
        [0.1550, 0.1368, 0.1330, 0.1162, 0.1193, 0.1168, 0.1138, 0.1093],
        [0.1024, 0.0879, 0.1626, 0.1169, 0.1329, 0.1284, 0.1246, 0.1442],
        [0.0914, 0.0820, 0.1455, 0.1457, 0.1220, 0.1225, 0.1396, 0.1512],
        [0.0915, 0.0807, 0.1339, 0.1232, 0.1359, 0.1336, 0.1406, 0.1606]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/24 12:55:11 PM | Train: [20/50] Step 000/390 Loss 0.763 Prec@(1,5) (71.9%, 96.9%)
07/24 12:59:52 PM | Train: [20/50] Step 050/390 Loss 0.796 Prec@(1,5) (75.7%, 96.2%)
07/24 01:04:33 PM | Train: [20/50] Step 100/390 Loss 0.812 Prec@(1,5) (75.4%, 95.9%)
07/24 01:09:15 PM | Train: [20/50] Step 150/390 Loss 0.839 Prec@(1,5) (74.7%, 95.6%)
07/24 01:13:56 PM | Train: [20/50] Step 200/390 Loss 0.851 Prec@(1,5) (74.2%, 95.5%)
07/24 01:18:39 PM | Train: [20/50] Step 250/390 Loss 0.857 Prec@(1,5) (74.1%, 95.4%)
07/24 01:23:22 PM | Train: [20/50] Step 300/390 Loss 0.865 Prec@(1,5) (73.8%, 95.3%)
07/24 01:28:04 PM | Train: [20/50] Step 350/390 Loss 0.872 Prec@(1,5) (73.7%, 95.2%)
07/24 01:31:50 PM | Train: [20/50] Step 390/390 Loss 0.875 Prec@(1,5) (73.6%, 95.1%)
07/24 01:31:50 PM | Train: [20/50] Final Prec@1 73.6280%
07/24 01:31:51 PM | Valid: [20/50] Step 000/390 Loss 1.710 Prec@(1,5) (60.9%, 84.4%)
07/24 01:32:08 PM | Valid: [20/50] Step 050/390 Loss 1.690 Prec@(1,5) (56.2%, 83.4%)
07/24 01:32:25 PM | Valid: [20/50] Step 100/390 Loss 1.734 Prec@(1,5) (55.4%, 82.8%)
07/24 01:32:41 PM | Valid: [20/50] Step 150/390 Loss 1.745 Prec@(1,5) (54.8%, 82.8%)
07/24 01:32:58 PM | Valid: [20/50] Step 200/390 Loss 1.733 Prec@(1,5) (55.1%, 83.0%)
07/24 01:33:14 PM | Valid: [20/50] Step 250/390 Loss 1.734 Prec@(1,5) (55.3%, 83.1%)
07/24 01:33:31 PM | Valid: [20/50] Step 300/390 Loss 1.740 Prec@(1,5) (55.2%, 83.2%)
07/24 01:33:47 PM | Valid: [20/50] Step 350/390 Loss 1.743 Prec@(1,5) (55.2%, 83.1%)
07/24 01:34:00 PM | Valid: [20/50] Step 390/390 Loss 1.738 Prec@(1,5) (55.2%, 83.3%)
07/24 01:34:00 PM | Valid: [20/50] Final Prec@1 55.1840%
07/24 01:34:00 PM | genotype = Genotype(normal=[[('skip_connect', 0), ('dil_conv_5x5', 1)], [('skip_connect', 0), ('dil_conv_3x3', 1)], [('skip_connect', 0), ('dil_conv_3x3', 3)], [('dil_conv_5x5', 4), ('dil_conv_5x5', 3)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('skip_connect', 2)], [('skip_connect', 2), ('max_pool_3x3', 0)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1016, 0.0967, 0.1759, 0.1541, 0.1257, 0.1245, 0.1127, 0.1089],
        [0.0735, 0.0685, 0.1131, 0.1224, 0.1440, 0.1170, 0.1454, 0.2160]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1145, 0.1016, 0.1787, 0.1244, 0.1119, 0.1260, 0.1033, 0.1396],
        [0.0736, 0.0663, 0.1105, 0.1251, 0.1248, 0.1418, 0.1122, 0.2457],
        [0.0620, 0.0549, 0.1128, 0.1253, 0.1165, 0.1269, 0.1050, 0.2965]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1143, 0.0963, 0.1567, 0.1238, 0.1296, 0.1067, 0.1217, 0.1509],
        [0.0727, 0.0631, 0.1036, 0.1372, 0.1214, 0.1218, 0.1050, 0.2751],
        [0.0608, 0.0540, 0.1076, 0.1104, 0.0852, 0.1268, 0.1118, 0.3434],
        [0.0465, 0.0432, 0.0618, 0.1059, 0.1153, 0.1282, 0.1022, 0.3969]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1032, 0.0840, 0.1351, 0.1312, 0.1292, 0.1005, 0.1236, 0.1932],
        [0.0702, 0.0602, 0.0981, 0.1117, 0.0987, 0.1119, 0.1133, 0.3360],
        [0.0550, 0.0461, 0.0906, 0.1109, 0.0957, 0.1144, 0.0919, 0.3955],
        [0.0440, 0.0381, 0.0534, 0.0855, 0.0888, 0.0989, 0.1106, 0.4806],
        [0.0410, 0.0362, 0.0415, 0.1089, 0.0958, 0.1065, 0.1241, 0.4460]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1671, 0.1385, 0.1292, 0.1041, 0.1239, 0.1189, 0.1165, 0.1017],
        [0.1550, 0.1329, 0.1076, 0.1262, 0.1209, 0.1104, 0.1175, 0.1294]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1659, 0.1430, 0.1320, 0.1121, 0.1199, 0.1169, 0.1079, 0.1023],
        [0.1557, 0.1334, 0.1294, 0.1136, 0.1158, 0.1115, 0.1238, 0.1169],
        [0.0994, 0.0868, 0.1473, 0.1199, 0.1310, 0.1267, 0.1444, 0.1446]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1510, 0.1398, 0.1112, 0.1194, 0.1307, 0.1374, 0.1049, 0.1056],
        [0.1571, 0.1338, 0.1322, 0.1173, 0.1136, 0.1171, 0.1188, 0.1101],
        [0.0919, 0.0855, 0.1479, 0.1268, 0.1347, 0.1327, 0.1379, 0.1426],
        [0.0831, 0.0767, 0.1321, 0.1318, 0.1343, 0.1385, 0.1416, 0.1619]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1644, 0.1501, 0.1287, 0.1275, 0.1091, 0.1025, 0.1108, 0.1070],
        [0.1561, 0.1389, 0.1333, 0.1147, 0.1185, 0.1154, 0.1141, 0.1090],
        [0.0996, 0.0864, 0.1669, 0.1159, 0.1315, 0.1290, 0.1236, 0.1471],
        [0.0889, 0.0806, 0.1479, 0.1476, 0.1205, 0.1228, 0.1387, 0.1530],
        [0.0895, 0.0794, 0.1357, 0.1224, 0.1349, 0.1342, 0.1408, 0.1630]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/24 01:34:08 PM | Train: [21/50] Step 000/390 Loss 0.838 Prec@(1,5) (75.0%, 95.3%)
07/24 01:38:51 PM | Train: [21/50] Step 050/390 Loss 0.735 Prec@(1,5) (77.5%, 97.0%)
07/24 01:43:33 PM | Train: [21/50] Step 100/390 Loss 0.752 Prec@(1,5) (76.7%, 96.7%)
07/24 01:48:07 PM | Train: [21/50] Step 150/390 Loss 0.757 Prec@(1,5) (76.6%, 96.6%)
07/24 01:52:52 PM | Train: [21/50] Step 200/390 Loss 0.779 Prec@(1,5) (75.9%, 96.4%)
07/24 01:57:36 PM | Train: [21/50] Step 250/390 Loss 0.793 Prec@(1,5) (75.5%, 96.2%)
07/24 02:02:21 PM | Train: [21/50] Step 300/390 Loss 0.801 Prec@(1,5) (75.3%, 96.1%)
07/24 02:07:02 PM | Train: [21/50] Step 350/390 Loss 0.812 Prec@(1,5) (75.1%, 96.0%)
07/24 02:10:52 PM | Train: [21/50] Step 390/390 Loss 0.823 Prec@(1,5) (74.7%, 95.8%)
07/24 02:10:52 PM | Train: [21/50] Final Prec@1 74.7000%
07/24 02:10:53 PM | Valid: [21/50] Step 000/390 Loss 1.466 Prec@(1,5) (59.4%, 92.2%)
07/24 02:11:09 PM | Valid: [21/50] Step 050/390 Loss 1.722 Prec@(1,5) (55.7%, 83.5%)
07/24 02:11:25 PM | Valid: [21/50] Step 100/390 Loss 1.728 Prec@(1,5) (55.7%, 83.4%)
07/24 02:11:41 PM | Valid: [21/50] Step 150/390 Loss 1.735 Prec@(1,5) (55.4%, 83.7%)
07/24 02:11:58 PM | Valid: [21/50] Step 200/390 Loss 1.734 Prec@(1,5) (55.2%, 83.8%)
07/24 02:12:15 PM | Valid: [21/50] Step 250/390 Loss 1.736 Prec@(1,5) (55.1%, 83.7%)
07/24 02:12:31 PM | Valid: [21/50] Step 300/390 Loss 1.730 Prec@(1,5) (55.4%, 83.8%)
07/24 02:12:47 PM | Valid: [21/50] Step 350/390 Loss 1.729 Prec@(1,5) (55.4%, 83.8%)
07/24 02:13:00 PM | Valid: [21/50] Step 390/390 Loss 1.727 Prec@(1,5) (55.4%, 83.8%)
07/24 02:13:00 PM | Valid: [21/50] Final Prec@1 55.4440%
07/24 02:13:00 PM | genotype = Genotype(normal=[[('skip_connect', 0), ('dil_conv_5x5', 1)], [('skip_connect', 0), ('dil_conv_3x3', 1)], [('skip_connect', 0), ('dil_conv_3x3', 3)], [('dil_conv_5x5', 4), ('skip_connect', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('skip_connect', 2)], [('skip_connect', 2), ('max_pool_3x3', 0)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1010, 0.0970, 0.1830, 0.1558, 0.1232, 0.1225, 0.1106, 0.1070],
        [0.0710, 0.0666, 0.1129, 0.1210, 0.1411, 0.1159, 0.1440, 0.2275]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1135, 0.1016, 0.1849, 0.1243, 0.1090, 0.1242, 0.1019, 0.1406],
        [0.0709, 0.0645, 0.1103, 0.1229, 0.1213, 0.1401, 0.1099, 0.2601],
        [0.0587, 0.0522, 0.1116, 0.1234, 0.1124, 0.1245, 0.1014, 0.3159]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1136, 0.0961, 0.1607, 0.1217, 0.1279, 0.1058, 0.1204, 0.1538],
        [0.0693, 0.0608, 0.1023, 0.1351, 0.1157, 0.1189, 0.1016, 0.2963],
        [0.0571, 0.0511, 0.1055, 0.1066, 0.0811, 0.1216, 0.1072, 0.3698],
        [0.0429, 0.0402, 0.0584, 0.1008, 0.1089, 0.1223, 0.0958, 0.4306]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1027, 0.0832, 0.1377, 0.1291, 0.1275, 0.0975, 0.1215, 0.2009],
        [0.0670, 0.0576, 0.0963, 0.1077, 0.0939, 0.1071, 0.1087, 0.3616],
        [0.0516, 0.0431, 0.0880, 0.1056, 0.0908, 0.1101, 0.0854, 0.4253],
        [0.0405, 0.0352, 0.0499, 0.0790, 0.0817, 0.0921, 0.1025, 0.5190],
        [0.0378, 0.0335, 0.0384, 0.1026, 0.0885, 0.0997, 0.1162, 0.4834]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1705, 0.1414, 0.1292, 0.1028, 0.1227, 0.1167, 0.1157, 0.1010],
        [0.1576, 0.1359, 0.1056, 0.1257, 0.1203, 0.1095, 0.1169, 0.1285]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1680, 0.1447, 0.1315, 0.1108, 0.1190, 0.1183, 0.1068, 0.1008],
        [0.1578, 0.1360, 0.1293, 0.1136, 0.1152, 0.1110, 0.1222, 0.1151],
        [0.0972, 0.0859, 0.1502, 0.1188, 0.1299, 0.1269, 0.1447, 0.1464]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1533, 0.1425, 0.1094, 0.1186, 0.1307, 0.1355, 0.1052, 0.1050],
        [0.1601, 0.1366, 0.1326, 0.1160, 0.1123, 0.1163, 0.1184, 0.1078],
        [0.0903, 0.0847, 0.1504, 0.1256, 0.1345, 0.1337, 0.1367, 0.1440],
        [0.0820, 0.0763, 0.1350, 0.1299, 0.1331, 0.1373, 0.1421, 0.1643]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1664, 0.1530, 0.1293, 0.1274, 0.1060, 0.1007, 0.1110, 0.1061],
        [0.1580, 0.1419, 0.1326, 0.1141, 0.1173, 0.1154, 0.1133, 0.1075],
        [0.0972, 0.0857, 0.1716, 0.1143, 0.1307, 0.1285, 0.1221, 0.1500],
        [0.0870, 0.0798, 0.1508, 0.1480, 0.1189, 0.1235, 0.1379, 0.1541],
        [0.0881, 0.0788, 0.1381, 0.1215, 0.1323, 0.1329, 0.1407, 0.1677]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/24 02:13:08 PM | Train: [22/50] Step 000/390 Loss 0.722 Prec@(1,5) (71.9%, 96.9%)
07/24 02:17:55 PM | Train: [22/50] Step 050/390 Loss 0.728 Prec@(1,5) (77.5%, 96.5%)
07/24 02:22:46 PM | Train: [22/50] Step 100/390 Loss 0.723 Prec@(1,5) (77.8%, 96.5%)
07/24 02:27:29 PM | Train: [22/50] Step 150/390 Loss 0.718 Prec@(1,5) (78.0%, 96.8%)
07/24 02:32:13 PM | Train: [22/50] Step 200/390 Loss 0.736 Prec@(1,5) (77.6%, 96.6%)
07/24 02:37:00 PM | Train: [22/50] Step 250/390 Loss 0.752 Prec@(1,5) (77.1%, 96.3%)
07/24 02:41:47 PM | Train: [22/50] Step 300/390 Loss 0.764 Prec@(1,5) (76.6%, 96.3%)
07/24 02:46:25 PM | Train: [22/50] Step 350/390 Loss 0.769 Prec@(1,5) (76.5%, 96.2%)
07/24 02:50:11 PM | Train: [22/50] Step 390/390 Loss 0.776 Prec@(1,5) (76.3%, 96.1%)
07/24 02:50:11 PM | Train: [22/50] Final Prec@1 76.3360%
07/24 02:50:12 PM | Valid: [22/50] Step 000/390 Loss 1.830 Prec@(1,5) (56.2%, 87.5%)
07/24 02:50:28 PM | Valid: [22/50] Step 050/390 Loss 1.761 Prec@(1,5) (54.9%, 82.6%)
07/24 02:50:46 PM | Valid: [22/50] Step 100/390 Loss 1.738 Prec@(1,5) (55.1%, 83.3%)
07/24 02:51:02 PM | Valid: [22/50] Step 150/390 Loss 1.726 Prec@(1,5) (55.5%, 83.7%)
07/24 02:51:19 PM | Valid: [22/50] Step 200/390 Loss 1.705 Prec@(1,5) (55.9%, 84.0%)
07/24 02:51:38 PM | Valid: [22/50] Step 250/390 Loss 1.711 Prec@(1,5) (55.9%, 84.0%)
07/24 02:51:56 PM | Valid: [22/50] Step 300/390 Loss 1.703 Prec@(1,5) (56.2%, 84.0%)
07/24 02:52:12 PM | Valid: [22/50] Step 350/390 Loss 1.692 Prec@(1,5) (56.4%, 84.2%)
07/24 02:52:25 PM | Valid: [22/50] Step 390/390 Loss 1.698 Prec@(1,5) (56.4%, 84.1%)
07/24 02:52:25 PM | Valid: [22/50] Final Prec@1 56.3960%
07/24 02:52:25 PM | genotype = Genotype(normal=[[('skip_connect', 0), ('dil_conv_5x5', 1)], [('skip_connect', 0), ('dil_conv_3x3', 1)], [('skip_connect', 0), ('sep_conv_3x3', 1)], [('skip_connect', 0), ('dil_conv_5x5', 4)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('skip_connect', 2)], [('skip_connect', 2), ('max_pool_3x3', 0)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.0991, 0.0966, 0.1894, 0.1557, 0.1214, 0.1221, 0.1094, 0.1063],
        [0.0690, 0.0648, 0.1130, 0.1196, 0.1381, 0.1155, 0.1426, 0.2374]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1110, 0.1005, 0.1895, 0.1241, 0.1077, 0.1243, 0.1003, 0.1427],
        [0.0687, 0.0628, 0.1103, 0.1191, 0.1179, 0.1383, 0.1073, 0.2756],
        [0.0552, 0.0494, 0.1098, 0.1204, 0.1081, 0.1232, 0.0984, 0.3355]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1118, 0.0957, 0.1645, 0.1195, 0.1259, 0.1047, 0.1198, 0.1582],
        [0.0667, 0.0588, 0.1015, 0.1312, 0.1110, 0.1158, 0.0980, 0.3170],
        [0.0535, 0.0484, 0.1037, 0.1017, 0.0770, 0.1164, 0.1031, 0.3962],
        [0.0396, 0.0374, 0.0553, 0.0959, 0.1023, 0.1161, 0.0904, 0.4629]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1008, 0.0822, 0.1402, 0.1257, 0.1250, 0.0962, 0.1196, 0.2102],
        [0.0643, 0.0555, 0.0951, 0.1036, 0.0900, 0.1025, 0.1029, 0.3860],
        [0.0483, 0.0406, 0.0862, 0.0997, 0.0847, 0.1030, 0.0797, 0.4578],
        [0.0370, 0.0324, 0.0464, 0.0729, 0.0746, 0.0850, 0.0954, 0.5564],
        [0.0345, 0.0307, 0.0353, 0.0950, 0.0823, 0.0932, 0.1077, 0.5214]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1729, 0.1441, 0.1295, 0.1011, 0.1217, 0.1168, 0.1142, 0.0997],
        [0.1596, 0.1387, 0.1032, 0.1243, 0.1190, 0.1095, 0.1170, 0.1287]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1711, 0.1484, 0.1298, 0.1100, 0.1177, 0.1180, 0.1056, 0.0993],
        [0.1602, 0.1387, 0.1290, 0.1117, 0.1140, 0.1112, 0.1217, 0.1135],
        [0.0953, 0.0856, 0.1542, 0.1166, 0.1298, 0.1264, 0.1445, 0.1476]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1551, 0.1452, 0.1075, 0.1184, 0.1298, 0.1365, 0.1046, 0.1029],
        [0.1641, 0.1409, 0.1324, 0.1143, 0.1113, 0.1145, 0.1162, 0.1063],
        [0.0891, 0.0846, 0.1545, 0.1236, 0.1335, 0.1335, 0.1360, 0.1452],
        [0.0807, 0.0760, 0.1390, 0.1289, 0.1320, 0.1365, 0.1413, 0.1658]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1688, 0.1566, 0.1299, 0.1256, 0.1047, 0.0998, 0.1090, 0.1055],
        [0.1605, 0.1452, 0.1332, 0.1122, 0.1157, 0.1149, 0.1122, 0.1061],
        [0.0950, 0.0852, 0.1761, 0.1134, 0.1290, 0.1283, 0.1210, 0.1520],
        [0.0854, 0.0791, 0.1548, 0.1464, 0.1155, 0.1236, 0.1383, 0.1568],
        [0.0859, 0.0774, 0.1400, 0.1212, 0.1296, 0.1320, 0.1417, 0.1721]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/24 02:52:33 PM | Train: [23/50] Step 000/390 Loss 0.811 Prec@(1,5) (68.8%, 96.9%)
07/24 02:57:16 PM | Train: [23/50] Step 050/390 Loss 0.656 Prec@(1,5) (79.8%, 97.6%)
07/24 03:02:05 PM | Train: [23/50] Step 100/390 Loss 0.650 Prec@(1,5) (80.5%, 97.5%)
07/24 03:06:46 PM | Train: [23/50] Step 150/390 Loss 0.654 Prec@(1,5) (80.1%, 97.5%)
07/24 03:11:28 PM | Train: [23/50] Step 200/390 Loss 0.664 Prec@(1,5) (79.9%, 97.3%)
07/24 03:16:08 PM | Train: [23/50] Step 250/390 Loss 0.681 Prec@(1,5) (79.4%, 97.1%)
07/24 03:20:52 PM | Train: [23/50] Step 300/390 Loss 0.692 Prec@(1,5) (79.1%, 96.9%)
07/24 03:25:39 PM | Train: [23/50] Step 350/390 Loss 0.703 Prec@(1,5) (78.7%, 96.8%)
07/24 03:29:29 PM | Train: [23/50] Step 390/390 Loss 0.710 Prec@(1,5) (78.5%, 96.8%)
07/24 03:29:29 PM | Train: [23/50] Final Prec@1 78.4560%
07/24 03:29:30 PM | Valid: [23/50] Step 000/390 Loss 2.057 Prec@(1,5) (53.1%, 79.7%)
07/24 03:29:47 PM | Valid: [23/50] Step 050/390 Loss 1.755 Prec@(1,5) (55.3%, 83.7%)
07/24 03:30:03 PM | Valid: [23/50] Step 100/390 Loss 1.724 Prec@(1,5) (55.9%, 84.1%)
07/24 03:30:20 PM | Valid: [23/50] Step 150/390 Loss 1.717 Prec@(1,5) (56.0%, 84.1%)
07/24 03:30:36 PM | Valid: [23/50] Step 200/390 Loss 1.722 Prec@(1,5) (56.0%, 84.1%)
07/24 03:30:53 PM | Valid: [23/50] Step 250/390 Loss 1.719 Prec@(1,5) (56.2%, 84.2%)
07/24 03:31:09 PM | Valid: [23/50] Step 300/390 Loss 1.705 Prec@(1,5) (56.4%, 84.3%)
07/24 03:31:26 PM | Valid: [23/50] Step 350/390 Loss 1.701 Prec@(1,5) (56.4%, 84.3%)
07/24 03:31:39 PM | Valid: [23/50] Step 390/390 Loss 1.702 Prec@(1,5) (56.3%, 84.3%)
07/24 03:31:39 PM | Valid: [23/50] Final Prec@1 56.3320%
07/24 03:31:39 PM | genotype = Genotype(normal=[[('skip_connect', 0), ('dil_conv_5x5', 1)], [('skip_connect', 0), ('dil_conv_3x3', 1)], [('skip_connect', 0), ('sep_conv_3x3', 1)], [('skip_connect', 0), ('dil_conv_5x5', 4)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('skip_connect', 2)], [('max_pool_3x3', 1), ('skip_connect', 2)], [('skip_connect', 2), ('max_pool_3x3', 0)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.0979, 0.0962, 0.1957, 0.1568, 0.1198, 0.1213, 0.1074, 0.1049],
        [0.0672, 0.0629, 0.1125, 0.1178, 0.1349, 0.1139, 0.1399, 0.2509]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1104, 0.0999, 0.1951, 0.1234, 0.1050, 0.1237, 0.0978, 0.1447],
        [0.0671, 0.0610, 0.1101, 0.1167, 0.1125, 0.1359, 0.1041, 0.2926],
        [0.0524, 0.0468, 0.1078, 0.1166, 0.1040, 0.1196, 0.0945, 0.3581]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1109, 0.0953, 0.1687, 0.1179, 0.1243, 0.1033, 0.1183, 0.1613],
        [0.0649, 0.0571, 0.1014, 0.1273, 0.1068, 0.1116, 0.0951, 0.3358],
        [0.0503, 0.0456, 0.1006, 0.0965, 0.0729, 0.1117, 0.0982, 0.4241],
        [0.0367, 0.0349, 0.0523, 0.0901, 0.0953, 0.1086, 0.0843, 0.4979]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.0999, 0.0814, 0.1426, 0.1232, 0.1219, 0.0939, 0.1175, 0.2197],
        [0.0615, 0.0530, 0.0930, 0.0995, 0.0852, 0.0983, 0.0983, 0.4113],
        [0.0450, 0.0379, 0.0825, 0.0940, 0.0803, 0.0964, 0.0745, 0.4894],
        [0.0337, 0.0296, 0.0428, 0.0667, 0.0683, 0.0786, 0.0864, 0.5940],
        [0.0315, 0.0281, 0.0323, 0.0881, 0.0769, 0.0865, 0.0989, 0.5577]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1752, 0.1460, 0.1300, 0.0991, 0.1207, 0.1173, 0.1131, 0.0987],
        [0.1606, 0.1394, 0.1022, 0.1238, 0.1203, 0.1085, 0.1166, 0.1287]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1724, 0.1509, 0.1286, 0.1088, 0.1168, 0.1192, 0.1058, 0.0974],
        [0.1614, 0.1408, 0.1286, 0.1109, 0.1124, 0.1110, 0.1224, 0.1125],
        [0.0933, 0.0852, 0.1574, 0.1154, 0.1288, 0.1256, 0.1457, 0.1485]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1571, 0.1485, 0.1055, 0.1180, 0.1295, 0.1366, 0.1037, 0.1012],
        [0.1656, 0.1426, 0.1315, 0.1147, 0.1110, 0.1131, 0.1163, 0.1053],
        [0.0873, 0.0836, 0.1574, 0.1228, 0.1329, 0.1331, 0.1363, 0.1466],
        [0.0792, 0.0755, 0.1425, 0.1259, 0.1318, 0.1363, 0.1415, 0.1673]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1695, 0.1592, 0.1302, 0.1254, 0.1041, 0.0982, 0.1084, 0.1049],
        [0.1622, 0.1478, 0.1330, 0.1107, 0.1147, 0.1144, 0.1118, 0.1054],
        [0.0930, 0.0846, 0.1800, 0.1140, 0.1276, 0.1282, 0.1200, 0.1526],
        [0.0831, 0.0778, 0.1575, 0.1462, 0.1146, 0.1230, 0.1388, 0.1588],
        [0.0835, 0.0760, 0.1420, 0.1214, 0.1271, 0.1315, 0.1414, 0.1771]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/24 03:31:46 PM | Train: [24/50] Step 000/390 Loss 0.597 Prec@(1,5) (84.4%, 95.3%)
07/24 03:36:34 PM | Train: [24/50] Step 050/390 Loss 0.639 Prec@(1,5) (81.8%, 97.0%)
07/24 03:41:17 PM | Train: [24/50] Step 100/390 Loss 0.642 Prec@(1,5) (81.3%, 97.2%)
07/24 03:45:55 PM | Train: [24/50] Step 150/390 Loss 0.647 Prec@(1,5) (80.9%, 97.2%)
07/24 03:50:35 PM | Train: [24/50] Step 200/390 Loss 0.649 Prec@(1,5) (80.7%, 97.2%)
07/24 03:55:17 PM | Train: [24/50] Step 250/390 Loss 0.656 Prec@(1,5) (80.4%, 97.1%)
07/24 03:59:59 PM | Train: [24/50] Step 300/390 Loss 0.663 Prec@(1,5) (80.0%, 97.1%)
07/24 04:04:42 PM | Train: [24/50] Step 350/390 Loss 0.671 Prec@(1,5) (79.7%, 97.1%)
07/24 04:08:32 PM | Train: [24/50] Step 390/390 Loss 0.673 Prec@(1,5) (79.7%, 97.1%)
07/24 04:08:32 PM | Train: [24/50] Final Prec@1 79.6680%
07/24 04:08:33 PM | Valid: [24/50] Step 000/390 Loss 1.474 Prec@(1,5) (53.1%, 84.4%)
07/24 04:08:49 PM | Valid: [24/50] Step 050/390 Loss 1.655 Prec@(1,5) (56.6%, 85.6%)
07/24 04:09:05 PM | Valid: [24/50] Step 100/390 Loss 1.678 Prec@(1,5) (56.4%, 85.1%)
07/24 04:09:21 PM | Valid: [24/50] Step 150/390 Loss 1.658 Prec@(1,5) (57.1%, 85.0%)
07/24 04:09:37 PM | Valid: [24/50] Step 200/390 Loss 1.668 Prec@(1,5) (56.8%, 84.8%)
07/24 04:09:53 PM | Valid: [24/50] Step 250/390 Loss 1.679 Prec@(1,5) (56.9%, 84.8%)
07/24 04:10:10 PM | Valid: [24/50] Step 300/390 Loss 1.678 Prec@(1,5) (57.0%, 84.7%)
07/24 04:10:28 PM | Valid: [24/50] Step 350/390 Loss 1.685 Prec@(1,5) (57.0%, 84.6%)
07/24 04:10:43 PM | Valid: [24/50] Step 390/390 Loss 1.685 Prec@(1,5) (57.0%, 84.7%)
07/24 04:10:43 PM | Valid: [24/50] Final Prec@1 56.9760%
07/24 04:10:43 PM | genotype = Genotype(normal=[[('skip_connect', 0), ('dil_conv_5x5', 1)], [('skip_connect', 0), ('dil_conv_3x3', 1)], [('skip_connect', 0), ('sep_conv_3x3', 1)], [('skip_connect', 0), ('dil_conv_5x5', 4)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('skip_connect', 2)], [('skip_connect', 2), ('max_pool_3x3', 1)], [('skip_connect', 2), ('max_pool_3x3', 0)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.0966, 0.0961, 0.2022, 0.1575, 0.1181, 0.1193, 0.1065, 0.1037],
        [0.0648, 0.0610, 0.1117, 0.1163, 0.1329, 0.1120, 0.1379, 0.2634]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1089, 0.0998, 0.2010, 0.1218, 0.1034, 0.1222, 0.0963, 0.1468],
        [0.0649, 0.0592, 0.1092, 0.1134, 0.1086, 0.1342, 0.1015, 0.3090],
        [0.0498, 0.0445, 0.1059, 0.1131, 0.0994, 0.1161, 0.0899, 0.3813]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1100, 0.0952, 0.1731, 0.1153, 0.1209, 0.1024, 0.1172, 0.1659],
        [0.0625, 0.0551, 0.0999, 0.1229, 0.1016, 0.1085, 0.0911, 0.3585],
        [0.0475, 0.0432, 0.0979, 0.0915, 0.0683, 0.1059, 0.0930, 0.4528],
        [0.0338, 0.0323, 0.0490, 0.0839, 0.0880, 0.1009, 0.0785, 0.5338]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.0983, 0.0803, 0.1453, 0.1200, 0.1188, 0.0909, 0.1152, 0.2311],
        [0.0589, 0.0511, 0.0919, 0.0941, 0.0804, 0.0930, 0.0926, 0.4381],
        [0.0420, 0.0355, 0.0796, 0.0877, 0.0745, 0.0890, 0.0689, 0.5228],
        [0.0306, 0.0270, 0.0393, 0.0611, 0.0621, 0.0715, 0.0780, 0.6305],
        [0.0286, 0.0257, 0.0296, 0.0812, 0.0707, 0.0803, 0.0909, 0.5930]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1781, 0.1494, 0.1289, 0.0975, 0.1204, 0.1171, 0.1111, 0.0975],
        [0.1622, 0.1424, 0.0999, 0.1221, 0.1197, 0.1083, 0.1166, 0.1288]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1735, 0.1531, 0.1282, 0.1075, 0.1172, 0.1198, 0.1049, 0.0957],
        [0.1632, 0.1439, 0.1277, 0.1101, 0.1120, 0.1105, 0.1213, 0.1114],
        [0.0918, 0.0846, 0.1598, 0.1135, 0.1283, 0.1257, 0.1470, 0.1493]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1583, 0.1519, 0.1029, 0.1172, 0.1292, 0.1375, 0.1026, 0.1004],
        [0.1680, 0.1460, 0.1304, 0.1132, 0.1102, 0.1125, 0.1156, 0.1041],
        [0.0862, 0.0833, 0.1608, 0.1214, 0.1315, 0.1329, 0.1362, 0.1477],
        [0.0779, 0.0752, 0.1456, 0.1238, 0.1320, 0.1353, 0.1410, 0.1691]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1722, 0.1630, 0.1292, 0.1241, 0.1030, 0.0969, 0.1074, 0.1043],
        [0.1642, 0.1515, 0.1322, 0.1090, 0.1130, 0.1148, 0.1112, 0.1040],
        [0.0918, 0.0844, 0.1859, 0.1132, 0.1246, 0.1272, 0.1183, 0.1546],
        [0.0816, 0.0770, 0.1610, 0.1451, 0.1128, 0.1242, 0.1385, 0.1598],
        [0.0819, 0.0752, 0.1447, 0.1201, 0.1265, 0.1296, 0.1411, 0.1810]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/24 04:10:51 PM | Train: [25/50] Step 000/390 Loss 0.555 Prec@(1,5) (84.4%, 96.9%)
07/24 04:15:34 PM | Train: [25/50] Step 050/390 Loss 0.585 Prec@(1,5) (81.9%, 97.9%)
07/24 04:20:14 PM | Train: [25/50] Step 100/390 Loss 0.595 Prec@(1,5) (81.8%, 97.8%)
07/24 04:24:59 PM | Train: [25/50] Step 150/390 Loss 0.593 Prec@(1,5) (81.9%, 97.8%)
07/24 04:29:40 PM | Train: [25/50] Step 200/390 Loss 0.605 Prec@(1,5) (81.4%, 97.8%)
07/24 04:34:22 PM | Train: [25/50] Step 250/390 Loss 0.612 Prec@(1,5) (81.2%, 97.7%)
07/24 04:39:06 PM | Train: [25/50] Step 300/390 Loss 0.624 Prec@(1,5) (80.7%, 97.6%)
07/24 04:43:51 PM | Train: [25/50] Step 350/390 Loss 0.625 Prec@(1,5) (80.7%, 97.6%)
07/24 04:47:37 PM | Train: [25/50] Step 390/390 Loss 0.631 Prec@(1,5) (80.6%, 97.5%)
07/24 04:47:38 PM | Train: [25/50] Final Prec@1 80.6120%
07/24 04:47:38 PM | Valid: [25/50] Step 000/390 Loss 1.208 Prec@(1,5) (59.4%, 93.8%)
07/24 04:47:55 PM | Valid: [25/50] Step 050/390 Loss 1.668 Prec@(1,5) (56.2%, 85.4%)
07/24 04:48:11 PM | Valid: [25/50] Step 100/390 Loss 1.639 Prec@(1,5) (57.5%, 85.7%)
07/24 04:48:27 PM | Valid: [25/50] Step 150/390 Loss 1.633 Prec@(1,5) (57.7%, 85.6%)
07/24 04:48:43 PM | Valid: [25/50] Step 200/390 Loss 1.638 Prec@(1,5) (57.7%, 85.4%)
07/24 04:48:59 PM | Valid: [25/50] Step 250/390 Loss 1.642 Prec@(1,5) (57.5%, 85.2%)
07/24 04:49:16 PM | Valid: [25/50] Step 300/390 Loss 1.641 Prec@(1,5) (57.6%, 85.2%)
07/24 04:49:35 PM | Valid: [25/50] Step 350/390 Loss 1.650 Prec@(1,5) (57.5%, 85.0%)
07/24 04:49:50 PM | Valid: [25/50] Step 390/390 Loss 1.654 Prec@(1,5) (57.4%, 85.0%)
07/24 04:49:50 PM | Valid: [25/50] Final Prec@1 57.4360%
07/24 04:49:50 PM | genotype = Genotype(normal=[[('skip_connect', 0), ('dil_conv_5x5', 1)], [('skip_connect', 0), ('dil_conv_3x3', 1)], [('skip_connect', 0), ('sep_conv_3x3', 1)], [('skip_connect', 0), ('dil_conv_5x5', 4)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('skip_connect', 2)], [('skip_connect', 2), ('max_pool_3x3', 1)], [('skip_connect', 2), ('max_pool_3x3', 0)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.0946, 0.0949, 0.2075, 0.1598, 0.1161, 0.1187, 0.1058, 0.1027],
        [0.0633, 0.0594, 0.1116, 0.1141, 0.1299, 0.1087, 0.1369, 0.2762]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1065, 0.0980, 0.2052, 0.1218, 0.1027, 0.1214, 0.0948, 0.1496],
        [0.0631, 0.0576, 0.1092, 0.1108, 0.1040, 0.1309, 0.0988, 0.3256],
        [0.0471, 0.0421, 0.1038, 0.1086, 0.0954, 0.1120, 0.0863, 0.4048]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1078, 0.0940, 0.1770, 0.1135, 0.1186, 0.1006, 0.1166, 0.1720],
        [0.0603, 0.0531, 0.0985, 0.1188, 0.0951, 0.1046, 0.0881, 0.3816],
        [0.0444, 0.0405, 0.0941, 0.0879, 0.0645, 0.1012, 0.0889, 0.4785],
        [0.0311, 0.0298, 0.0457, 0.0784, 0.0821, 0.0941, 0.0731, 0.5657]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.0965, 0.0785, 0.1464, 0.1177, 0.1163, 0.0888, 0.1132, 0.2426],
        [0.0566, 0.0490, 0.0903, 0.0887, 0.0753, 0.0873, 0.0874, 0.4654],
        [0.0394, 0.0331, 0.0759, 0.0827, 0.0702, 0.0836, 0.0645, 0.5505],
        [0.0280, 0.0246, 0.0361, 0.0554, 0.0567, 0.0654, 0.0718, 0.6620],
        [0.0262, 0.0234, 0.0271, 0.0747, 0.0650, 0.0741, 0.0841, 0.6254]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1813, 0.1529, 0.1292, 0.0968, 0.1189, 0.1158, 0.1090, 0.0961],
        [0.1632, 0.1444, 0.0984, 0.1213, 0.1190, 0.1075, 0.1168, 0.1294]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1760, 0.1559, 0.1282, 0.1062, 0.1163, 0.1201, 0.1033, 0.0939],
        [0.1647, 0.1457, 0.1280, 0.1087, 0.1115, 0.1092, 0.1216, 0.1106],
        [0.0903, 0.0837, 0.1620, 0.1112, 0.1281, 0.1257, 0.1485, 0.1505]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1616, 0.1559, 0.1007, 0.1168, 0.1274, 0.1370, 0.1018, 0.0987],
        [0.1706, 0.1485, 0.1301, 0.1122, 0.1103, 0.1116, 0.1140, 0.1026],
        [0.0851, 0.0830, 0.1648, 0.1198, 0.1292, 0.1330, 0.1359, 0.1493],
        [0.0767, 0.0751, 0.1498, 0.1218, 0.1306, 0.1333, 0.1416, 0.1710]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1760, 0.1675, 0.1285, 0.1232, 0.1006, 0.0957, 0.1052, 0.1032],
        [0.1662, 0.1541, 0.1321, 0.1083, 0.1112, 0.1140, 0.1109, 0.1031],
        [0.0898, 0.0837, 0.1913, 0.1128, 0.1228, 0.1262, 0.1171, 0.1564],
        [0.0798, 0.0761, 0.1651, 0.1448, 0.1097, 0.1247, 0.1388, 0.1609],
        [0.0798, 0.0743, 0.1471, 0.1186, 0.1248, 0.1291, 0.1406, 0.1858]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/24 04:49:58 PM | Train: [26/50] Step 000/390 Loss 0.492 Prec@(1,5) (87.5%, 100.0%)
07/24 04:54:42 PM | Train: [26/50] Step 050/390 Loss 0.538 Prec@(1,5) (83.8%, 98.2%)
07/24 04:59:27 PM | Train: [26/50] Step 100/390 Loss 0.556 Prec@(1,5) (83.4%, 98.1%)
07/24 05:04:11 PM | Train: [26/50] Step 150/390 Loss 0.565 Prec@(1,5) (83.3%, 98.1%)
07/24 05:08:54 PM | Train: [26/50] Step 200/390 Loss 0.573 Prec@(1,5) (82.9%, 97.9%)
07/24 05:13:38 PM | Train: [26/50] Step 250/390 Loss 0.578 Prec@(1,5) (82.8%, 97.9%)
07/24 05:18:23 PM | Train: [26/50] Step 300/390 Loss 0.577 Prec@(1,5) (82.7%, 98.0%)
07/24 05:23:06 PM | Train: [26/50] Step 350/390 Loss 0.581 Prec@(1,5) (82.5%, 97.9%)
07/24 05:26:53 PM | Train: [26/50] Step 390/390 Loss 0.586 Prec@(1,5) (82.3%, 97.9%)
07/24 05:26:53 PM | Train: [26/50] Final Prec@1 82.3280%
07/24 05:26:53 PM | Valid: [26/50] Step 000/390 Loss 1.415 Prec@(1,5) (56.2%, 89.1%)
07/24 05:27:10 PM | Valid: [26/50] Step 050/390 Loss 1.721 Prec@(1,5) (57.0%, 84.3%)
07/24 05:27:27 PM | Valid: [26/50] Step 100/390 Loss 1.703 Prec@(1,5) (57.0%, 84.7%)
07/24 05:27:43 PM | Valid: [26/50] Step 150/390 Loss 1.689 Prec@(1,5) (57.4%, 84.8%)
07/24 05:27:59 PM | Valid: [26/50] Step 200/390 Loss 1.681 Prec@(1,5) (57.4%, 85.0%)
07/24 05:28:15 PM | Valid: [26/50] Step 250/390 Loss 1.672 Prec@(1,5) (57.5%, 85.0%)
07/24 05:28:32 PM | Valid: [26/50] Step 300/390 Loss 1.669 Prec@(1,5) (57.7%, 85.0%)
07/24 05:28:49 PM | Valid: [26/50] Step 350/390 Loss 1.669 Prec@(1,5) (57.5%, 85.1%)
07/24 05:29:05 PM | Valid: [26/50] Step 390/390 Loss 1.668 Prec@(1,5) (57.5%, 85.1%)
07/24 05:29:05 PM | Valid: [26/50] Final Prec@1 57.5480%
07/24 05:29:05 PM | genotype = Genotype(normal=[[('skip_connect', 0), ('dil_conv_5x5', 1)], [('skip_connect', 0), ('dil_conv_3x3', 1)], [('skip_connect', 0), ('sep_conv_3x3', 1)], [('skip_connect', 0), ('dil_conv_5x5', 4)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('skip_connect', 2)], [('skip_connect', 2), ('max_pool_3x3', 1)], [('skip_connect', 2), ('max_pool_3x3', 0)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.0935, 0.0947, 0.2140, 0.1603, 0.1136, 0.1174, 0.1048, 0.1018],
        [0.0620, 0.0580, 0.1117, 0.1112, 0.1254, 0.1061, 0.1345, 0.2911]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1046, 0.0970, 0.2088, 0.1209, 0.1019, 0.1201, 0.0936, 0.1532],
        [0.0609, 0.0556, 0.1081, 0.1081, 0.0995, 0.1284, 0.0965, 0.3429],
        [0.0445, 0.0399, 0.1013, 0.1045, 0.0907, 0.1081, 0.0829, 0.4281]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1072, 0.0936, 0.1819, 0.1117, 0.1146, 0.0993, 0.1147, 0.1770],
        [0.0584, 0.0512, 0.0973, 0.1138, 0.0898, 0.1002, 0.0842, 0.4051],
        [0.0416, 0.0381, 0.0906, 0.0830, 0.0596, 0.0954, 0.0835, 0.5082],
        [0.0290, 0.0279, 0.0430, 0.0729, 0.0763, 0.0876, 0.0685, 0.5949]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.0952, 0.0771, 0.1482, 0.1150, 0.1129, 0.0868, 0.1102, 0.2546],
        [0.0543, 0.0468, 0.0880, 0.0850, 0.0713, 0.0838, 0.0819, 0.4890],
        [0.0365, 0.0307, 0.0719, 0.0768, 0.0649, 0.0776, 0.0601, 0.5816],
        [0.0255, 0.0225, 0.0330, 0.0507, 0.0515, 0.0596, 0.0653, 0.6921],
        [0.0238, 0.0213, 0.0247, 0.0682, 0.0597, 0.0683, 0.0769, 0.6570]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1832, 0.1555, 0.1289, 0.0963, 0.1180, 0.1155, 0.1083, 0.0942],
        [0.1639, 0.1461, 0.0968, 0.1195, 0.1196, 0.1072, 0.1165, 0.1304]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1769, 0.1579, 0.1272, 0.1051, 0.1167, 0.1214, 0.1023, 0.0925],
        [0.1672, 0.1482, 0.1272, 0.1087, 0.1104, 0.1076, 0.1212, 0.1095],
        [0.0891, 0.0832, 0.1649, 0.1093, 0.1268, 0.1257, 0.1504, 0.1507]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1639, 0.1595, 0.0991, 0.1160, 0.1264, 0.1371, 0.1009, 0.0971],
        [0.1738, 0.1522, 0.1280, 0.1105, 0.1107, 0.1101, 0.1129, 0.1017],
        [0.0844, 0.0836, 0.1696, 0.1182, 0.1274, 0.1328, 0.1346, 0.1494],
        [0.0757, 0.0754, 0.1543, 0.1189, 0.1292, 0.1336, 0.1403, 0.1725]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1771, 0.1694, 0.1272, 0.1238, 0.0993, 0.0954, 0.1054, 0.1025],
        [0.1687, 0.1569, 0.1314, 0.1059, 0.1104, 0.1143, 0.1104, 0.1021],
        [0.0881, 0.0830, 0.1965, 0.1119, 0.1198, 0.1261, 0.1160, 0.1586],
        [0.0781, 0.0753, 0.1685, 0.1439, 0.1074, 0.1251, 0.1394, 0.1621],
        [0.0779, 0.0731, 0.1488, 0.1180, 0.1214, 0.1295, 0.1399, 0.1915]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/24 05:29:12 PM | Train: [27/50] Step 000/390 Loss 0.644 Prec@(1,5) (82.8%, 93.8%)
07/24 05:33:52 PM | Train: [27/50] Step 050/390 Loss 0.515 Prec@(1,5) (84.5%, 98.6%)
07/24 05:38:42 PM | Train: [27/50] Step 100/390 Loss 0.498 Prec@(1,5) (85.8%, 98.5%)
07/24 05:43:23 PM | Train: [27/50] Step 150/390 Loss 0.498 Prec@(1,5) (85.6%, 98.6%)
07/24 05:48:11 PM | Train: [27/50] Step 200/390 Loss 0.505 Prec@(1,5) (85.3%, 98.5%)
07/24 05:52:58 PM | Train: [27/50] Step 250/390 Loss 0.515 Prec@(1,5) (84.9%, 98.5%)
07/24 05:57:39 PM | Train: [27/50] Step 300/390 Loss 0.524 Prec@(1,5) (84.6%, 98.5%)
07/24 06:02:27 PM | Train: [27/50] Step 350/390 Loss 0.532 Prec@(1,5) (84.3%, 98.5%)
07/24 06:06:13 PM | Train: [27/50] Step 390/390 Loss 0.539 Prec@(1,5) (84.0%, 98.4%)
07/24 06:06:13 PM | Train: [27/50] Final Prec@1 84.0240%
07/24 06:06:14 PM | Valid: [27/50] Step 000/390 Loss 1.492 Prec@(1,5) (56.2%, 87.5%)
07/24 06:06:30 PM | Valid: [27/50] Step 050/390 Loss 1.650 Prec@(1,5) (58.4%, 85.2%)
07/24 06:06:46 PM | Valid: [27/50] Step 100/390 Loss 1.630 Prec@(1,5) (58.6%, 85.9%)
07/24 06:07:02 PM | Valid: [27/50] Step 150/390 Loss 1.612 Prec@(1,5) (58.7%, 86.0%)
07/24 06:07:18 PM | Valid: [27/50] Step 200/390 Loss 1.622 Prec@(1,5) (58.5%, 85.7%)
07/24 06:07:34 PM | Valid: [27/50] Step 250/390 Loss 1.614 Prec@(1,5) (58.8%, 85.8%)
07/24 06:07:51 PM | Valid: [27/50] Step 300/390 Loss 1.614 Prec@(1,5) (58.7%, 85.8%)
07/24 06:08:07 PM | Valid: [27/50] Step 350/390 Loss 1.618 Prec@(1,5) (58.6%, 85.7%)
07/24 06:08:20 PM | Valid: [27/50] Step 390/390 Loss 1.627 Prec@(1,5) (58.5%, 85.7%)
07/24 06:08:20 PM | Valid: [27/50] Final Prec@1 58.5160%
07/24 06:08:20 PM | genotype = Genotype(normal=[[('skip_connect', 0), ('dil_conv_5x5', 1)], [('skip_connect', 0), ('dil_conv_3x3', 1)], [('skip_connect', 0), ('sep_conv_3x3', 1)], [('skip_connect', 0), ('skip_connect', 1)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('skip_connect', 2)], [('skip_connect', 2), ('max_pool_3x3', 1)], [('skip_connect', 2), ('skip_connect', 3)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.0918, 0.0944, 0.2204, 0.1597, 0.1114, 0.1170, 0.1039, 0.1014],
        [0.0604, 0.0566, 0.1112, 0.1090, 0.1220, 0.1047, 0.1316, 0.3045]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1031, 0.0967, 0.2132, 0.1195, 0.1000, 0.1187, 0.0917, 0.1570],
        [0.0595, 0.0544, 0.1076, 0.1066, 0.0960, 0.1226, 0.0926, 0.3607],
        [0.0418, 0.0377, 0.0985, 0.1007, 0.0859, 0.1045, 0.0786, 0.4523]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1061, 0.0935, 0.1865, 0.1094, 0.1123, 0.0973, 0.1123, 0.1826],
        [0.0568, 0.0496, 0.0958, 0.1088, 0.0861, 0.0962, 0.0798, 0.4270],
        [0.0386, 0.0356, 0.0865, 0.0786, 0.0551, 0.0899, 0.0778, 0.5379],
        [0.0267, 0.0257, 0.0399, 0.0683, 0.0705, 0.0812, 0.0632, 0.6244]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.0934, 0.0760, 0.1506, 0.1114, 0.1092, 0.0843, 0.1070, 0.2681],
        [0.0525, 0.0453, 0.0870, 0.0802, 0.0677, 0.0785, 0.0765, 0.5123],
        [0.0337, 0.0286, 0.0681, 0.0712, 0.0596, 0.0714, 0.0551, 0.6123],
        [0.0231, 0.0205, 0.0301, 0.0459, 0.0464, 0.0539, 0.0586, 0.7215],
        [0.0217, 0.0196, 0.0226, 0.0622, 0.0546, 0.0625, 0.0700, 0.6869]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1848, 0.1579, 0.1297, 0.0955, 0.1172, 0.1146, 0.1078, 0.0926],
        [0.1651, 0.1486, 0.0947, 0.1189, 0.1190, 0.1074, 0.1159, 0.1304]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1791, 0.1610, 0.1257, 0.1043, 0.1161, 0.1215, 0.1010, 0.0913],
        [0.1697, 0.1515, 0.1272, 0.1080, 0.1096, 0.1055, 0.1202, 0.1082],
        [0.0879, 0.0831, 0.1690, 0.1079, 0.1257, 0.1246, 0.1497, 0.1521]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1666, 0.1633, 0.0973, 0.1145, 0.1258, 0.1368, 0.0999, 0.0959],
        [0.1772, 0.1560, 0.1259, 0.1088, 0.1109, 0.1087, 0.1116, 0.1008],
        [0.0840, 0.0842, 0.1752, 0.1154, 0.1246, 0.1329, 0.1341, 0.1497],
        [0.0750, 0.0751, 0.1579, 0.1160, 0.1278, 0.1345, 0.1404, 0.1733]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1784, 0.1724, 0.1263, 0.1230, 0.0974, 0.0951, 0.1057, 0.1017],
        [0.1707, 0.1603, 0.1305, 0.1043, 0.1094, 0.1140, 0.1103, 0.1005],
        [0.0866, 0.0828, 0.2023, 0.1104, 0.1174, 0.1265, 0.1143, 0.1596],
        [0.0767, 0.0744, 0.1721, 0.1434, 0.1059, 0.1238, 0.1387, 0.1650],
        [0.0766, 0.0723, 0.1514, 0.1168, 0.1189, 0.1284, 0.1384, 0.1972]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/24 06:08:28 PM | Train: [28/50] Step 000/390 Loss 0.426 Prec@(1,5) (89.1%, 98.4%)
07/24 06:13:14 PM | Train: [28/50] Step 050/390 Loss 0.455 Prec@(1,5) (87.2%, 98.9%)
07/24 06:17:56 PM | Train: [28/50] Step 100/390 Loss 0.466 Prec@(1,5) (86.7%, 98.9%)
07/24 06:22:39 PM | Train: [28/50] Step 150/390 Loss 0.472 Prec@(1,5) (86.5%, 98.9%)
07/24 06:27:19 PM | Train: [28/50] Step 200/390 Loss 0.480 Prec@(1,5) (86.2%, 98.7%)
07/24 06:32:03 PM | Train: [28/50] Step 250/390 Loss 0.486 Prec@(1,5) (86.0%, 98.7%)
07/24 06:36:50 PM | Train: [28/50] Step 300/390 Loss 0.493 Prec@(1,5) (85.7%, 98.6%)
07/24 06:41:34 PM | Train: [28/50] Step 350/390 Loss 0.494 Prec@(1,5) (85.6%, 98.6%)
07/24 06:45:23 PM | Train: [28/50] Step 390/390 Loss 0.498 Prec@(1,5) (85.6%, 98.6%)
07/24 06:45:23 PM | Train: [28/50] Final Prec@1 85.5920%
07/24 06:45:23 PM | Valid: [28/50] Step 000/390 Loss 1.665 Prec@(1,5) (62.5%, 85.9%)
07/24 06:45:40 PM | Valid: [28/50] Step 050/390 Loss 1.630 Prec@(1,5) (58.8%, 85.2%)
07/24 06:45:56 PM | Valid: [28/50] Step 100/390 Loss 1.646 Prec@(1,5) (58.5%, 85.2%)
07/24 06:46:13 PM | Valid: [28/50] Step 150/390 Loss 1.639 Prec@(1,5) (58.5%, 85.3%)
07/24 06:46:29 PM | Valid: [28/50] Step 200/390 Loss 1.638 Prec@(1,5) (58.7%, 85.5%)
07/24 06:46:47 PM | Valid: [28/50] Step 250/390 Loss 1.627 Prec@(1,5) (58.9%, 85.5%)
07/24 06:47:05 PM | Valid: [28/50] Step 300/390 Loss 1.620 Prec@(1,5) (58.9%, 85.5%)
07/24 06:47:26 PM | Valid: [28/50] Step 350/390 Loss 1.622 Prec@(1,5) (58.7%, 85.6%)
07/24 06:47:40 PM | Valid: [28/50] Step 390/390 Loss 1.624 Prec@(1,5) (58.7%, 85.6%)
07/24 06:47:40 PM | Valid: [28/50] Final Prec@1 58.6520%
07/24 06:47:40 PM | genotype = Genotype(normal=[[('skip_connect', 0), ('dil_conv_5x5', 1)], [('skip_connect', 0), ('dil_conv_3x3', 1)], [('skip_connect', 0), ('sep_conv_3x3', 1)], [('skip_connect', 0), ('skip_connect', 1)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('skip_connect', 2), ('max_pool_3x3', 0)], [('skip_connect', 2), ('max_pool_3x3', 1)], [('skip_connect', 2), ('skip_connect', 3)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.0909, 0.0944, 0.2267, 0.1595, 0.1092, 0.1160, 0.1028, 0.1005],
        [0.0587, 0.0549, 0.1103, 0.1069, 0.1180, 0.1017, 0.1282, 0.3213]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1021, 0.0963, 0.2168, 0.1179, 0.0984, 0.1175, 0.0903, 0.1607],
        [0.0575, 0.0526, 0.1059, 0.1039, 0.0931, 0.1185, 0.0884, 0.3801],
        [0.0395, 0.0356, 0.0955, 0.0958, 0.0810, 0.0995, 0.0739, 0.4792]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1052, 0.0934, 0.1915, 0.1066, 0.1101, 0.0950, 0.1102, 0.1879],
        [0.0548, 0.0476, 0.0938, 0.1047, 0.0812, 0.0919, 0.0756, 0.4505],
        [0.0360, 0.0333, 0.0823, 0.0740, 0.0510, 0.0842, 0.0729, 0.5663],
        [0.0244, 0.0236, 0.0368, 0.0632, 0.0645, 0.0748, 0.0579, 0.6547]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.0920, 0.0752, 0.1526, 0.1083, 0.1064, 0.0818, 0.1044, 0.2791],
        [0.0501, 0.0432, 0.0847, 0.0757, 0.0635, 0.0731, 0.0713, 0.5383],
        [0.0312, 0.0265, 0.0640, 0.0663, 0.0550, 0.0666, 0.0509, 0.6395],
        [0.0208, 0.0185, 0.0273, 0.0415, 0.0421, 0.0488, 0.0531, 0.7477],
        [0.0198, 0.0179, 0.0207, 0.0566, 0.0500, 0.0569, 0.0640, 0.7141]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1863, 0.1610, 0.1290, 0.0948, 0.1166, 0.1140, 0.1074, 0.0909],
        [0.1667, 0.1510, 0.0925, 0.1179, 0.1187, 0.1059, 0.1155, 0.1317]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1805, 0.1632, 0.1251, 0.1037, 0.1157, 0.1214, 0.1008, 0.0897],
        [0.1714, 0.1537, 0.1265, 0.1083, 0.1082, 0.1046, 0.1202, 0.1071],
        [0.0866, 0.0827, 0.1725, 0.1063, 0.1241, 0.1247, 0.1492, 0.1539]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1694, 0.1672, 0.0952, 0.1136, 0.1243, 0.1368, 0.0994, 0.0941],
        [0.1801, 0.1593, 0.1253, 0.1071, 0.1103, 0.1068, 0.1108, 0.1002],
        [0.0830, 0.0844, 0.1805, 0.1133, 0.1221, 0.1325, 0.1337, 0.1505],
        [0.0740, 0.0751, 0.1622, 0.1136, 0.1272, 0.1333, 0.1404, 0.1741]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1808, 0.1764, 0.1258, 0.1209, 0.0964, 0.0944, 0.1050, 0.1003],
        [0.1729, 0.1631, 0.1308, 0.1028, 0.1086, 0.1138, 0.1095, 0.0986],
        [0.0850, 0.0824, 0.2095, 0.1091, 0.1138, 0.1259, 0.1128, 0.1615],
        [0.0755, 0.0738, 0.1767, 0.1424, 0.1036, 0.1234, 0.1384, 0.1663],
        [0.0749, 0.0710, 0.1532, 0.1147, 0.1160, 0.1285, 0.1381, 0.2037]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/24 06:47:48 PM | Train: [29/50] Step 000/390 Loss 0.434 Prec@(1,5) (90.6%, 98.4%)
07/24 06:52:33 PM | Train: [29/50] Step 050/390 Loss 0.413 Prec@(1,5) (88.8%, 99.2%)
07/24 06:57:15 PM | Train: [29/50] Step 100/390 Loss 0.418 Prec@(1,5) (88.6%, 99.0%)
07/24 07:01:58 PM | Train: [29/50] Step 150/390 Loss 0.436 Prec@(1,5) (87.8%, 99.1%)
07/24 07:06:38 PM | Train: [29/50] Step 200/390 Loss 0.446 Prec@(1,5) (87.4%, 99.0%)
07/24 07:11:21 PM | Train: [29/50] Step 250/390 Loss 0.452 Prec@(1,5) (87.4%, 98.9%)
07/24 07:16:06 PM | Train: [29/50] Step 300/390 Loss 0.457 Prec@(1,5) (87.0%, 98.9%)
07/24 07:20:50 PM | Train: [29/50] Step 350/390 Loss 0.460 Prec@(1,5) (86.9%, 98.9%)
07/24 07:24:37 PM | Train: [29/50] Step 390/390 Loss 0.467 Prec@(1,5) (86.7%, 98.8%)
07/24 07:24:37 PM | Train: [29/50] Final Prec@1 86.7280%
07/24 07:24:38 PM | Valid: [29/50] Step 000/390 Loss 2.157 Prec@(1,5) (48.4%, 79.7%)
07/24 07:24:54 PM | Valid: [29/50] Step 050/390 Loss 1.624 Prec@(1,5) (59.7%, 84.5%)
07/24 07:25:11 PM | Valid: [29/50] Step 100/390 Loss 1.654 Prec@(1,5) (58.5%, 84.3%)
07/24 07:25:27 PM | Valid: [29/50] Step 150/390 Loss 1.628 Prec@(1,5) (58.7%, 85.2%)
07/24 07:25:43 PM | Valid: [29/50] Step 200/390 Loss 1.634 Prec@(1,5) (58.4%, 85.3%)
07/24 07:26:00 PM | Valid: [29/50] Step 250/390 Loss 1.633 Prec@(1,5) (58.3%, 85.5%)
07/24 07:26:16 PM | Valid: [29/50] Step 300/390 Loss 1.617 Prec@(1,5) (58.5%, 85.7%)
07/24 07:26:32 PM | Valid: [29/50] Step 350/390 Loss 1.617 Prec@(1,5) (58.5%, 85.5%)
07/24 07:26:45 PM | Valid: [29/50] Step 390/390 Loss 1.626 Prec@(1,5) (58.4%, 85.3%)
07/24 07:26:45 PM | Valid: [29/50] Final Prec@1 58.4200%
07/24 07:26:45 PM | genotype = Genotype(normal=[[('skip_connect', 0), ('dil_conv_5x5', 1)], [('skip_connect', 0), ('dil_conv_3x3', 1)], [('skip_connect', 0), ('sep_conv_3x3', 1)], [('skip_connect', 0), ('skip_connect', 1)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('skip_connect', 2), ('max_pool_3x3', 0)], [('skip_connect', 2), ('max_pool_3x3', 1)], [('skip_connect', 2), ('skip_connect', 3)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.0896, 0.0941, 0.2322, 0.1587, 0.1084, 0.1156, 0.1017, 0.0996],
        [0.0569, 0.0533, 0.1093, 0.1045, 0.1148, 0.0992, 0.1244, 0.3377]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1000, 0.0950, 0.2188, 0.1168, 0.0979, 0.1182, 0.0880, 0.1653],
        [0.0557, 0.0510, 0.1044, 0.1018, 0.0890, 0.1149, 0.0849, 0.3982],
        [0.0373, 0.0338, 0.0923, 0.0921, 0.0770, 0.0946, 0.0701, 0.5027]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1036, 0.0932, 0.1958, 0.1040, 0.1077, 0.0928, 0.1085, 0.1945],
        [0.0525, 0.0460, 0.0919, 0.1001, 0.0763, 0.0872, 0.0721, 0.4739],
        [0.0335, 0.0313, 0.0784, 0.0697, 0.0476, 0.0795, 0.0672, 0.5926],
        [0.0224, 0.0218, 0.0341, 0.0581, 0.0592, 0.0683, 0.0529, 0.6832]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.0908, 0.0742, 0.1551, 0.1056, 0.1028, 0.0791, 0.1008, 0.2917],
        [0.0478, 0.0414, 0.0823, 0.0714, 0.0599, 0.0684, 0.0665, 0.5623],
        [0.0289, 0.0247, 0.0602, 0.0610, 0.0507, 0.0612, 0.0464, 0.6670],
        [0.0190, 0.0170, 0.0250, 0.0374, 0.0381, 0.0440, 0.0476, 0.7720],
        [0.0180, 0.0164, 0.0190, 0.0514, 0.0454, 0.0521, 0.0586, 0.7390]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1899, 0.1647, 0.1284, 0.0939, 0.1148, 0.1137, 0.1056, 0.0890],
        [0.1682, 0.1532, 0.0907, 0.1169, 0.1192, 0.1045, 0.1150, 0.1323]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1826, 0.1665, 0.1244, 0.1026, 0.1145, 0.1217, 0.0995, 0.0882],
        [0.1734, 0.1570, 0.1262, 0.1068, 0.1067, 0.1032, 0.1199, 0.1068],
        [0.0855, 0.0829, 0.1774, 0.1042, 0.1224, 0.1221, 0.1494, 0.1561]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1722, 0.1713, 0.0927, 0.1121, 0.1233, 0.1371, 0.0988, 0.0924],
        [0.1826, 0.1626, 0.1252, 0.1057, 0.1099, 0.1052, 0.1096, 0.0992],
        [0.0817, 0.0848, 0.1860, 0.1105, 0.1207, 0.1324, 0.1332, 0.1507],
        [0.0724, 0.0745, 0.1653, 0.1110, 0.1263, 0.1343, 0.1400, 0.1761]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1842, 0.1800, 0.1248, 0.1190, 0.0954, 0.0927, 0.1045, 0.0994],
        [0.1750, 0.1665, 0.1292, 0.1013, 0.1078, 0.1134, 0.1098, 0.0969],
        [0.0836, 0.0822, 0.2164, 0.1075, 0.1118, 0.1250, 0.1114, 0.1621],
        [0.0735, 0.0727, 0.1796, 0.1416, 0.1016, 0.1233, 0.1395, 0.1683],
        [0.0735, 0.0700, 0.1556, 0.1124, 0.1137, 0.1283, 0.1374, 0.2090]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/24 07:26:53 PM | Train: [30/50] Step 000/390 Loss 0.415 Prec@(1,5) (89.1%, 98.4%)
07/24 07:31:33 PM | Train: [30/50] Step 050/390 Loss 0.405 Prec@(1,5) (89.3%, 99.0%)
07/24 07:36:17 PM | Train: [30/50] Step 100/390 Loss 0.400 Prec@(1,5) (89.4%, 99.2%)
07/24 07:41:01 PM | Train: [30/50] Step 150/390 Loss 0.407 Prec@(1,5) (89.2%, 99.2%)
07/24 07:45:40 PM | Train: [30/50] Step 200/390 Loss 0.411 Prec@(1,5) (89.0%, 99.2%)
07/24 07:50:20 PM | Train: [30/50] Step 250/390 Loss 0.413 Prec@(1,5) (88.7%, 99.2%)
07/24 07:55:02 PM | Train: [30/50] Step 300/390 Loss 0.421 Prec@(1,5) (88.4%, 99.2%)
07/24 07:59:51 PM | Train: [30/50] Step 350/390 Loss 0.430 Prec@(1,5) (88.1%, 99.1%)
07/24 08:03:39 PM | Train: [30/50] Step 390/390 Loss 0.434 Prec@(1,5) (87.9%, 99.0%)
07/24 08:03:40 PM | Train: [30/50] Final Prec@1 87.8880%
07/24 08:03:40 PM | Valid: [30/50] Step 000/390 Loss 2.029 Prec@(1,5) (54.7%, 79.7%)
07/24 08:03:58 PM | Valid: [30/50] Step 050/390 Loss 1.688 Prec@(1,5) (58.1%, 85.1%)
07/24 08:04:16 PM | Valid: [30/50] Step 100/390 Loss 1.652 Prec@(1,5) (58.8%, 84.9%)
07/24 08:04:33 PM | Valid: [30/50] Step 150/390 Loss 1.670 Prec@(1,5) (58.6%, 84.6%)
07/24 08:04:49 PM | Valid: [30/50] Step 200/390 Loss 1.665 Prec@(1,5) (58.5%, 84.8%)
07/24 08:05:06 PM | Valid: [30/50] Step 250/390 Loss 1.658 Prec@(1,5) (58.4%, 84.8%)
07/24 08:05:24 PM | Valid: [30/50] Step 300/390 Loss 1.646 Prec@(1,5) (58.6%, 85.0%)
07/24 08:05:42 PM | Valid: [30/50] Step 350/390 Loss 1.633 Prec@(1,5) (58.8%, 85.1%)
07/24 08:05:56 PM | Valid: [30/50] Step 390/390 Loss 1.623 Prec@(1,5) (59.0%, 85.2%)
07/24 08:05:56 PM | Valid: [30/50] Final Prec@1 58.9720%
07/24 08:05:56 PM | genotype = Genotype(normal=[[('skip_connect', 0), ('dil_conv_5x5', 1)], [('skip_connect', 0), ('dil_conv_3x3', 1)], [('skip_connect', 0), ('sep_conv_3x3', 1)], [('skip_connect', 0), ('skip_connect', 1)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('skip_connect', 2), ('max_pool_3x3', 0)], [('skip_connect', 2), ('max_pool_3x3', 1)], [('skip_connect', 2), ('skip_connect', 3)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.0877, 0.0932, 0.2364, 0.1586, 0.1062, 0.1161, 0.1018, 0.0999],
        [0.0553, 0.0521, 0.1089, 0.1015, 0.1113, 0.0966, 0.1211, 0.3533]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.0976, 0.0936, 0.2198, 0.1168, 0.0967, 0.1183, 0.0864, 0.1708],
        [0.0537, 0.0493, 0.1025, 0.0990, 0.0857, 0.1116, 0.0808, 0.4173],
        [0.0349, 0.0315, 0.0880, 0.0887, 0.0727, 0.0895, 0.0663, 0.5285]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1022, 0.0930, 0.2002, 0.1013, 0.1058, 0.0907, 0.1057, 0.2012],
        [0.0505, 0.0442, 0.0895, 0.0952, 0.0724, 0.0820, 0.0683, 0.4979],
        [0.0312, 0.0292, 0.0740, 0.0650, 0.0440, 0.0744, 0.0624, 0.6198],
        [0.0206, 0.0201, 0.0314, 0.0533, 0.0543, 0.0628, 0.0485, 0.7090]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.0891, 0.0728, 0.1562, 0.1030, 0.0994, 0.0770, 0.0979, 0.3046],
        [0.0459, 0.0399, 0.0807, 0.0672, 0.0561, 0.0643, 0.0619, 0.5841],
        [0.0268, 0.0230, 0.0566, 0.0570, 0.0468, 0.0561, 0.0429, 0.6908],
        [0.0173, 0.0155, 0.0229, 0.0337, 0.0343, 0.0395, 0.0423, 0.7945],
        [0.0165, 0.0152, 0.0176, 0.0470, 0.0413, 0.0472, 0.0533, 0.7619]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1924, 0.1682, 0.1273, 0.0921, 0.1140, 0.1134, 0.1046, 0.0881],
        [0.1701, 0.1567, 0.0888, 0.1151, 0.1197, 0.1036, 0.1138, 0.1321]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1833, 0.1689, 0.1241, 0.1023, 0.1131, 0.1222, 0.0989, 0.0873],
        [0.1761, 0.1613, 0.1256, 0.1049, 0.1058, 0.1020, 0.1186, 0.1057],
        [0.0844, 0.0826, 0.1804, 0.1036, 0.1220, 0.1210, 0.1495, 0.1565]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1744, 0.1749, 0.0904, 0.1115, 0.1220, 0.1379, 0.0977, 0.0911],
        [0.1871, 0.1681, 0.1232, 0.1035, 0.1094, 0.1034, 0.1073, 0.0980],
        [0.0811, 0.0854, 0.1920, 0.1081, 0.1177, 0.1318, 0.1320, 0.1520],
        [0.0719, 0.0741, 0.1694, 0.1092, 0.1251, 0.1336, 0.1388, 0.1780]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1871, 0.1839, 0.1236, 0.1176, 0.0936, 0.0913, 0.1044, 0.0985],
        [0.1774, 0.1712, 0.1285, 0.0989, 0.1068, 0.1134, 0.1089, 0.0949],
        [0.0823, 0.0820, 0.2231, 0.1057, 0.1104, 0.1240, 0.1092, 0.1632],
        [0.0725, 0.0720, 0.1833, 0.1400, 0.1000, 0.1227, 0.1393, 0.1703],
        [0.0723, 0.0693, 0.1579, 0.1105, 0.1115, 0.1272, 0.1368, 0.2146]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/24 08:06:03 PM | Train: [31/50] Step 000/390 Loss 0.472 Prec@(1,5) (85.9%, 100.0%)
07/24 08:10:52 PM | Train: [31/50] Step 050/390 Loss 0.390 Prec@(1,5) (89.5%, 99.2%)
07/24 08:15:33 PM | Train: [31/50] Step 100/390 Loss 0.387 Prec@(1,5) (89.9%, 99.4%)
07/24 08:20:14 PM | Train: [31/50] Step 150/390 Loss 0.392 Prec@(1,5) (89.7%, 99.3%)
07/24 08:24:55 PM | Train: [31/50] Step 200/390 Loss 0.396 Prec@(1,5) (89.5%, 99.3%)
07/24 08:29:36 PM | Train: [31/50] Step 250/390 Loss 0.397 Prec@(1,5) (89.4%, 99.2%)
07/24 08:34:21 PM | Train: [31/50] Step 300/390 Loss 0.404 Prec@(1,5) (89.2%, 99.2%)
07/24 08:39:01 PM | Train: [31/50] Step 350/390 Loss 0.410 Prec@(1,5) (89.1%, 99.1%)
07/24 08:42:51 PM | Train: [31/50] Step 390/390 Loss 0.413 Prec@(1,5) (88.9%, 99.1%)
07/24 08:42:51 PM | Train: [31/50] Final Prec@1 88.8920%
07/24 08:42:52 PM | Valid: [31/50] Step 000/390 Loss 1.698 Prec@(1,5) (56.2%, 84.4%)
07/24 08:43:08 PM | Valid: [31/50] Step 050/390 Loss 1.623 Prec@(1,5) (58.1%, 85.0%)
07/24 08:43:25 PM | Valid: [31/50] Step 100/390 Loss 1.613 Prec@(1,5) (58.2%, 85.4%)
07/24 08:43:42 PM | Valid: [31/50] Step 150/390 Loss 1.608 Prec@(1,5) (58.4%, 85.5%)
07/24 08:43:59 PM | Valid: [31/50] Step 200/390 Loss 1.588 Prec@(1,5) (59.0%, 85.8%)
07/24 08:44:15 PM | Valid: [31/50] Step 250/390 Loss 1.576 Prec@(1,5) (59.2%, 86.1%)
07/24 08:44:31 PM | Valid: [31/50] Step 300/390 Loss 1.576 Prec@(1,5) (59.3%, 86.0%)
07/24 08:44:48 PM | Valid: [31/50] Step 350/390 Loss 1.573 Prec@(1,5) (59.5%, 86.1%)
07/24 08:45:01 PM | Valid: [31/50] Step 390/390 Loss 1.575 Prec@(1,5) (59.3%, 86.1%)
07/24 08:45:01 PM | Valid: [31/50] Final Prec@1 59.3320%
07/24 08:45:01 PM | genotype = Genotype(normal=[[('skip_connect', 0), ('dil_conv_5x5', 1)], [('skip_connect', 0), ('dil_conv_3x3', 1)], [('skip_connect', 0), ('sep_conv_3x3', 1)], [('skip_connect', 0), ('skip_connect', 1)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('skip_connect', 2), ('max_pool_3x3', 0)], [('skip_connect', 2), ('max_pool_3x3', 1)], [('skip_connect', 2), ('skip_connect', 3)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.0863, 0.0925, 0.2398, 0.1595, 0.1054, 0.1153, 0.1013, 0.0998],
        [0.0537, 0.0509, 0.1081, 0.0982, 0.1073, 0.0942, 0.1172, 0.3703]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.0956, 0.0922, 0.2214, 0.1165, 0.0955, 0.1166, 0.0851, 0.1770],
        [0.0520, 0.0478, 0.1006, 0.0962, 0.0815, 0.1062, 0.0774, 0.4383],
        [0.0327, 0.0297, 0.0840, 0.0841, 0.0693, 0.0851, 0.0622, 0.5530]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1003, 0.0924, 0.2034, 0.0994, 0.1028, 0.0896, 0.1043, 0.2079],
        [0.0487, 0.0429, 0.0878, 0.0900, 0.0678, 0.0774, 0.0641, 0.5213],
        [0.0291, 0.0275, 0.0701, 0.0607, 0.0411, 0.0695, 0.0583, 0.6437],
        [0.0190, 0.0187, 0.0291, 0.0495, 0.0504, 0.0582, 0.0447, 0.7304]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.0873, 0.0717, 0.1583, 0.0985, 0.0957, 0.0744, 0.0935, 0.3207],
        [0.0441, 0.0386, 0.0791, 0.0637, 0.0530, 0.0602, 0.0581, 0.6031],
        [0.0248, 0.0214, 0.0528, 0.0528, 0.0426, 0.0516, 0.0394, 0.7146],
        [0.0158, 0.0143, 0.0210, 0.0307, 0.0311, 0.0358, 0.0382, 0.8130],
        [0.0152, 0.0140, 0.0163, 0.0429, 0.0379, 0.0430, 0.0487, 0.7820]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1945, 0.1712, 0.1264, 0.0914, 0.1134, 0.1127, 0.1042, 0.0861],
        [0.1717, 0.1595, 0.0873, 0.1135, 0.1182, 0.1022, 0.1140, 0.1334]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1845, 0.1718, 0.1232, 0.1015, 0.1118, 0.1230, 0.0982, 0.0859],
        [0.1786, 0.1654, 0.1245, 0.1039, 0.1046, 0.1007, 0.1169, 0.1054],
        [0.0835, 0.0823, 0.1844, 0.1017, 0.1203, 0.1210, 0.1502, 0.1567]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1760, 0.1789, 0.0883, 0.1112, 0.1208, 0.1381, 0.0972, 0.0893],
        [0.1893, 0.1717, 0.1218, 0.1025, 0.1088, 0.1021, 0.1062, 0.0977],
        [0.0801, 0.0852, 0.1960, 0.1072, 0.1155, 0.1319, 0.1324, 0.1516],
        [0.0711, 0.0737, 0.1734, 0.1069, 0.1230, 0.1331, 0.1386, 0.1802]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1891, 0.1875, 0.1232, 0.1161, 0.0922, 0.0903, 0.1042, 0.0974],
        [0.1796, 0.1762, 0.1275, 0.0967, 0.1059, 0.1123, 0.1083, 0.0934],
        [0.0811, 0.0815, 0.2296, 0.1051, 0.1082, 0.1238, 0.1070, 0.1637],
        [0.0714, 0.0715, 0.1881, 0.1382, 0.0974, 0.1204, 0.1402, 0.1728],
        [0.0713, 0.0692, 0.1628, 0.1086, 0.1082, 0.1245, 0.1352, 0.2202]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/24 08:45:09 PM | Train: [32/50] Step 000/390 Loss 0.248 Prec@(1,5) (96.9%, 100.0%)
07/24 08:49:48 PM | Train: [32/50] Step 050/390 Loss 0.347 Prec@(1,5) (91.7%, 99.6%)
07/24 08:54:29 PM | Train: [32/50] Step 100/390 Loss 0.353 Prec@(1,5) (91.1%, 99.6%)
07/24 08:59:17 PM | Train: [32/50] Step 150/390 Loss 0.361 Prec@(1,5) (90.9%, 99.5%)
