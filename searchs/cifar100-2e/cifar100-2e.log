07/23 08:37:14 AM | 
07/23 08:37:14 AM | Parameters:
07/23 08:37:14 AM | ALPHA_LR=0.0003
07/23 08:37:14 AM | ALPHA_WEIGHT_DECAY=0.001
07/23 08:37:14 AM | BATCH_SIZE=64
07/23 08:37:14 AM | DATA_PATH=./data/
07/23 08:37:14 AM | DATASET=cifar100
07/23 08:37:14 AM | EPOCHS=50
07/23 08:37:14 AM | GPUS=[0]
07/23 08:37:14 AM | INIT_CHANNELS=16
07/23 08:37:14 AM | LAYERS=8
07/23 08:37:14 AM | NAME=cifar100-2e
07/23 08:37:14 AM | PATH=searchs/cifar100-2e
07/23 08:37:14 AM | PLOT_PATH=searchs/cifar100-2e/plots
07/23 08:37:14 AM | PRINT_FREQ=50
07/23 08:37:14 AM | SEED=2
07/23 08:37:14 AM | W_GRAD_CLIP=5.0
07/23 08:37:14 AM | W_LR=0.025
07/23 08:37:14 AM | W_LR_MIN=0.001
07/23 08:37:14 AM | W_MOMENTUM=0.9
07/23 08:37:14 AM | W_WEIGHT_DECAY=0.0003
07/23 08:37:14 AM | WORKERS=4
07/23 08:37:14 AM | 
07/23 08:37:14 AM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/23 08:37:23 AM | size of train loader is 391
07/23 08:37:29 AM | Train: [ 1/50] Step 000/390 Loss 0.012 Prec@(1,5) (1.6%, 9.4%)
07/23 08:40:59 AM | Train: [ 1/50] Step 050/390 Loss 0.012 Prec@(1,5) (1.0%, 5.2%)
07/23 08:43:01 AM | 
07/23 08:43:01 AM | Parameters:
07/23 08:43:01 AM | ALPHA_LR=0.0003
07/23 08:43:01 AM | ALPHA_WEIGHT_DECAY=0.001
07/23 08:43:01 AM | BATCH_SIZE=64
07/23 08:43:01 AM | DATA_PATH=./data/
07/23 08:43:01 AM | DATASET=cifar100
07/23 08:43:01 AM | EPOCHS=50
07/23 08:43:01 AM | GPUS=[0]
07/23 08:43:01 AM | INIT_CHANNELS=16
07/23 08:43:01 AM | LAYERS=8
07/23 08:43:01 AM | NAME=cifar100-2e
07/23 08:43:01 AM | PATH=searchs/cifar100-2e
07/23 08:43:01 AM | PLOT_PATH=searchs/cifar100-2e/plots
07/23 08:43:01 AM | PRINT_FREQ=50
07/23 08:43:01 AM | SEED=2
07/23 08:43:01 AM | W_GRAD_CLIP=5.0
07/23 08:43:01 AM | W_LR=0.025
07/23 08:43:01 AM | W_LR_MIN=0.001
07/23 08:43:01 AM | W_MOMENTUM=0.9
07/23 08:43:01 AM | W_WEIGHT_DECAY=0.0003
07/23 08:43:01 AM | WORKERS=4
07/23 08:43:01 AM | 
07/23 08:43:01 AM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/23 08:43:08 AM | size of train loader is 391
07/23 08:43:17 AM | Train: [ 1/50] Step 000/390 Loss 0.012 Prec@(1,5) (1.6%, 9.4%)
07/23 08:46:47 AM | Train: [ 1/50] Step 050/390 Loss 0.012 Prec@(1,5) (1.0%, 5.2%)
07/23 08:50:35 AM | Train: [ 1/50] Step 100/390 Loss 0.012 Prec@(1,5) (1.2%, 5.5%)
07/23 08:54:14 AM | Train: [ 1/50] Step 150/390 Loss 0.012 Prec@(1,5) (1.3%, 5.9%)
07/23 08:57:53 AM | Train: [ 1/50] Step 200/390 Loss 0.012 Prec@(1,5) (1.3%, 6.1%)
07/23 09:01:31 AM | Train: [ 1/50] Step 250/390 Loss 0.012 Prec@(1,5) (1.5%, 6.6%)
07/23 09:05:08 AM | Train: [ 1/50] Step 300/390 Loss 0.012 Prec@(1,5) (1.7%, 6.9%)
07/23 09:08:34 AM | Train: [ 1/50] Step 350/390 Loss 0.012 Prec@(1,5) (1.8%, 7.3%)
07/23 09:11:28 AM | Train: [ 1/50] Step 390/390 Loss 0.012 Prec@(1,5) (1.9%, 7.6%)
07/23 09:11:28 AM | Train: [ 1/50] Final Prec@1 1.9360%
07/23 09:11:29 AM | Valid: [ 1/50] Step 000/390 Loss 4.342 Prec@(1,5) (6.2%, 9.4%)
07/23 09:11:41 AM | Valid: [ 1/50] Step 050/390 Loss 4.483 Prec@(1,5) (3.4%, 11.6%)
07/23 09:11:51 AM | Valid: [ 1/50] Step 100/390 Loss 4.490 Prec@(1,5) (3.0%, 11.1%)
07/23 09:12:04 AM | Valid: [ 1/50] Step 150/390 Loss 4.492 Prec@(1,5) (3.1%, 11.1%)
07/23 09:12:16 AM | Valid: [ 1/50] Step 200/390 Loss 4.491 Prec@(1,5) (3.1%, 11.1%)
07/23 09:12:27 AM | Valid: [ 1/50] Step 250/390 Loss 4.495 Prec@(1,5) (2.9%, 10.9%)
07/23 09:12:40 AM | Valid: [ 1/50] Step 300/390 Loss 4.497 Prec@(1,5) (2.9%, 10.8%)
07/23 09:12:55 AM | Valid: [ 1/50] Step 350/390 Loss 4.498 Prec@(1,5) (2.8%, 10.7%)
07/23 09:13:06 AM | Valid: [ 1/50] Step 390/390 Loss 4.499 Prec@(1,5) (2.8%, 10.6%)
07/23 09:13:06 AM | Valid: [ 1/50] Final Prec@1 2.7920%
07/23 09:13:06 AM | genotype = Genotype(normal=[[('dil_conv_3x3', 0), ('dil_conv_3x3', 1)], [('sep_conv_3x3', 1), ('dil_conv_5x5', 2)], [('dil_conv_3x3', 3), ('dil_conv_3x3', 1)], [('sep_conv_5x5', 1), ('dil_conv_5x5', 2)]], normal_concat=range(2, 6), reduce=[[('avg_pool_3x3', 0), ('skip_connect', 1)], [('skip_connect', 1), ('max_pool_3x3', 0)], [('sep_conv_5x5', 0), ('dil_conv_5x5', 3)], [('max_pool_3x3', 0), ('avg_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1238, 0.1257, 0.1252, 0.1232, 0.1257, 0.1269, 0.1247, 0.1248],
        [0.1239, 0.1250, 0.1258, 0.1243, 0.1240, 0.1259, 0.1256, 0.1256]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1226, 0.1245, 0.1239, 0.1263, 0.1264, 0.1258, 0.1254, 0.1252],
        [0.1216, 0.1225, 0.1233, 0.1280, 0.1248, 0.1267, 0.1270, 0.1262],
        [0.1214, 0.1227, 0.1243, 0.1249, 0.1261, 0.1260, 0.1265, 0.1280]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1234, 0.1236, 0.1240, 0.1237, 0.1248, 0.1271, 0.1267, 0.1268],
        [0.1230, 0.1233, 0.1235, 0.1254, 0.1252, 0.1279, 0.1264, 0.1253],
        [0.1221, 0.1218, 0.1234, 0.1272, 0.1256, 0.1263, 0.1251, 0.1285],
        [0.1219, 0.1217, 0.1219, 0.1259, 0.1273, 0.1284, 0.1269, 0.1259]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1252, 0.1269, 0.1270, 0.1238, 0.1240, 0.1244, 0.1243, 0.1243],
        [0.1225, 0.1223, 0.1238, 0.1246, 0.1281, 0.1257, 0.1238, 0.1292],
        [0.1217, 0.1218, 0.1244, 0.1268, 0.1260, 0.1238, 0.1277, 0.1277],
        [0.1218, 0.1224, 0.1240, 0.1267, 0.1253, 0.1266, 0.1261, 0.1271],
        [0.1224, 0.1227, 0.1237, 0.1233, 0.1276, 0.1266, 0.1252, 0.1285]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1269, 0.1276, 0.1220, 0.1248, 0.1276, 0.1225, 0.1254, 0.1231],
        [0.1228, 0.1229, 0.1267, 0.1257, 0.1259, 0.1266, 0.1252, 0.1243]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1278, 0.1276, 0.1239, 0.1229, 0.1243, 0.1239, 0.1257, 0.1239],
        [0.1214, 0.1216, 0.1278, 0.1268, 0.1252, 0.1270, 0.1225, 0.1277],
        [0.1236, 0.1233, 0.1243, 0.1251, 0.1255, 0.1259, 0.1270, 0.1253]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1252, 0.1229, 0.1249, 0.1272, 0.1250, 0.1228, 0.1270],
        [0.1246, 0.1247, 0.1231, 0.1269, 0.1249, 0.1243, 0.1244, 0.1271],
        [0.1223, 0.1241, 0.1249, 0.1259, 0.1255, 0.1259, 0.1267, 0.1246],
        [0.1225, 0.1239, 0.1252, 0.1265, 0.1246, 0.1238, 0.1271, 0.1264]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1286, 0.1279, 0.1240, 0.1235, 0.1226, 0.1243, 0.1256, 0.1236],
        [0.1241, 0.1230, 0.1270, 0.1253, 0.1233, 0.1267, 0.1242, 0.1263],
        [0.1266, 0.1276, 0.1265, 0.1233, 0.1259, 0.1226, 0.1249, 0.1226],
        [0.1255, 0.1260, 0.1266, 0.1245, 0.1227, 0.1263, 0.1250, 0.1234],
        [0.1260, 0.1266, 0.1267, 0.1227, 0.1246, 0.1249, 0.1241, 0.1244]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/23 09:13:08 AM | size of train loader is 391
07/23 09:13:14 AM | Train: [ 2/50] Step 000/390 Loss 0.011 Prec@(1,5) (1.6%, 12.5%)
07/23 09:16:38 AM | Train: [ 2/50] Step 050/390 Loss 0.012 Prec@(1,5) (2.8%, 11.5%)
07/23 09:20:02 AM | Train: [ 2/50] Step 100/390 Loss 0.011 Prec@(1,5) (3.0%, 11.4%)
07/23 09:23:25 AM | Train: [ 2/50] Step 150/390 Loss 0.011 Prec@(1,5) (3.1%, 11.3%)
07/23 09:26:52 AM | Train: [ 2/50] Step 200/390 Loss 0.011 Prec@(1,5) (3.2%, 11.5%)
07/23 09:30:14 AM | Train: [ 2/50] Step 250/390 Loss 0.011 Prec@(1,5) (3.3%, 11.8%)
07/23 09:33:40 AM | Train: [ 2/50] Step 300/390 Loss 0.011 Prec@(1,5) (3.3%, 11.9%)
07/23 09:37:06 AM | Train: [ 2/50] Step 350/390 Loss 0.011 Prec@(1,5) (3.4%, 11.9%)
07/23 09:39:52 AM | Train: [ 2/50] Step 390/390 Loss 0.011 Prec@(1,5) (3.5%, 12.1%)
07/23 09:39:52 AM | Train: [ 2/50] Final Prec@1 3.4720%
07/23 09:39:52 AM | Valid: [ 2/50] Step 000/390 Loss 4.300 Prec@(1,5) (6.2%, 15.6%)
07/23 09:40:02 AM | Valid: [ 2/50] Step 050/390 Loss 4.418 Prec@(1,5) (3.6%, 13.5%)
07/23 09:40:12 AM | Valid: [ 2/50] Step 100/390 Loss 4.414 Prec@(1,5) (3.9%, 14.0%)
07/23 09:40:23 AM | Valid: [ 2/50] Step 150/390 Loss 4.412 Prec@(1,5) (4.0%, 14.3%)
07/23 09:40:34 AM | Valid: [ 2/50] Step 200/390 Loss 4.414 Prec@(1,5) (4.1%, 14.3%)
07/23 09:40:45 AM | Valid: [ 2/50] Step 250/390 Loss 4.414 Prec@(1,5) (4.0%, 13.9%)
07/23 09:40:55 AM | Valid: [ 2/50] Step 300/390 Loss 4.413 Prec@(1,5) (4.0%, 13.9%)
07/23 09:41:06 AM | Valid: [ 2/50] Step 350/390 Loss 4.413 Prec@(1,5) (4.1%, 14.0%)
07/23 09:41:16 AM | Valid: [ 2/50] Step 390/390 Loss 4.411 Prec@(1,5) (4.1%, 14.1%)
07/23 09:41:16 AM | Valid: [ 2/50] Final Prec@1 4.0760%
07/23 09:41:16 AM | genotype = Genotype(normal=[[('dil_conv_3x3', 0), ('dil_conv_3x3', 1)], [('sep_conv_3x3', 1), ('sep_conv_5x5', 0)], [('dil_conv_3x3', 1), ('sep_conv_5x5', 3)], [('avg_pool_3x3', 0), ('sep_conv_5x5', 1)]], normal_concat=range(2, 6), reduce=[[('avg_pool_3x3', 0), ('sep_conv_5x5', 1)], [('max_pool_3x3', 0), ('dil_conv_3x3', 1)], [('dil_conv_5x5', 3), ('dil_conv_5x5', 2)], [('max_pool_3x3', 0), ('avg_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1235, 0.1263, 0.1260, 0.1223, 0.1258, 0.1278, 0.1241, 0.1242],
        [0.1237, 0.1263, 0.1266, 0.1224, 0.1245, 0.1272, 0.1246, 0.1247]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1215, 0.1255, 0.1243, 0.1265, 0.1281, 0.1254, 0.1253, 0.1235],
        [0.1200, 0.1236, 0.1246, 0.1286, 0.1235, 0.1268, 0.1266, 0.1264],
        [0.1206, 0.1231, 0.1248, 0.1253, 0.1259, 0.1252, 0.1273, 0.1278]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1243, 0.1245, 0.1244, 0.1217, 0.1237, 0.1273, 0.1270, 0.1272],
        [0.1227, 0.1236, 0.1236, 0.1243, 0.1246, 0.1285, 0.1274, 0.1252],
        [0.1228, 0.1225, 0.1240, 0.1277, 0.1257, 0.1253, 0.1235, 0.1284],
        [0.1227, 0.1226, 0.1228, 0.1254, 0.1282, 0.1279, 0.1256, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1272, 0.1293, 0.1291, 0.1226, 0.1226, 0.1222, 0.1238, 0.1233],
        [0.1225, 0.1233, 0.1246, 0.1234, 0.1290, 0.1247, 0.1229, 0.1296],
        [0.1216, 0.1222, 0.1254, 0.1270, 0.1258, 0.1232, 0.1283, 0.1266],
        [0.1223, 0.1237, 0.1262, 0.1266, 0.1241, 0.1254, 0.1252, 0.1266],
        [0.1222, 0.1230, 0.1244, 0.1208, 0.1287, 0.1262, 0.1268, 0.1279]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1286, 0.1300, 0.1203, 0.1260, 0.1276, 0.1201, 0.1266, 0.1208],
        [0.1206, 0.1207, 0.1265, 0.1264, 0.1274, 0.1272, 0.1262, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1306, 0.1303, 0.1212, 0.1207, 0.1236, 0.1244, 0.1255, 0.1237],
        [0.1186, 0.1192, 0.1293, 0.1264, 0.1253, 0.1298, 0.1209, 0.1305],
        [0.1232, 0.1227, 0.1242, 0.1250, 0.1249, 0.1271, 0.1275, 0.1255]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1256, 0.1259, 0.1222, 0.1241, 0.1273, 0.1232, 0.1226, 0.1291],
        [0.1233, 0.1234, 0.1225, 0.1274, 0.1266, 0.1242, 0.1245, 0.1280],
        [0.1204, 0.1227, 0.1241, 0.1265, 0.1252, 0.1268, 0.1288, 0.1255],
        [0.1224, 0.1242, 0.1252, 0.1249, 0.1253, 0.1224, 0.1285, 0.1272]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1323, 0.1316, 0.1218, 0.1226, 0.1201, 0.1232, 0.1257, 0.1226],
        [0.1242, 0.1227, 0.1278, 0.1247, 0.1228, 0.1261, 0.1242, 0.1275],
        [0.1298, 0.1313, 0.1284, 0.1214, 0.1257, 0.1196, 0.1235, 0.1203],
        [0.1273, 0.1284, 0.1283, 0.1236, 0.1203, 0.1268, 0.1241, 0.1213],
        [0.1286, 0.1290, 0.1284, 0.1196, 0.1244, 0.1248, 0.1235, 0.1217]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/23 09:41:17 AM | size of train loader is 391
07/23 09:41:21 AM | Train: [ 3/50] Step 000/390 Loss 0.011 Prec@(1,5) (3.1%, 9.4%)
07/23 09:44:52 AM | Train: [ 3/50] Step 050/390 Loss 0.011 Prec@(1,5) (4.2%, 13.7%)
07/23 09:48:15 AM | Train: [ 3/50] Step 100/390 Loss 0.011 Prec@(1,5) (4.4%, 14.3%)
07/23 09:51:44 AM | Train: [ 3/50] Step 150/390 Loss 0.011 Prec@(1,5) (4.3%, 14.5%)
07/23 09:55:18 AM | Train: [ 3/50] Step 200/390 Loss 0.011 Prec@(1,5) (4.3%, 14.5%)
07/23 09:59:01 AM | Train: [ 3/50] Step 250/390 Loss 0.011 Prec@(1,5) (4.4%, 14.9%)
07/23 10:02:43 AM | Train: [ 3/50] Step 300/390 Loss 0.011 Prec@(1,5) (4.5%, 15.2%)
07/23 10:06:23 AM | Train: [ 3/50] Step 350/390 Loss 0.011 Prec@(1,5) (4.5%, 15.3%)
07/23 10:09:17 AM | Train: [ 3/50] Step 390/390 Loss 0.011 Prec@(1,5) (4.6%, 15.4%)
07/23 10:09:17 AM | Train: [ 3/50] Final Prec@1 4.5600%
07/23 10:09:18 AM | Valid: [ 3/50] Step 000/390 Loss 4.302 Prec@(1,5) (1.6%, 17.2%)
07/23 10:09:30 AM | Valid: [ 3/50] Step 050/390 Loss 4.359 Prec@(1,5) (4.8%, 17.6%)
07/23 10:09:43 AM | Valid: [ 3/50] Step 100/390 Loss 4.359 Prec@(1,5) (5.0%, 17.2%)
07/23 10:09:54 AM | Valid: [ 3/50] Step 150/390 Loss 4.358 Prec@(1,5) (5.2%, 17.3%)
07/23 10:10:06 AM | Valid: [ 3/50] Step 200/390 Loss 4.355 Prec@(1,5) (5.3%, 17.3%)
07/23 10:10:23 AM | Valid: [ 3/50] Step 250/390 Loss 4.348 Prec@(1,5) (5.3%, 17.4%)
07/23 10:10:36 AM | Valid: [ 3/50] Step 300/390 Loss 4.346 Prec@(1,5) (5.3%, 17.4%)
07/23 10:10:47 AM | Valid: [ 3/50] Step 350/390 Loss 4.343 Prec@(1,5) (5.4%, 17.5%)
07/23 10:10:57 AM | Valid: [ 3/50] Step 390/390 Loss 4.342 Prec@(1,5) (5.4%, 17.5%)
07/23 10:10:57 AM | Valid: [ 3/50] Final Prec@1 5.4320%
07/23 10:10:57 AM | genotype = Genotype(normal=[[('avg_pool_3x3', 1), ('dil_conv_3x3', 0)], [('sep_conv_3x3', 1), ('sep_conv_5x5', 0)], [('sep_conv_3x3', 2), ('dil_conv_3x3', 1)], [('avg_pool_3x3', 0), ('skip_connect', 3)]], normal_concat=range(2, 6), reduce=[[('avg_pool_3x3', 0), ('dil_conv_3x3', 1)], [('max_pool_3x3', 0), ('dil_conv_3x3', 1)], [('dil_conv_5x5', 3), ('dil_conv_5x5', 2)], [('max_pool_3x3', 0), ('avg_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1232, 0.1276, 0.1269, 0.1230, 0.1251, 0.1281, 0.1231, 0.1230],
        [0.1245, 0.1288, 0.1285, 0.1200, 0.1240, 0.1268, 0.1244, 0.1229]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1216, 0.1278, 0.1256, 0.1254, 0.1288, 0.1238, 0.1255, 0.1214],
        [0.1195, 0.1256, 0.1270, 0.1294, 0.1222, 0.1257, 0.1256, 0.1249],
        [0.1217, 0.1252, 0.1270, 0.1243, 0.1237, 0.1245, 0.1273, 0.1263]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1256, 0.1264, 0.1262, 0.1191, 0.1238, 0.1265, 0.1272, 0.1252],
        [0.1225, 0.1246, 0.1246, 0.1216, 0.1252, 0.1295, 0.1273, 0.1248],
        [0.1231, 0.1234, 0.1249, 0.1292, 0.1244, 0.1238, 0.1233, 0.1279],
        [0.1238, 0.1247, 0.1246, 0.1252, 0.1272, 0.1268, 0.1242, 0.1235]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1278, 0.1313, 0.1307, 0.1224, 0.1213, 0.1197, 0.1240, 0.1228],
        [0.1243, 0.1268, 0.1273, 0.1216, 0.1271, 0.1244, 0.1208, 0.1278],
        [0.1223, 0.1236, 0.1263, 0.1265, 0.1263, 0.1226, 0.1281, 0.1242],
        [0.1248, 0.1269, 0.1290, 0.1259, 0.1230, 0.1233, 0.1222, 0.1248],
        [0.1239, 0.1255, 0.1266, 0.1178, 0.1258, 0.1271, 0.1279, 0.1254]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1301, 0.1321, 0.1199, 0.1265, 0.1278, 0.1183, 0.1271, 0.1182],
        [0.1197, 0.1198, 0.1249, 0.1255, 0.1275, 0.1282, 0.1279, 0.1264]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1324, 0.1320, 0.1183, 0.1192, 0.1236, 0.1254, 0.1260, 0.1231],
        [0.1173, 0.1179, 0.1299, 0.1260, 0.1240, 0.1312, 0.1210, 0.1327],
        [0.1217, 0.1216, 0.1236, 0.1249, 0.1249, 0.1291, 0.1282, 0.1261]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1270, 0.1271, 0.1200, 0.1247, 0.1268, 0.1220, 0.1211, 0.1312],
        [0.1218, 0.1218, 0.1228, 0.1281, 0.1282, 0.1236, 0.1257, 0.1280],
        [0.1185, 0.1207, 0.1218, 0.1277, 0.1256, 0.1293, 0.1305, 0.1259],
        [0.1216, 0.1228, 0.1243, 0.1247, 0.1258, 0.1222, 0.1299, 0.1288]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1369, 0.1360, 0.1195, 0.1214, 0.1190, 0.1211, 0.1254, 0.1205],
        [0.1237, 0.1221, 0.1281, 0.1242, 0.1219, 0.1259, 0.1249, 0.1292],
        [0.1315, 0.1349, 0.1305, 0.1192, 0.1254, 0.1177, 0.1215, 0.1193],
        [0.1289, 0.1308, 0.1303, 0.1230, 0.1185, 0.1256, 0.1228, 0.1201],
        [0.1300, 0.1311, 0.1294, 0.1176, 0.1253, 0.1245, 0.1228, 0.1193]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/23 10:10:58 AM | size of train loader is 391
07/23 10:11:04 AM | Train: [ 4/50] Step 000/390 Loss 0.011 Prec@(1,5) (3.1%, 15.6%)
07/23 10:14:40 AM | Train: [ 4/50] Step 050/390 Loss 0.011 Prec@(1,5) (5.4%, 17.8%)
07/23 10:17:59 AM | Train: [ 4/50] Step 100/390 Loss 0.011 Prec@(1,5) (5.4%, 17.7%)
07/23 10:21:15 AM | Train: [ 4/50] Step 150/390 Loss 0.011 Prec@(1,5) (5.6%, 18.0%)
07/23 10:24:31 AM | Train: [ 4/50] Step 200/390 Loss 0.011 Prec@(1,5) (5.5%, 18.0%)
07/23 10:28:07 AM | Train: [ 4/50] Step 250/390 Loss 0.011 Prec@(1,5) (5.6%, 18.0%)
07/23 10:31:31 AM | Train: [ 4/50] Step 300/390 Loss 0.011 Prec@(1,5) (5.6%, 18.2%)
07/23 10:35:02 AM | Train: [ 4/50] Step 350/390 Loss 0.011 Prec@(1,5) (5.5%, 18.3%)
07/23 10:37:58 AM | Train: [ 4/50] Step 390/390 Loss 0.011 Prec@(1,5) (5.6%, 18.4%)
07/23 10:37:58 AM | Train: [ 4/50] Final Prec@1 5.6240%
07/23 10:37:58 AM | Valid: [ 4/50] Step 000/390 Loss 4.202 Prec@(1,5) (6.2%, 28.1%)
07/23 10:38:08 AM | Valid: [ 4/50] Step 050/390 Loss 4.268 Prec@(1,5) (6.4%, 20.3%)
07/23 10:38:20 AM | Valid: [ 4/50] Step 100/390 Loss 4.279 Prec@(1,5) (6.5%, 20.3%)
07/23 10:38:36 AM | Valid: [ 4/50] Step 150/390 Loss 4.281 Prec@(1,5) (6.4%, 20.1%)
07/23 10:38:51 AM | Valid: [ 4/50] Step 200/390 Loss 4.281 Prec@(1,5) (6.4%, 20.1%)
07/23 10:39:03 AM | Valid: [ 4/50] Step 250/390 Loss 4.282 Prec@(1,5) (6.3%, 20.0%)
07/23 10:39:16 AM | Valid: [ 4/50] Step 300/390 Loss 4.282 Prec@(1,5) (6.3%, 20.0%)
07/23 10:39:27 AM | Valid: [ 4/50] Step 350/390 Loss 4.281 Prec@(1,5) (6.3%, 20.1%)
07/23 10:39:35 AM | Valid: [ 4/50] Step 390/390 Loss 4.283 Prec@(1,5) (6.3%, 20.0%)
07/23 10:39:35 AM | Valid: [ 4/50] Final Prec@1 6.2800%
07/23 10:39:35 AM | genotype = Genotype(normal=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('skip_connect', 1), ('skip_connect', 2)], [('avg_pool_3x3', 3), ('sep_conv_3x3', 2)], [('avg_pool_3x3', 0), ('skip_connect', 3)]], normal_concat=range(2, 6), reduce=[[('avg_pool_3x3', 0), ('dil_conv_5x5', 1)], [('max_pool_3x3', 0), ('dil_conv_3x3', 1)], [('dil_conv_5x5', 2), ('dil_conv_5x5', 3)], [('max_pool_3x3', 0), ('avg_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1238, 0.1293, 0.1277, 0.1213, 0.1247, 0.1276, 0.1229, 0.1227],
        [0.1270, 0.1327, 0.1322, 0.1172, 0.1232, 0.1262, 0.1217, 0.1197]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1212, 0.1292, 0.1259, 0.1253, 0.1298, 0.1235, 0.1261, 0.1192],
        [0.1201, 0.1287, 0.1301, 0.1286, 0.1222, 0.1228, 0.1247, 0.1228],
        [0.1229, 0.1277, 0.1295, 0.1241, 0.1227, 0.1225, 0.1256, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1269, 0.1280, 0.1278, 0.1173, 0.1237, 0.1249, 0.1268, 0.1246],
        [0.1257, 0.1287, 0.1290, 0.1181, 0.1235, 0.1278, 0.1256, 0.1215],
        [0.1258, 0.1269, 0.1268, 0.1294, 0.1222, 0.1217, 0.1219, 0.1252],
        [0.1280, 0.1296, 0.1289, 0.1228, 0.1257, 0.1246, 0.1216, 0.1188]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1302, 0.1346, 0.1328, 0.1215, 0.1188, 0.1172, 0.1237, 0.1212],
        [0.1269, 0.1306, 0.1310, 0.1205, 0.1254, 0.1229, 0.1188, 0.1240],
        [0.1253, 0.1270, 0.1283, 0.1268, 0.1250, 0.1213, 0.1266, 0.1197],
        [0.1291, 0.1320, 0.1338, 0.1255, 0.1200, 0.1207, 0.1183, 0.1204],
        [0.1277, 0.1298, 0.1315, 0.1145, 0.1238, 0.1258, 0.1258, 0.1210]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1325, 0.1338, 0.1190, 0.1271, 0.1290, 0.1168, 0.1272, 0.1144],
        [0.1184, 0.1185, 0.1222, 0.1247, 0.1285, 0.1293, 0.1294, 0.1291]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1357, 0.1346, 0.1162, 0.1178, 0.1224, 0.1253, 0.1253, 0.1226],
        [0.1156, 0.1163, 0.1308, 0.1267, 0.1232, 0.1327, 0.1199, 0.1348],
        [0.1218, 0.1217, 0.1238, 0.1244, 0.1244, 0.1296, 0.1279, 0.1264]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1299, 0.1293, 0.1185, 0.1243, 0.1270, 0.1203, 0.1189, 0.1320],
        [0.1205, 0.1206, 0.1244, 0.1291, 0.1289, 0.1225, 0.1254, 0.1286],
        [0.1175, 0.1199, 0.1203, 0.1288, 0.1255, 0.1306, 0.1324, 0.1250],
        [0.1225, 0.1231, 0.1246, 0.1236, 0.1257, 0.1204, 0.1305, 0.1296]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1408, 0.1397, 0.1186, 0.1203, 0.1170, 0.1187, 0.1249, 0.1199],
        [0.1225, 0.1211, 0.1292, 0.1238, 0.1203, 0.1268, 0.1264, 0.1299],
        [0.1332, 0.1376, 0.1318, 0.1165, 0.1257, 0.1162, 0.1211, 0.1180],
        [0.1301, 0.1322, 0.1308, 0.1231, 0.1184, 0.1246, 0.1212, 0.1195],
        [0.1306, 0.1314, 0.1285, 0.1185, 0.1259, 0.1242, 0.1211, 0.1196]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/23 10:39:37 AM | size of train loader is 391
07/23 10:39:42 AM | Train: [ 5/50] Step 000/390 Loss 0.011 Prec@(1,5) (1.6%, 17.2%)
07/23 10:43:18 AM | Train: [ 5/50] Step 050/390 Loss 0.011 Prec@(1,5) (7.0%, 20.6%)
07/23 10:47:06 AM | Train: [ 5/50] Step 100/390 Loss 0.011 Prec@(1,5) (6.9%, 20.6%)
07/23 10:50:47 AM | Train: [ 5/50] Step 150/390 Loss 0.011 Prec@(1,5) (6.9%, 20.7%)
07/23 10:54:20 AM | Train: [ 5/50] Step 200/390 Loss 0.011 Prec@(1,5) (6.8%, 20.4%)
07/23 10:58:07 AM | Train: [ 5/50] Step 250/390 Loss 0.011 Prec@(1,5) (6.7%, 20.6%)
07/23 11:01:40 AM | Train: [ 5/50] Step 300/390 Loss 0.011 Prec@(1,5) (6.7%, 20.9%)
07/23 11:05:15 AM | Train: [ 5/50] Step 350/390 Loss 0.011 Prec@(1,5) (6.8%, 21.0%)
07/23 11:07:59 AM | Train: [ 5/50] Step 390/390 Loss 0.011 Prec@(1,5) (6.8%, 21.0%)
07/23 11:07:59 AM | Train: [ 5/50] Final Prec@1 6.7880%
07/23 11:08:00 AM | Valid: [ 5/50] Step 000/390 Loss 4.237 Prec@(1,5) (6.2%, 21.9%)
07/23 11:08:13 AM | Valid: [ 5/50] Step 050/390 Loss 4.232 Prec@(1,5) (6.6%, 22.1%)
07/23 11:08:27 AM | Valid: [ 5/50] Step 100/390 Loss 4.229 Prec@(1,5) (6.7%, 21.9%)
07/23 11:08:41 AM | Valid: [ 5/50] Step 150/390 Loss 4.228 Prec@(1,5) (6.9%, 22.1%)
07/23 11:08:53 AM | Valid: [ 5/50] Step 200/390 Loss 4.224 Prec@(1,5) (7.0%, 22.0%)
07/23 11:09:03 AM | Valid: [ 5/50] Step 250/390 Loss 4.222 Prec@(1,5) (7.1%, 22.0%)
07/23 11:09:15 AM | Valid: [ 5/50] Step 300/390 Loss 4.224 Prec@(1,5) (7.1%, 22.0%)
07/23 11:09:26 AM | Valid: [ 5/50] Step 350/390 Loss 4.225 Prec@(1,5) (7.1%, 21.9%)
07/23 11:09:36 AM | Valid: [ 5/50] Step 390/390 Loss 4.225 Prec@(1,5) (7.1%, 22.0%)
07/23 11:09:36 AM | Valid: [ 5/50] Final Prec@1 7.0800%
07/23 11:09:36 AM | genotype = Genotype(normal=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('skip_connect', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 1)], [('skip_connect', 3), ('avg_pool_3x3', 0)]], normal_concat=range(2, 6), reduce=[[('avg_pool_3x3', 0), ('dil_conv_5x5', 1)], [('max_pool_3x3', 0), ('dil_conv_3x3', 1)], [('max_pool_3x3', 0), ('sep_conv_5x5', 1)], [('max_pool_3x3', 0), ('avg_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1264, 0.1304, 0.1288, 0.1205, 0.1230, 0.1266, 0.1228, 0.1215],
        [0.1295, 0.1350, 0.1341, 0.1144, 0.1229, 0.1266, 0.1205, 0.1170]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1245, 0.1322, 0.1274, 0.1234, 0.1303, 0.1220, 0.1250, 0.1153],
        [0.1220, 0.1314, 0.1326, 0.1286, 0.1209, 0.1214, 0.1229, 0.1202],
        [0.1269, 0.1322, 0.1330, 0.1227, 0.1198, 0.1199, 0.1234, 0.1222]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1300, 0.1301, 0.1296, 0.1151, 0.1227, 0.1232, 0.1269, 0.1224],
        [0.1289, 0.1316, 0.1315, 0.1151, 0.1225, 0.1274, 0.1233, 0.1197],
        [0.1289, 0.1302, 0.1283, 0.1302, 0.1208, 0.1198, 0.1206, 0.1211],
        [0.1320, 0.1339, 0.1327, 0.1207, 0.1242, 0.1226, 0.1191, 0.1148]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1336, 0.1383, 0.1366, 0.1198, 0.1166, 0.1149, 0.1217, 0.1185],
        [0.1309, 0.1349, 0.1353, 0.1187, 0.1229, 0.1207, 0.1156, 0.1210],
        [0.1292, 0.1308, 0.1308, 0.1260, 0.1248, 0.1186, 0.1255, 0.1144],
        [0.1337, 0.1363, 0.1385, 0.1245, 0.1173, 0.1183, 0.1156, 0.1158],
        [0.1317, 0.1338, 0.1362, 0.1121, 0.1218, 0.1237, 0.1247, 0.1159]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1351, 0.1352, 0.1183, 0.1275, 0.1307, 0.1135, 0.1266, 0.1131],
        [0.1189, 0.1187, 0.1199, 0.1247, 0.1278, 0.1301, 0.1304, 0.1294]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1374, 0.1354, 0.1160, 0.1167, 0.1224, 0.1243, 0.1245, 0.1233],
        [0.1160, 0.1160, 0.1303, 0.1259, 0.1216, 0.1340, 0.1203, 0.1358],
        [0.1225, 0.1214, 0.1236, 0.1246, 0.1237, 0.1295, 0.1281, 0.1267]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1318, 0.1298, 0.1176, 0.1233, 0.1271, 0.1201, 0.1172, 0.1330],
        [0.1191, 0.1191, 0.1264, 0.1300, 0.1312, 0.1219, 0.1247, 0.1277],
        [0.1175, 0.1186, 0.1192, 0.1315, 0.1248, 0.1313, 0.1322, 0.1249],
        [0.1233, 0.1228, 0.1251, 0.1229, 0.1263, 0.1184, 0.1303, 0.1309]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1448, 0.1425, 0.1174, 0.1194, 0.1162, 0.1161, 0.1244, 0.1192],
        [0.1218, 0.1198, 0.1295, 0.1245, 0.1205, 0.1276, 0.1261, 0.1301],
        [0.1351, 0.1394, 0.1324, 0.1150, 0.1258, 0.1149, 0.1200, 0.1174],
        [0.1319, 0.1328, 0.1306, 0.1230, 0.1183, 0.1248, 0.1196, 0.1190],
        [0.1322, 0.1324, 0.1289, 0.1178, 0.1262, 0.1237, 0.1198, 0.1190]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/23 11:09:37 AM | size of train loader is 391
07/23 11:09:42 AM | Train: [ 6/50] Step 000/390 Loss 0.010 Prec@(1,5) (4.7%, 26.6%)
07/23 11:13:20 AM | Train: [ 6/50] Step 050/390 Loss 0.011 Prec@(1,5) (7.4%, 22.7%)
07/23 11:16:52 AM | Train: [ 6/50] Step 100/390 Loss 0.011 Prec@(1,5) (7.3%, 22.5%)
07/23 11:20:25 AM | Train: [ 6/50] Step 150/390 Loss 0.011 Prec@(1,5) (7.5%, 22.6%)
07/23 11:23:57 AM | Train: [ 6/50] Step 200/390 Loss 0.011 Prec@(1,5) (7.6%, 22.8%)
07/23 11:27:34 AM | Train: [ 6/50] Step 250/390 Loss 0.011 Prec@(1,5) (7.8%, 23.0%)
07/23 11:31:07 AM | Train: [ 6/50] Step 300/390 Loss 0.011 Prec@(1,5) (7.6%, 22.9%)
07/23 11:34:50 AM | Train: [ 6/50] Step 350/390 Loss 0.011 Prec@(1,5) (7.5%, 23.0%)
07/23 11:37:47 AM | Train: [ 6/50] Step 390/390 Loss 0.011 Prec@(1,5) (7.6%, 23.1%)
07/23 11:37:47 AM | Train: [ 6/50] Final Prec@1 7.5640%
07/23 11:37:48 AM | Valid: [ 6/50] Step 000/390 Loss 4.296 Prec@(1,5) (9.4%, 20.3%)
07/23 11:37:59 AM | Valid: [ 6/50] Step 050/390 Loss 4.155 Prec@(1,5) (8.1%, 24.4%)
07/23 11:38:10 AM | Valid: [ 6/50] Step 100/390 Loss 4.150 Prec@(1,5) (8.1%, 25.0%)
07/23 11:38:20 AM | Valid: [ 6/50] Step 150/390 Loss 4.156 Prec@(1,5) (8.1%, 24.3%)
07/23 11:38:31 AM | Valid: [ 6/50] Step 200/390 Loss 4.159 Prec@(1,5) (8.0%, 24.2%)
07/23 11:38:42 AM | Valid: [ 6/50] Step 250/390 Loss 4.161 Prec@(1,5) (8.1%, 24.4%)
07/23 11:38:53 AM | Valid: [ 6/50] Step 300/390 Loss 4.160 Prec@(1,5) (8.1%, 24.5%)
07/23 11:39:05 AM | Valid: [ 6/50] Step 350/390 Loss 4.159 Prec@(1,5) (8.1%, 24.5%)
07/23 11:39:13 AM | Valid: [ 6/50] Step 390/390 Loss 4.161 Prec@(1,5) (8.0%, 24.5%)
07/23 11:39:13 AM | Valid: [ 6/50] Final Prec@1 8.0440%
07/23 11:39:13 AM | genotype = Genotype(normal=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('skip_connect', 1), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 0), ('skip_connect', 3)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_5x5', 1)], [('max_pool_3x3', 0), ('dil_conv_3x3', 1)], [('max_pool_3x3', 0), ('sep_conv_5x5', 1)], [('max_pool_3x3', 0), ('avg_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1297, 0.1340, 0.1316, 0.1199, 0.1211, 0.1235, 0.1212, 0.1192],
        [0.1325, 0.1387, 0.1372, 0.1112, 0.1220, 0.1256, 0.1179, 0.1148]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1264, 0.1356, 0.1294, 0.1224, 0.1302, 0.1198, 0.1244, 0.1119],
        [0.1231, 0.1343, 0.1358, 0.1267, 0.1211, 0.1198, 0.1215, 0.1177],
        [0.1285, 0.1348, 0.1348, 0.1215, 0.1181, 0.1195, 0.1230, 0.1197]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1323, 0.1330, 0.1321, 0.1133, 0.1218, 0.1218, 0.1262, 0.1195],
        [0.1320, 0.1355, 0.1347, 0.1118, 0.1215, 0.1260, 0.1225, 0.1160],
        [0.1321, 0.1346, 0.1312, 0.1295, 0.1196, 0.1175, 0.1177, 0.1179],
        [0.1371, 0.1394, 0.1376, 0.1182, 0.1228, 0.1192, 0.1161, 0.1095]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1376, 0.1433, 0.1415, 0.1169, 0.1130, 0.1118, 0.1210, 0.1149],
        [0.1335, 0.1380, 0.1386, 0.1186, 0.1209, 0.1198, 0.1128, 0.1177],
        [0.1323, 0.1343, 0.1322, 0.1253, 0.1224, 0.1189, 0.1253, 0.1093],
        [0.1381, 0.1410, 0.1424, 0.1239, 0.1159, 0.1153, 0.1121, 0.1113],
        [0.1357, 0.1383, 0.1418, 0.1098, 0.1193, 0.1215, 0.1222, 0.1114]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1380, 0.1373, 0.1179, 0.1266, 0.1314, 0.1114, 0.1261, 0.1113],
        [0.1192, 0.1188, 0.1171, 0.1241, 0.1277, 0.1293, 0.1331, 0.1308]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1398, 0.1368, 0.1157, 0.1152, 0.1216, 0.1236, 0.1243, 0.1231],
        [0.1163, 0.1159, 0.1314, 0.1251, 0.1201, 0.1340, 0.1193, 0.1379],
        [0.1227, 0.1215, 0.1233, 0.1237, 0.1248, 0.1287, 0.1291, 0.1262]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1350, 0.1317, 0.1157, 0.1225, 0.1259, 0.1193, 0.1165, 0.1334],
        [0.1180, 0.1181, 0.1278, 0.1322, 0.1327, 0.1202, 0.1246, 0.1263],
        [0.1175, 0.1179, 0.1178, 0.1324, 0.1249, 0.1329, 0.1334, 0.1233],
        [0.1249, 0.1235, 0.1264, 0.1208, 0.1264, 0.1164, 0.1295, 0.1321]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1476, 0.1443, 0.1178, 0.1191, 0.1143, 0.1138, 0.1243, 0.1187],
        [0.1217, 0.1197, 0.1292, 0.1224, 0.1190, 0.1295, 0.1278, 0.1307],
        [0.1362, 0.1418, 0.1339, 0.1127, 0.1248, 0.1144, 0.1195, 0.1167],
        [0.1316, 0.1322, 0.1305, 0.1231, 0.1176, 0.1249, 0.1202, 0.1198],
        [0.1322, 0.1315, 0.1279, 0.1180, 0.1283, 0.1233, 0.1194, 0.1195]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/23 11:39:14 AM | size of train loader is 391
07/23 11:39:19 AM | Train: [ 7/50] Step 000/390 Loss 0.011 Prec@(1,5) (1.6%, 17.2%)
07/23 11:42:57 AM | Train: [ 7/50] Step 050/390 Loss 0.011 Prec@(1,5) (8.6%, 25.0%)
07/23 11:46:27 AM | Train: [ 7/50] Step 100/390 Loss 0.011 Prec@(1,5) (8.6%, 25.3%)
07/23 11:49:55 AM | Train: [ 7/50] Step 150/390 Loss 0.011 Prec@(1,5) (8.4%, 25.2%)
07/23 11:53:27 AM | Train: [ 7/50] Step 200/390 Loss 0.011 Prec@(1,5) (8.5%, 25.2%)
07/23 11:56:51 AM | Train: [ 7/50] Step 250/390 Loss 0.011 Prec@(1,5) (8.4%, 25.3%)
07/23 12:00:20 PM | Train: [ 7/50] Step 300/390 Loss 0.011 Prec@(1,5) (8.4%, 25.3%)
07/23 12:04:04 PM | Train: [ 7/50] Step 350/390 Loss 0.011 Prec@(1,5) (8.4%, 25.5%)
07/23 12:07:01 PM | Train: [ 7/50] Step 390/390 Loss 0.011 Prec@(1,5) (8.4%, 25.6%)
07/23 12:07:01 PM | Train: [ 7/50] Final Prec@1 8.4240%
07/23 12:07:01 PM | Valid: [ 7/50] Step 000/390 Loss 4.041 Prec@(1,5) (9.4%, 29.7%)
07/23 12:07:12 PM | Valid: [ 7/50] Step 050/390 Loss 4.070 Prec@(1,5) (9.7%, 27.2%)
07/23 12:07:24 PM | Valid: [ 7/50] Step 100/390 Loss 4.082 Prec@(1,5) (8.9%, 26.8%)
07/23 12:07:35 PM | Valid: [ 7/50] Step 150/390 Loss 4.092 Prec@(1,5) (8.7%, 26.4%)
07/23 12:07:46 PM | Valid: [ 7/50] Step 200/390 Loss 4.092 Prec@(1,5) (8.6%, 26.4%)
07/23 12:07:57 PM | Valid: [ 7/50] Step 250/390 Loss 4.093 Prec@(1,5) (8.6%, 26.5%)
07/23 12:08:07 PM | Valid: [ 7/50] Step 300/390 Loss 4.097 Prec@(1,5) (8.5%, 26.3%)
07/23 12:08:18 PM | Valid: [ 7/50] Step 350/390 Loss 4.099 Prec@(1,5) (8.6%, 26.3%)
07/23 12:08:28 PM | Valid: [ 7/50] Step 390/390 Loss 4.101 Prec@(1,5) (8.5%, 26.1%)
07/23 12:08:28 PM | Valid: [ 7/50] Final Prec@1 8.5480%
07/23 12:08:28 PM | genotype = Genotype(normal=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 0), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('skip_connect', 4), ('avg_pool_3x3', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_5x5', 1)], [('max_pool_3x3', 0), ('dil_conv_3x3', 1)], [('max_pool_3x3', 0), ('sep_conv_5x5', 1)], [('max_pool_3x3', 0), ('avg_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1342, 0.1370, 0.1338, 0.1176, 0.1190, 0.1217, 0.1205, 0.1161],
        [0.1349, 0.1401, 0.1384, 0.1098, 0.1211, 0.1247, 0.1166, 0.1144]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1307, 0.1401, 0.1328, 0.1203, 0.1300, 0.1172, 0.1217, 0.1072],
        [0.1238, 0.1355, 0.1373, 0.1267, 0.1218, 0.1175, 0.1207, 0.1167],
        [0.1312, 0.1375, 0.1359, 0.1213, 0.1163, 0.1189, 0.1232, 0.1157]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1373, 0.1369, 0.1357, 0.1116, 0.1191, 0.1193, 0.1245, 0.1156],
        [0.1344, 0.1374, 0.1360, 0.1106, 0.1216, 0.1243, 0.1216, 0.1141],
        [0.1358, 0.1380, 0.1330, 0.1298, 0.1193, 0.1154, 0.1152, 0.1134],
        [0.1431, 0.1451, 0.1431, 0.1160, 0.1201, 0.1154, 0.1128, 0.1044]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1424, 0.1487, 0.1459, 0.1140, 0.1097, 0.1082, 0.1207, 0.1104],
        [0.1368, 0.1409, 0.1413, 0.1195, 0.1188, 0.1179, 0.1104, 0.1145],
        [0.1362, 0.1382, 0.1342, 0.1232, 0.1212, 0.1182, 0.1244, 0.1044],
        [0.1435, 0.1468, 0.1474, 0.1223, 0.1120, 0.1135, 0.1085, 0.1061],
        [0.1413, 0.1437, 0.1490, 0.1054, 0.1160, 0.1191, 0.1194, 0.1061]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1405, 0.1379, 0.1180, 0.1278, 0.1312, 0.1093, 0.1253, 0.1101],
        [0.1188, 0.1179, 0.1154, 0.1245, 0.1282, 0.1296, 0.1342, 0.1314]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1429, 0.1378, 0.1160, 0.1152, 0.1212, 0.1214, 0.1241, 0.1214],
        [0.1156, 0.1147, 0.1305, 0.1245, 0.1209, 0.1352, 0.1194, 0.1393],
        [0.1243, 0.1229, 0.1248, 0.1215, 0.1227, 0.1287, 0.1285, 0.1266]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1380, 0.1326, 0.1147, 0.1225, 0.1247, 0.1189, 0.1152, 0.1335],
        [0.1165, 0.1166, 0.1285, 0.1331, 0.1354, 0.1191, 0.1249, 0.1260],
        [0.1163, 0.1164, 0.1156, 0.1336, 0.1253, 0.1343, 0.1359, 0.1226],
        [0.1265, 0.1242, 0.1276, 0.1201, 0.1249, 0.1143, 0.1282, 0.1341]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1507, 0.1456, 0.1178, 0.1193, 0.1136, 0.1122, 0.1225, 0.1184],
        [0.1208, 0.1183, 0.1299, 0.1205, 0.1203, 0.1302, 0.1292, 0.1308],
        [0.1378, 0.1434, 0.1351, 0.1100, 0.1249, 0.1135, 0.1191, 0.1162],
        [0.1326, 0.1330, 0.1315, 0.1226, 0.1166, 0.1242, 0.1198, 0.1196],
        [0.1327, 0.1314, 0.1275, 0.1179, 0.1288, 0.1224, 0.1195, 0.1198]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/23 12:08:29 PM | size of train loader is 391
07/23 12:08:35 PM | Train: [ 8/50] Step 000/390 Loss 0.010 Prec@(1,5) (7.8%, 25.0%)
07/23 12:12:15 PM | Train: [ 8/50] Step 050/390 Loss 0.010 Prec@(1,5) (9.7%, 26.8%)
07/23 12:15:53 PM | Train: [ 8/50] Step 100/390 Loss 0.010 Prec@(1,5) (9.5%, 26.5%)
07/23 12:19:39 PM | Train: [ 8/50] Step 150/390 Loss 0.010 Prec@(1,5) (9.6%, 26.5%)
07/23 12:23:16 PM | Train: [ 8/50] Step 200/390 Loss 0.010 Prec@(1,5) (9.4%, 26.4%)
07/23 12:26:55 PM | Train: [ 8/50] Step 250/390 Loss 0.010 Prec@(1,5) (9.3%, 26.7%)
07/23 12:30:41 PM | Train: [ 8/50] Step 300/390 Loss 0.011 Prec@(1,5) (9.3%, 26.8%)
07/23 12:34:15 PM | Train: [ 8/50] Step 350/390 Loss 0.011 Prec@(1,5) (9.5%, 27.0%)
07/23 12:37:06 PM | Train: [ 8/50] Step 390/390 Loss 0.011 Prec@(1,5) (9.5%, 27.1%)
07/23 12:37:06 PM | Train: [ 8/50] Final Prec@1 9.4920%
07/23 12:37:06 PM | Valid: [ 8/50] Step 000/390 Loss 4.107 Prec@(1,5) (7.8%, 17.2%)
07/23 12:37:18 PM | Valid: [ 8/50] Step 050/390 Loss 4.054 Prec@(1,5) (9.8%, 27.7%)
07/23 12:37:32 PM | Valid: [ 8/50] Step 100/390 Loss 4.043 Prec@(1,5) (9.9%, 28.3%)
07/23 12:37:45 PM | Valid: [ 8/50] Step 150/390 Loss 4.046 Prec@(1,5) (9.8%, 28.3%)
07/23 12:37:59 PM | Valid: [ 8/50] Step 200/390 Loss 4.050 Prec@(1,5) (9.7%, 28.0%)
07/23 12:38:11 PM | Valid: [ 8/50] Step 250/390 Loss 4.046 Prec@(1,5) (9.6%, 28.1%)
07/23 12:38:21 PM | Valid: [ 8/50] Step 300/390 Loss 4.040 Prec@(1,5) (9.8%, 28.4%)
07/23 12:38:33 PM | Valid: [ 8/50] Step 350/390 Loss 4.042 Prec@(1,5) (9.8%, 28.2%)
07/23 12:38:42 PM | Valid: [ 8/50] Step 390/390 Loss 4.045 Prec@(1,5) (9.7%, 28.2%)
07/23 12:38:42 PM | Valid: [ 8/50] Final Prec@1 9.6960%
07/23 12:38:42 PM | genotype = Genotype(normal=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 0), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 3), ('max_pool_3x3', 0)], [('skip_connect', 4), ('skip_connect', 3)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_5x5', 1)], [('max_pool_3x3', 0), ('dil_conv_3x3', 1)], [('max_pool_3x3', 0), ('sep_conv_5x5', 1)], [('max_pool_3x3', 0), ('avg_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1393, 0.1396, 0.1366, 0.1138, 0.1184, 0.1197, 0.1188, 0.1138],
        [0.1386, 0.1416, 0.1402, 0.1083, 0.1214, 0.1219, 0.1155, 0.1124]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1354, 0.1439, 0.1348, 0.1186, 0.1290, 0.1157, 0.1195, 0.1030],
        [0.1255, 0.1370, 0.1390, 0.1246, 0.1215, 0.1167, 0.1205, 0.1151],
        [0.1346, 0.1401, 0.1374, 0.1209, 0.1152, 0.1183, 0.1218, 0.1117]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1425, 0.1407, 0.1393, 0.1094, 0.1179, 0.1162, 0.1230, 0.1110],
        [0.1373, 0.1393, 0.1380, 0.1082, 0.1223, 0.1234, 0.1209, 0.1107],
        [0.1397, 0.1414, 0.1356, 0.1286, 0.1175, 0.1140, 0.1135, 0.1099],
        [0.1487, 0.1498, 0.1483, 0.1136, 0.1183, 0.1117, 0.1104, 0.0993]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1470, 0.1513, 0.1488, 0.1131, 0.1065, 0.1060, 0.1201, 0.1072],
        [0.1399, 0.1430, 0.1442, 0.1198, 0.1169, 0.1163, 0.1078, 0.1120],
        [0.1392, 0.1400, 0.1355, 0.1217, 0.1203, 0.1179, 0.1240, 0.1013],
        [0.1482, 0.1502, 0.1516, 0.1211, 0.1089, 0.1131, 0.1053, 0.1016],
        [0.1456, 0.1473, 0.1550, 0.1034, 0.1127, 0.1172, 0.1163, 0.1026]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1438, 0.1381, 0.1188, 0.1290, 0.1310, 0.1071, 0.1236, 0.1086],
        [0.1186, 0.1165, 0.1143, 0.1250, 0.1276, 0.1304, 0.1353, 0.1324]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1459, 0.1383, 0.1163, 0.1154, 0.1204, 0.1203, 0.1226, 0.1208],
        [0.1146, 0.1125, 0.1315, 0.1241, 0.1214, 0.1357, 0.1186, 0.1416],
        [0.1253, 0.1225, 0.1232, 0.1202, 0.1232, 0.1309, 0.1290, 0.1257]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1419, 0.1336, 0.1136, 0.1220, 0.1247, 0.1180, 0.1131, 0.1331],
        [0.1155, 0.1147, 0.1292, 0.1334, 0.1369, 0.1183, 0.1252, 0.1267],
        [0.1167, 0.1155, 0.1147, 0.1334, 0.1251, 0.1356, 0.1368, 0.1222],
        [0.1292, 0.1251, 0.1291, 0.1179, 0.1240, 0.1120, 0.1280, 0.1347]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1531, 0.1463, 0.1184, 0.1192, 0.1119, 0.1107, 0.1228, 0.1175],
        [0.1196, 0.1171, 0.1316, 0.1210, 0.1189, 0.1302, 0.1314, 0.1302],
        [0.1391, 0.1439, 0.1349, 0.1073, 0.1244, 0.1146, 0.1186, 0.1172],
        [0.1338, 0.1328, 0.1304, 0.1242, 0.1162, 0.1245, 0.1197, 0.1185],
        [0.1342, 0.1315, 0.1274, 0.1184, 0.1281, 0.1212, 0.1179, 0.1214]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/23 12:38:43 PM | size of train loader is 391
07/23 12:38:47 PM | Train: [ 9/50] Step 000/390 Loss 0.010 Prec@(1,5) (9.4%, 28.1%)
07/23 12:42:18 PM | Train: [ 9/50] Step 050/390 Loss 0.010 Prec@(1,5) (9.1%, 28.3%)
07/23 12:45:49 PM | Train: [ 9/50] Step 100/390 Loss 0.010 Prec@(1,5) (9.2%, 28.4%)
07/23 12:49:18 PM | Train: [ 9/50] Step 150/390 Loss 0.010 Prec@(1,5) (9.4%, 28.5%)
07/23 12:52:36 PM | Train: [ 9/50] Step 200/390 Loss 0.010 Prec@(1,5) (9.6%, 28.6%)
07/23 12:55:42 PM | Train: [ 9/50] Step 250/390 Loss 0.010 Prec@(1,5) (9.5%, 28.7%)
07/23 12:58:53 PM | Train: [ 9/50] Step 300/390 Loss 0.011 Prec@(1,5) (9.6%, 28.8%)
07/23 01:02:17 PM | Train: [ 9/50] Step 350/390 Loss 0.011 Prec@(1,5) (9.8%, 28.8%)
07/23 01:05:01 PM | Train: [ 9/50] Step 390/390 Loss 0.011 Prec@(1,5) (9.7%, 28.7%)
07/23 01:05:01 PM | Train: [ 9/50] Final Prec@1 9.7080%
07/23 01:05:01 PM | Valid: [ 9/50] Step 000/390 Loss 4.084 Prec@(1,5) (12.5%, 32.8%)
07/23 01:05:12 PM | Valid: [ 9/50] Step 050/390 Loss 3.969 Prec@(1,5) (11.0%, 30.1%)
07/23 01:05:23 PM | Valid: [ 9/50] Step 100/390 Loss 3.974 Prec@(1,5) (10.5%, 30.0%)
07/23 01:05:37 PM | Valid: [ 9/50] Step 150/390 Loss 3.982 Prec@(1,5) (10.6%, 29.9%)
07/23 01:05:49 PM | Valid: [ 9/50] Step 200/390 Loss 3.988 Prec@(1,5) (10.3%, 29.9%)
07/23 01:06:02 PM | Valid: [ 9/50] Step 250/390 Loss 3.986 Prec@(1,5) (10.5%, 30.1%)
07/23 01:06:13 PM | Valid: [ 9/50] Step 300/390 Loss 3.987 Prec@(1,5) (10.4%, 30.0%)
07/23 01:06:23 PM | Valid: [ 9/50] Step 350/390 Loss 3.986 Prec@(1,5) (10.4%, 30.1%)
07/23 01:06:31 PM | Valid: [ 9/50] Step 390/390 Loss 3.987 Prec@(1,5) (10.5%, 30.0%)
07/23 01:06:31 PM | Valid: [ 9/50] Final Prec@1 10.4560%
07/23 01:06:31 PM | genotype = Genotype(normal=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('avg_pool_3x3', 0), ('avg_pool_3x3', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 0)], [('skip_connect', 4), ('skip_connect', 3)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_5x5', 1)], [('max_pool_3x3', 0), ('dil_conv_3x3', 1)], [('max_pool_3x3', 0), ('sep_conv_5x5', 1)], [('max_pool_3x3', 0), ('avg_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1452, 0.1412, 0.1386, 0.1104, 0.1186, 0.1173, 0.1175, 0.1112],
        [0.1434, 0.1427, 0.1415, 0.1068, 0.1210, 0.1198, 0.1149, 0.1101]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1406, 0.1482, 0.1380, 0.1165, 0.1284, 0.1137, 0.1163, 0.0985],
        [0.1273, 0.1378, 0.1408, 0.1248, 0.1216, 0.1147, 0.1196, 0.1133],
        [0.1388, 0.1429, 0.1403, 0.1184, 0.1131, 0.1182, 0.1201, 0.1082]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1471, 0.1435, 0.1415, 0.1072, 0.1177, 0.1137, 0.1208, 0.1086],
        [0.1409, 0.1418, 0.1408, 0.1059, 0.1217, 0.1212, 0.1196, 0.1081],
        [0.1427, 0.1433, 0.1364, 0.1293, 0.1171, 0.1125, 0.1126, 0.1062],
        [0.1541, 0.1533, 0.1525, 0.1105, 0.1165, 0.1094, 0.1083, 0.0953]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1513, 0.1543, 0.1513, 0.1112, 0.1043, 0.1041, 0.1196, 0.1038],
        [0.1430, 0.1437, 0.1457, 0.1205, 0.1151, 0.1152, 0.1067, 0.1101],
        [0.1413, 0.1407, 0.1365, 0.1205, 0.1199, 0.1178, 0.1245, 0.0988],
        [0.1522, 0.1524, 0.1550, 0.1202, 0.1068, 0.1119, 0.1033, 0.0983],
        [0.1495, 0.1494, 0.1600, 0.1014, 0.1106, 0.1158, 0.1144, 0.0988]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1457, 0.1379, 0.1203, 0.1300, 0.1305, 0.1055, 0.1226, 0.1074],
        [0.1186, 0.1152, 0.1143, 0.1245, 0.1261, 0.1311, 0.1364, 0.1338]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1488, 0.1391, 0.1164, 0.1154, 0.1204, 0.1191, 0.1209, 0.1198],
        [0.1133, 0.1100, 0.1323, 0.1244, 0.1222, 0.1343, 0.1196, 0.1437],
        [0.1266, 0.1222, 0.1226, 0.1193, 0.1235, 0.1309, 0.1295, 0.1254]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1453, 0.1337, 0.1142, 0.1215, 0.1234, 0.1171, 0.1124, 0.1324],
        [0.1149, 0.1131, 0.1307, 0.1330, 0.1384, 0.1187, 0.1249, 0.1263],
        [0.1164, 0.1137, 0.1126, 0.1344, 0.1264, 0.1357, 0.1397, 0.1211],
        [0.1311, 0.1257, 0.1303, 0.1165, 0.1229, 0.1110, 0.1278, 0.1347]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1549, 0.1468, 0.1196, 0.1192, 0.1112, 0.1096, 0.1221, 0.1167],
        [0.1169, 0.1141, 0.1338, 0.1203, 0.1185, 0.1311, 0.1344, 0.1309],
        [0.1392, 0.1421, 0.1328, 0.1069, 0.1252, 0.1159, 0.1194, 0.1184],
        [0.1342, 0.1307, 0.1278, 0.1265, 0.1156, 0.1249, 0.1212, 0.1191],
        [0.1354, 0.1306, 0.1267, 0.1199, 0.1280, 0.1200, 0.1165, 0.1230]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/23 01:06:32 PM | size of train loader is 391
07/23 01:06:36 PM | Train: [10/50] Step 000/390 Loss 0.009 Prec@(1,5) (17.2%, 35.9%)
07/23 01:09:49 PM | Train: [10/50] Step 050/390 Loss 0.010 Prec@(1,5) (10.1%, 30.6%)
07/23 01:12:53 PM | Train: [10/50] Step 100/390 Loss 0.010 Prec@(1,5) (10.3%, 29.9%)
07/23 01:16:00 PM | Train: [10/50] Step 150/390 Loss 0.010 Prec@(1,5) (10.1%, 29.5%)
07/23 01:19:02 PM | Train: [10/50] Step 200/390 Loss 0.010 Prec@(1,5) (10.3%, 29.9%)
07/23 01:22:10 PM | Train: [10/50] Step 250/390 Loss 0.010 Prec@(1,5) (10.5%, 30.1%)
07/23 01:25:21 PM | Train: [10/50] Step 300/390 Loss 0.011 Prec@(1,5) (10.6%, 30.1%)
07/23 01:28:26 PM | Train: [10/50] Step 350/390 Loss 0.011 Prec@(1,5) (10.6%, 30.1%)
07/23 01:30:54 PM | Train: [10/50] Step 390/390 Loss 0.011 Prec@(1,5) (10.6%, 30.2%)
07/23 01:30:54 PM | Train: [10/50] Final Prec@1 10.6000%
07/23 01:30:55 PM | Valid: [10/50] Step 000/390 Loss 4.019 Prec@(1,5) (1.6%, 20.3%)
07/23 01:31:04 PM | Valid: [10/50] Step 050/390 Loss 3.937 Prec@(1,5) (11.3%, 31.1%)
07/23 01:31:14 PM | Valid: [10/50] Step 100/390 Loss 3.949 Prec@(1,5) (11.0%, 30.8%)
07/23 01:31:24 PM | Valid: [10/50] Step 150/390 Loss 3.941 Prec@(1,5) (11.0%, 31.2%)
07/23 01:31:33 PM | Valid: [10/50] Step 200/390 Loss 3.939 Prec@(1,5) (11.1%, 31.5%)
07/23 01:31:43 PM | Valid: [10/50] Step 250/390 Loss 3.938 Prec@(1,5) (11.0%, 31.3%)
07/23 01:31:54 PM | Valid: [10/50] Step 300/390 Loss 3.940 Prec@(1,5) (10.9%, 31.2%)
07/23 01:32:04 PM | Valid: [10/50] Step 350/390 Loss 3.942 Prec@(1,5) (10.8%, 31.2%)
07/23 01:32:12 PM | Valid: [10/50] Step 390/390 Loss 3.941 Prec@(1,5) (10.9%, 31.3%)
07/23 01:32:13 PM | Valid: [10/50] Final Prec@1 10.9320%
07/23 01:32:13 PM | genotype = Genotype(normal=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('avg_pool_3x3', 0), ('avg_pool_3x3', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 0)], [('skip_connect', 4), ('skip_connect', 3)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_5x5', 1)], [('max_pool_3x3', 0), ('dil_conv_3x3', 1)], [('max_pool_3x3', 0), ('sep_conv_5x5', 1)], [('max_pool_3x3', 0), ('avg_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1517, 0.1427, 0.1406, 0.1067, 0.1172, 0.1157, 0.1170, 0.1084],
        [0.1461, 0.1416, 0.1400, 0.1070, 0.1217, 0.1183, 0.1160, 0.1094]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1451, 0.1508, 0.1399, 0.1171, 0.1272, 0.1110, 0.1146, 0.0942],
        [0.1280, 0.1375, 0.1409, 0.1247, 0.1218, 0.1139, 0.1205, 0.1127],
        [0.1419, 0.1445, 0.1417, 0.1167, 0.1124, 0.1191, 0.1191, 0.1047]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1519, 0.1449, 0.1430, 0.1060, 0.1164, 0.1120, 0.1198, 0.1060],
        [0.1416, 0.1410, 0.1406, 0.1059, 0.1233, 0.1202, 0.1203, 0.1071],
        [0.1442, 0.1436, 0.1362, 0.1296, 0.1176, 0.1118, 0.1122, 0.1049],
        [0.1589, 0.1556, 0.1561, 0.1089, 0.1133, 0.1083, 0.1062, 0.0928]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1551, 0.1561, 0.1539, 0.1095, 0.1020, 0.1033, 0.1198, 0.1005],
        [0.1460, 0.1442, 0.1470, 0.1226, 0.1134, 0.1142, 0.1042, 0.1084],
        [0.1447, 0.1424, 0.1383, 0.1170, 0.1190, 0.1174, 0.1248, 0.0963],
        [0.1562, 0.1545, 0.1587, 0.1197, 0.1039, 0.1117, 0.1001, 0.0952],
        [0.1538, 0.1515, 0.1647, 0.0990, 0.1087, 0.1140, 0.1126, 0.0957]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1504, 0.1388, 0.1221, 0.1319, 0.1288, 0.1027, 0.1199, 0.1055],
        [0.1172, 0.1126, 0.1134, 0.1248, 0.1259, 0.1330, 0.1369, 0.1361]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1543, 0.1414, 0.1167, 0.1148, 0.1207, 0.1163, 0.1188, 0.1170],
        [0.1118, 0.1071, 0.1315, 0.1255, 0.1238, 0.1337, 0.1201, 0.1465],
        [0.1303, 0.1223, 0.1230, 0.1171, 0.1227, 0.1301, 0.1289, 0.1257]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1500, 0.1354, 0.1124, 0.1213, 0.1228, 0.1170, 0.1098, 0.1312],
        [0.1136, 0.1104, 0.1312, 0.1332, 0.1413, 0.1195, 0.1243, 0.1264],
        [0.1179, 0.1123, 0.1116, 0.1330, 0.1274, 0.1355, 0.1408, 0.1213],
        [0.1343, 0.1272, 0.1327, 0.1134, 0.1219, 0.1097, 0.1271, 0.1336]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1593, 0.1485, 0.1206, 0.1186, 0.1091, 0.1083, 0.1216, 0.1140],
        [0.1152, 0.1121, 0.1353, 0.1205, 0.1179, 0.1306, 0.1348, 0.1336],
        [0.1417, 0.1419, 0.1317, 0.1059, 0.1241, 0.1164, 0.1197, 0.1186],
        [0.1349, 0.1299, 0.1266, 0.1295, 0.1146, 0.1234, 0.1220, 0.1191],
        [0.1368, 0.1306, 0.1273, 0.1202, 0.1264, 0.1188, 0.1153, 0.1247]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/23 01:32:14 PM | size of train loader is 391
07/23 01:32:18 PM | Train: [11/50] Step 000/390 Loss 0.010 Prec@(1,5) (12.5%, 23.4%)
07/23 01:35:23 PM | Train: [11/50] Step 050/390 Loss 0.009 Prec@(1,5) (10.6%, 29.8%)
07/23 01:38:24 PM | Train: [11/50] Step 100/390 Loss 0.009 Prec@(1,5) (10.5%, 30.0%)
07/23 01:41:24 PM | Train: [11/50] Step 150/390 Loss 0.010 Prec@(1,5) (10.8%, 30.3%)
07/23 01:44:30 PM | Train: [11/50] Step 200/390 Loss 0.010 Prec@(1,5) (10.9%, 30.6%)
07/23 01:47:39 PM | Train: [11/50] Step 250/390 Loss 0.010 Prec@(1,5) (10.8%, 31.0%)
07/23 01:50:41 PM | Train: [11/50] Step 300/390 Loss 0.010 Prec@(1,5) (10.8%, 31.1%)
07/23 01:53:48 PM | Train: [11/50] Step 350/390 Loss 0.011 Prec@(1,5) (11.0%, 31.4%)
07/23 01:56:29 PM | Train: [11/50] Step 390/390 Loss 0.011 Prec@(1,5) (10.9%, 31.4%)
07/23 01:56:29 PM | Train: [11/50] Final Prec@1 10.9160%
07/23 01:56:30 PM | Valid: [11/50] Step 000/390 Loss 3.915 Prec@(1,5) (10.9%, 35.9%)
07/23 01:56:41 PM | Valid: [11/50] Step 050/390 Loss 3.912 Prec@(1,5) (13.0%, 33.0%)
07/23 01:56:51 PM | Valid: [11/50] Step 100/390 Loss 3.914 Prec@(1,5) (12.2%, 32.4%)
07/23 01:57:00 PM | Valid: [11/50] Step 150/390 Loss 3.913 Prec@(1,5) (11.8%, 32.3%)
07/23 01:57:10 PM | Valid: [11/50] Step 200/390 Loss 3.924 Prec@(1,5) (11.6%, 31.9%)
07/23 01:57:20 PM | Valid: [11/50] Step 250/390 Loss 3.920 Prec@(1,5) (11.6%, 32.1%)
07/23 01:57:29 PM | Valid: [11/50] Step 300/390 Loss 3.923 Prec@(1,5) (11.4%, 31.9%)
07/23 01:57:39 PM | Valid: [11/50] Step 350/390 Loss 3.919 Prec@(1,5) (11.5%, 32.1%)
07/23 01:57:47 PM | Valid: [11/50] Step 390/390 Loss 3.917 Prec@(1,5) (11.5%, 32.1%)
07/23 01:57:47 PM | Valid: [11/50] Final Prec@1 11.5320%
07/23 01:57:47 PM | genotype = Genotype(normal=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('avg_pool_3x3', 0), ('avg_pool_3x3', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 0)], [('skip_connect', 4), ('skip_connect', 3)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_5x5', 1)], [('max_pool_3x3', 0), ('dil_conv_3x3', 1)], [('max_pool_3x3', 0), ('sep_conv_5x5', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1579, 0.1444, 0.1422, 0.1041, 0.1159, 0.1144, 0.1157, 0.1054],
        [0.1488, 0.1395, 0.1380, 0.1068, 0.1226, 0.1177, 0.1180, 0.1086]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1498, 0.1529, 0.1417, 0.1175, 0.1257, 0.1093, 0.1129, 0.0904],
        [0.1288, 0.1358, 0.1406, 0.1248, 0.1234, 0.1131, 0.1211, 0.1123],
        [0.1448, 0.1453, 0.1421, 0.1163, 0.1110, 0.1198, 0.1197, 0.1010]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1562, 0.1460, 0.1441, 0.1046, 0.1173, 0.1093, 0.1182, 0.1043],
        [0.1433, 0.1402, 0.1402, 0.1068, 0.1236, 0.1187, 0.1202, 0.1070],
        [0.1452, 0.1429, 0.1360, 0.1292, 0.1180, 0.1139, 0.1119, 0.1030],
        [0.1624, 0.1574, 0.1593, 0.1081, 0.1117, 0.1056, 0.1051, 0.0904]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1592, 0.1590, 0.1565, 0.1081, 0.0997, 0.1008, 0.1194, 0.0973],
        [0.1468, 0.1419, 0.1452, 0.1268, 0.1130, 0.1138, 0.1040, 0.1084],
        [0.1475, 0.1426, 0.1384, 0.1165, 0.1179, 0.1173, 0.1252, 0.0947],
        [0.1596, 0.1555, 0.1613, 0.1203, 0.1014, 0.1119, 0.0973, 0.0928],
        [0.1569, 0.1518, 0.1676, 0.0982, 0.1077, 0.1124, 0.1116, 0.0937]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1533, 0.1383, 0.1247, 0.1321, 0.1277, 0.1006, 0.1187, 0.1046],
        [0.1159, 0.1097, 0.1141, 0.1258, 0.1252, 0.1336, 0.1378, 0.1380]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1577, 0.1415, 0.1167, 0.1151, 0.1215, 0.1158, 0.1166, 0.1152],
        [0.1103, 0.1044, 0.1328, 0.1263, 0.1248, 0.1336, 0.1189, 0.1489],
        [0.1318, 0.1216, 0.1225, 0.1150, 0.1227, 0.1314, 0.1291, 0.1260]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1542, 0.1357, 0.1114, 0.1195, 0.1213, 0.1179, 0.1094, 0.1306],
        [0.1131, 0.1085, 0.1300, 0.1332, 0.1434, 0.1204, 0.1242, 0.1272],
        [0.1187, 0.1112, 0.1114, 0.1326, 0.1281, 0.1343, 0.1423, 0.1214],
        [0.1384, 0.1284, 0.1353, 0.1113, 0.1206, 0.1078, 0.1248, 0.1334]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1646, 0.1509, 0.1205, 0.1173, 0.1060, 0.1083, 0.1198, 0.1126],
        [0.1139, 0.1104, 0.1371, 0.1194, 0.1175, 0.1327, 0.1346, 0.1344],
        [0.1443, 0.1430, 0.1315, 0.1053, 0.1227, 0.1161, 0.1190, 0.1180],
        [0.1387, 0.1312, 0.1271, 0.1298, 0.1136, 0.1214, 0.1212, 0.1170],
        [0.1393, 0.1319, 0.1284, 0.1199, 0.1264, 0.1170, 0.1128, 0.1243]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/23 01:57:49 PM | size of train loader is 391
07/23 01:57:53 PM | Train: [12/50] Step 000/390 Loss 0.009 Prec@(1,5) (7.8%, 25.0%)
07/23 02:00:56 PM | Train: [12/50] Step 050/390 Loss 0.009 Prec@(1,5) (10.9%, 32.9%)
07/23 02:04:04 PM | Train: [12/50] Step 100/390 Loss 0.009 Prec@(1,5) (11.2%, 32.8%)
07/23 02:07:05 PM | Train: [12/50] Step 150/390 Loss 0.009 Prec@(1,5) (11.1%, 32.0%)
07/23 02:10:07 PM | Train: [12/50] Step 200/390 Loss 0.009 Prec@(1,5) (11.1%, 32.1%)
07/23 02:13:09 PM | Train: [12/50] Step 250/390 Loss 0.010 Prec@(1,5) (11.2%, 32.4%)
07/23 02:16:16 PM | Train: [12/50] Step 300/390 Loss 0.010 Prec@(1,5) (11.3%, 32.5%)
07/23 02:19:17 PM | Train: [12/50] Step 350/390 Loss 0.011 Prec@(1,5) (11.4%, 32.7%)
07/23 02:21:43 PM | Train: [12/50] Step 390/390 Loss 0.011 Prec@(1,5) (11.4%, 32.8%)
07/23 02:21:43 PM | Train: [12/50] Final Prec@1 11.3920%
07/23 02:21:44 PM | Valid: [12/50] Step 000/390 Loss 3.719 Prec@(1,5) (7.8%, 31.2%)
07/23 02:21:53 PM | Valid: [12/50] Step 050/390 Loss 3.895 Prec@(1,5) (12.5%, 33.4%)
07/23 02:22:04 PM | Valid: [12/50] Step 100/390 Loss 3.878 Prec@(1,5) (12.5%, 34.0%)
07/23 02:22:14 PM | Valid: [12/50] Step 150/390 Loss 3.871 Prec@(1,5) (12.6%, 33.8%)
07/23 02:22:23 PM | Valid: [12/50] Step 200/390 Loss 3.867 Prec@(1,5) (12.4%, 33.6%)
07/23 02:22:33 PM | Valid: [12/50] Step 250/390 Loss 3.870 Prec@(1,5) (12.3%, 33.5%)
07/23 02:22:43 PM | Valid: [12/50] Step 300/390 Loss 3.869 Prec@(1,5) (12.4%, 33.6%)
07/23 02:22:54 PM | Valid: [12/50] Step 350/390 Loss 3.868 Prec@(1,5) (12.3%, 33.6%)
07/23 02:23:03 PM | Valid: [12/50] Step 390/390 Loss 3.868 Prec@(1,5) (12.1%, 33.5%)
07/23 02:23:03 PM | Valid: [12/50] Final Prec@1 12.1240%
07/23 02:23:03 PM | genotype = Genotype(normal=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 0)], [('skip_connect', 4), ('skip_connect', 3)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_5x5', 1)], [('max_pool_3x3', 0), ('skip_connect', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 3)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1637, 0.1437, 0.1421, 0.1015, 0.1155, 0.1145, 0.1155, 0.1036],
        [0.1521, 0.1378, 0.1368, 0.1066, 0.1246, 0.1162, 0.1174, 0.1085]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1548, 0.1536, 0.1428, 0.1183, 0.1248, 0.1081, 0.1106, 0.0870],
        [0.1302, 0.1350, 0.1413, 0.1242, 0.1236, 0.1127, 0.1207, 0.1121],
        [0.1478, 0.1453, 0.1425, 0.1165, 0.1104, 0.1209, 0.1192, 0.0976]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1585, 0.1450, 0.1438, 0.1036, 0.1189, 0.1074, 0.1191, 0.1036],
        [0.1454, 0.1409, 0.1412, 0.1063, 0.1233, 0.1168, 0.1210, 0.1050],
        [0.1465, 0.1422, 0.1361, 0.1292, 0.1186, 0.1140, 0.1116, 0.1017],
        [0.1660, 0.1584, 0.1621, 0.1064, 0.1106, 0.1035, 0.1049, 0.0881]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1629, 0.1601, 0.1592, 0.1066, 0.0974, 0.1000, 0.1190, 0.0946],
        [0.1493, 0.1409, 0.1454, 0.1294, 0.1114, 0.1124, 0.1032, 0.1080],
        [0.1501, 0.1420, 0.1382, 0.1156, 0.1188, 0.1177, 0.1241, 0.0934],
        [0.1627, 0.1552, 0.1636, 0.1215, 0.1000, 0.1118, 0.0943, 0.0909],
        [0.1600, 0.1517, 0.1709, 0.0969, 0.1070, 0.1117, 0.1102, 0.0916]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1582, 0.1384, 0.1267, 0.1316, 0.1260, 0.0981, 0.1175, 0.1036],
        [0.1133, 0.1059, 0.1135, 0.1262, 0.1261, 0.1356, 0.1388, 0.1405]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1623, 0.1418, 0.1162, 0.1157, 0.1204, 0.1160, 0.1159, 0.1116],
        [0.1076, 0.1008, 0.1346, 0.1269, 0.1268, 0.1312, 0.1196, 0.1526],
        [0.1334, 0.1207, 0.1225, 0.1131, 0.1234, 0.1298, 0.1299, 0.1271]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1595, 0.1372, 0.1094, 0.1179, 0.1195, 0.1174, 0.1087, 0.1304],
        [0.1122, 0.1064, 0.1306, 0.1324, 0.1451, 0.1216, 0.1233, 0.1283],
        [0.1200, 0.1106, 0.1117, 0.1305, 0.1285, 0.1331, 0.1431, 0.1225],
        [0.1430, 0.1306, 0.1382, 0.1090, 0.1189, 0.1060, 0.1227, 0.1316]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1705, 0.1527, 0.1210, 0.1152, 0.1046, 0.1071, 0.1190, 0.1099],
        [0.1113, 0.1073, 0.1397, 0.1199, 0.1170, 0.1323, 0.1370, 0.1356],
        [0.1471, 0.1431, 0.1305, 0.1038, 0.1225, 0.1166, 0.1188, 0.1176],
        [0.1405, 0.1296, 0.1253, 0.1326, 0.1131, 0.1222, 0.1217, 0.1151],
        [0.1414, 0.1315, 0.1284, 0.1196, 0.1265, 0.1157, 0.1113, 0.1256]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/23 02:23:04 PM | size of train loader is 391
07/23 02:23:09 PM | Train: [13/50] Step 000/390 Loss 0.009 Prec@(1,5) (10.9%, 29.7%)
07/23 02:26:11 PM | Train: [13/50] Step 050/390 Loss 0.008 Prec@(1,5) (10.7%, 32.2%)
07/23 02:29:16 PM | Train: [13/50] Step 100/390 Loss 0.009 Prec@(1,5) (11.4%, 32.8%)
07/23 02:32:21 PM | Train: [13/50] Step 150/390 Loss 0.009 Prec@(1,5) (11.4%, 33.3%)
07/23 02:35:30 PM | Train: [13/50] Step 200/390 Loss 0.009 Prec@(1,5) (11.3%, 33.2%)
07/23 02:38:36 PM | Train: [13/50] Step 250/390 Loss 0.010 Prec@(1,5) (11.4%, 33.3%)
07/23 02:41:36 PM | Train: [13/50] Step 300/390 Loss 0.010 Prec@(1,5) (11.7%, 33.5%)
07/23 02:44:38 PM | Train: [13/50] Step 350/390 Loss 0.010 Prec@(1,5) (11.8%, 33.7%)
07/23 02:47:05 PM | Train: [13/50] Step 390/390 Loss 0.011 Prec@(1,5) (11.9%, 33.9%)
07/23 02:47:05 PM | Train: [13/50] Final Prec@1 11.8840%
07/23 02:47:05 PM | Valid: [13/50] Step 000/390 Loss 3.702 Prec@(1,5) (12.5%, 43.8%)
07/23 02:47:15 PM | Valid: [13/50] Step 050/390 Loss 3.827 Prec@(1,5) (12.5%, 34.9%)
07/23 02:47:24 PM | Valid: [13/50] Step 100/390 Loss 3.835 Prec@(1,5) (12.8%, 34.6%)
07/23 02:47:35 PM | Valid: [13/50] Step 150/390 Loss 3.843 Prec@(1,5) (12.6%, 34.3%)
07/23 02:47:48 PM | Valid: [13/50] Step 200/390 Loss 3.844 Prec@(1,5) (12.6%, 34.2%)
07/23 02:48:00 PM | Valid: [13/50] Step 250/390 Loss 3.848 Prec@(1,5) (12.5%, 34.0%)
07/23 02:48:13 PM | Valid: [13/50] Step 300/390 Loss 3.848 Prec@(1,5) (12.4%, 34.2%)
07/23 02:48:25 PM | Valid: [13/50] Step 350/390 Loss 3.846 Prec@(1,5) (12.5%, 34.1%)
07/23 02:48:33 PM | Valid: [13/50] Step 390/390 Loss 3.845 Prec@(1,5) (12.5%, 34.2%)
07/23 02:48:33 PM | Valid: [13/50] Final Prec@1 12.4880%
07/23 02:48:33 PM | genotype = Genotype(normal=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 0)], [('skip_connect', 4), ('max_pool_3x3', 3)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_3x3', 1)], [('max_pool_3x3', 0), ('skip_connect', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 3)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1687, 0.1413, 0.1411, 0.0999, 0.1169, 0.1152, 0.1146, 0.1023],
        [0.1542, 0.1345, 0.1348, 0.1080, 0.1260, 0.1163, 0.1178, 0.1084]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1610, 0.1546, 0.1443, 0.1190, 0.1223, 0.1058, 0.1089, 0.0841],
        [0.1312, 0.1336, 0.1412, 0.1247, 0.1246, 0.1111, 0.1214, 0.1121],
        [0.1513, 0.1447, 0.1422, 0.1156, 0.1095, 0.1227, 0.1195, 0.0943]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1616, 0.1444, 0.1452, 0.1017, 0.1197, 0.1062, 0.1186, 0.1026],
        [0.1463, 0.1396, 0.1408, 0.1069, 0.1241, 0.1170, 0.1208, 0.1045],
        [0.1472, 0.1404, 0.1368, 0.1284, 0.1182, 0.1158, 0.1114, 0.1018],
        [0.1688, 0.1587, 0.1647, 0.1048, 0.1090, 0.1032, 0.1041, 0.0868]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1668, 0.1592, 0.1592, 0.1059, 0.0966, 0.0994, 0.1194, 0.0935],
        [0.1513, 0.1393, 0.1449, 0.1321, 0.1106, 0.1121, 0.1024, 0.1074],
        [0.1530, 0.1418, 0.1395, 0.1121, 0.1193, 0.1179, 0.1233, 0.0932],
        [0.1654, 0.1543, 0.1646, 0.1222, 0.0985, 0.1130, 0.0918, 0.0903],
        [0.1633, 0.1513, 0.1728, 0.0969, 0.1048, 0.1110, 0.1088, 0.0911]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1609, 0.1366, 0.1295, 0.1332, 0.1234, 0.0966, 0.1170, 0.1028],
        [0.1119, 0.1032, 0.1123, 0.1268, 0.1260, 0.1396, 0.1382, 0.1421]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1647, 0.1391, 0.1159, 0.1182, 0.1219, 0.1165, 0.1142, 0.1094],
        [0.1059, 0.0977, 0.1368, 0.1276, 0.1281, 0.1299, 0.1200, 0.1541],
        [0.1351, 0.1191, 0.1221, 0.1116, 0.1238, 0.1313, 0.1300, 0.1269]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1625, 0.1357, 0.1083, 0.1174, 0.1194, 0.1181, 0.1078, 0.1308],
        [0.1123, 0.1047, 0.1310, 0.1327, 0.1462, 0.1230, 0.1224, 0.1279],
        [0.1205, 0.1082, 0.1107, 0.1296, 0.1299, 0.1339, 0.1438, 0.1234],
        [0.1455, 0.1305, 0.1391, 0.1079, 0.1171, 0.1056, 0.1220, 0.1324]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1726, 0.1502, 0.1233, 0.1145, 0.1042, 0.1084, 0.1182, 0.1085],
        [0.1097, 0.1048, 0.1409, 0.1197, 0.1161, 0.1340, 0.1392, 0.1356],
        [0.1485, 0.1411, 0.1299, 0.1034, 0.1229, 0.1175, 0.1180, 0.1186],
        [0.1413, 0.1281, 0.1236, 0.1368, 0.1120, 0.1205, 0.1239, 0.1138],
        [0.1434, 0.1306, 0.1284, 0.1197, 0.1262, 0.1139, 0.1099, 0.1279]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/23 02:48:34 PM | size of train loader is 391
07/23 02:48:38 PM | Train: [14/50] Step 000/390 Loss 0.008 Prec@(1,5) (20.3%, 42.2%)
07/23 02:51:49 PM | Train: [14/50] Step 050/390 Loss 0.008 Prec@(1,5) (13.0%, 35.1%)
07/23 02:55:02 PM | Train: [14/50] Step 100/390 Loss 0.008 Prec@(1,5) (12.9%, 34.9%)
07/23 02:58:12 PM | Train: [14/50] Step 150/390 Loss 0.009 Prec@(1,5) (12.8%, 34.8%)
07/23 03:01:25 PM | Train: [14/50] Step 200/390 Loss 0.009 Prec@(1,5) (12.6%, 34.6%)
07/23 03:04:38 PM | Train: [14/50] Step 250/390 Loss 0.009 Prec@(1,5) (12.6%, 34.7%)
07/23 03:07:53 PM | Train: [14/50] Step 300/390 Loss 0.010 Prec@(1,5) (12.5%, 34.7%)
07/23 03:11:07 PM | Train: [14/50] Step 350/390 Loss 0.010 Prec@(1,5) (12.5%, 34.5%)
07/23 03:13:44 PM | Train: [14/50] Step 390/390 Loss 0.010 Prec@(1,5) (12.6%, 34.6%)
07/23 03:13:44 PM | Train: [14/50] Final Prec@1 12.6000%
07/23 03:13:45 PM | Valid: [14/50] Step 000/390 Loss 3.740 Prec@(1,5) (15.6%, 35.9%)
07/23 03:13:55 PM | Valid: [14/50] Step 050/390 Loss 3.789 Prec@(1,5) (13.4%, 36.2%)
07/23 03:14:05 PM | Valid: [14/50] Step 100/390 Loss 3.807 Prec@(1,5) (13.1%, 35.7%)
07/23 03:14:15 PM | Valid: [14/50] Step 150/390 Loss 3.801 Prec@(1,5) (13.5%, 35.5%)
07/23 03:14:25 PM | Valid: [14/50] Step 200/390 Loss 3.805 Prec@(1,5) (13.3%, 35.5%)
07/23 03:14:35 PM | Valid: [14/50] Step 250/390 Loss 3.804 Prec@(1,5) (13.3%, 35.5%)
07/23 03:14:44 PM | Valid: [14/50] Step 300/390 Loss 3.804 Prec@(1,5) (13.3%, 35.4%)
07/23 03:14:54 PM | Valid: [14/50] Step 350/390 Loss 3.808 Prec@(1,5) (13.2%, 35.2%)
07/23 03:15:03 PM | Valid: [14/50] Step 390/390 Loss 3.809 Prec@(1,5) (13.1%, 35.1%)
07/23 03:15:03 PM | Valid: [14/50] Final Prec@1 13.0720%
07/23 03:15:03 PM | genotype = Genotype(normal=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 0)], [('skip_connect', 4), ('max_pool_3x3', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_3x3', 1)], [('max_pool_3x3', 0), ('skip_connect', 1)], [('max_pool_3x3', 0), ('max_pool_3x3', 3)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1743, 0.1393, 0.1399, 0.0979, 0.1172, 0.1159, 0.1148, 0.1006],
        [0.1562, 0.1316, 0.1331, 0.1098, 0.1268, 0.1154, 0.1183, 0.1089]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1669, 0.1555, 0.1466, 0.1202, 0.1194, 0.1037, 0.1065, 0.0813],
        [0.1303, 0.1310, 0.1398, 0.1262, 0.1267, 0.1098, 0.1237, 0.1125],
        [0.1526, 0.1429, 0.1415, 0.1156, 0.1098, 0.1254, 0.1200, 0.0921]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1645, 0.1428, 0.1445, 0.1013, 0.1214, 0.1040, 0.1196, 0.1018],
        [0.1482, 0.1391, 0.1412, 0.1059, 0.1251, 0.1167, 0.1199, 0.1039],
        [0.1483, 0.1393, 0.1366, 0.1296, 0.1176, 0.1159, 0.1108, 0.1019],
        [0.1726, 0.1596, 0.1674, 0.1029, 0.1075, 0.1018, 0.1043, 0.0840]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1703, 0.1578, 0.1595, 0.1066, 0.0957, 0.0979, 0.1198, 0.0923],
        [0.1503, 0.1350, 0.1423, 0.1367, 0.1106, 0.1124, 0.1036, 0.1089],
        [0.1541, 0.1394, 0.1387, 0.1100, 0.1196, 0.1191, 0.1245, 0.0945],
        [0.1666, 0.1517, 0.1636, 0.1254, 0.0977, 0.1145, 0.0905, 0.0901],
        [0.1665, 0.1497, 0.1725, 0.0960, 0.1046, 0.1097, 0.1087, 0.0923]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1660, 0.1367, 0.1308, 0.1342, 0.1200, 0.0934, 0.1170, 0.1018],
        [0.1102, 0.1003, 0.1127, 0.1267, 0.1257, 0.1425, 0.1379, 0.1439]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1683, 0.1386, 0.1150, 0.1194, 0.1223, 0.1165, 0.1130, 0.1069],
        [0.1037, 0.0946, 0.1383, 0.1272, 0.1307, 0.1304, 0.1191, 0.1560],
        [0.1364, 0.1177, 0.1216, 0.1082, 0.1258, 0.1317, 0.1310, 0.1276]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1688, 0.1370, 0.1066, 0.1178, 0.1173, 0.1166, 0.1061, 0.1298],
        [0.1123, 0.1031, 0.1314, 0.1322, 0.1475, 0.1251, 0.1199, 0.1285],
        [0.1211, 0.1069, 0.1103, 0.1280, 0.1314, 0.1350, 0.1436, 0.1237],
        [0.1506, 0.1324, 0.1420, 0.1052, 0.1153, 0.1033, 0.1208, 0.1304]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1769, 0.1513, 0.1242, 0.1129, 0.1017, 0.1082, 0.1173, 0.1074],
        [0.1078, 0.1027, 0.1430, 0.1189, 0.1153, 0.1345, 0.1408, 0.1370],
        [0.1494, 0.1398, 0.1286, 0.1028, 0.1238, 0.1193, 0.1181, 0.1182],
        [0.1429, 0.1272, 0.1231, 0.1394, 0.1104, 0.1207, 0.1230, 0.1133],
        [0.1457, 0.1305, 0.1290, 0.1196, 0.1240, 0.1123, 0.1087, 0.1302]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/23 03:15:05 PM | size of train loader is 391
07/23 03:15:10 PM | Train: [15/50] Step 000/390 Loss 0.009 Prec@(1,5) (10.9%, 25.0%)
07/23 03:18:34 PM | Train: [15/50] Step 050/390 Loss 0.008 Prec@(1,5) (13.4%, 36.0%)
07/23 03:21:46 PM | Train: [15/50] Step 100/390 Loss 0.008 Prec@(1,5) (12.6%, 35.2%)
07/23 03:25:05 PM | Train: [15/50] Step 150/390 Loss 0.009 Prec@(1,5) (12.9%, 35.9%)
07/23 03:28:26 PM | Train: [15/50] Step 200/390 Loss 0.009 Prec@(1,5) (12.7%, 35.6%)
07/23 03:31:29 PM | Train: [15/50] Step 250/390 Loss 0.009 Prec@(1,5) (12.7%, 35.6%)
07/23 03:34:27 PM | Train: [15/50] Step 300/390 Loss 0.010 Prec@(1,5) (12.7%, 35.6%)
07/23 03:37:33 PM | Train: [15/50] Step 350/390 Loss 0.010 Prec@(1,5) (12.7%, 35.6%)
07/23 03:39:50 PM | Train: [15/50] Step 390/390 Loss 0.010 Prec@(1,5) (12.7%, 35.8%)
