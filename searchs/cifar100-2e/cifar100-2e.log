07/23 11:52:23 PM | 
07/23 11:52:23 PM | Parameters:
07/23 11:52:23 PM | ALPHA_LR=0.0003
07/23 11:52:23 PM | ALPHA_WEIGHT_DECAY=0.001
07/23 11:52:23 PM | BATCH_SIZE=64
07/23 11:52:23 PM | DATA_PATH=./data/
07/23 11:52:23 PM | DATASET=cifar100
07/23 11:52:23 PM | EPOCHS=50
07/23 11:52:23 PM | GPUS=[0]
07/23 11:52:23 PM | INIT_CHANNELS=16
07/23 11:52:23 PM | LAYERS=8
07/23 11:52:23 PM | NAME=cifar100-2e
07/23 11:52:23 PM | PATH=searchs/cifar100-2e
07/23 11:52:23 PM | PLOT_PATH=searchs/cifar100-2e/plots
07/23 11:52:23 PM | PRINT_FREQ=50
07/23 11:52:23 PM | SEED=2
07/23 11:52:23 PM | W_GRAD_CLIP=5.0
07/23 11:52:23 PM | W_LR=0.025
07/23 11:52:23 PM | W_LR_MIN=0.001
07/23 11:52:23 PM | W_MOMENTUM=0.9
07/23 11:52:23 PM | W_WEIGHT_DECAY=0.0003
07/23 11:52:23 PM | WORKERS=4
07/23 11:52:23 PM | 
07/23 11:52:23 PM | Logger is set - training start
07/24 12:15:41 AM | 
07/24 12:15:41 AM | Parameters:
07/24 12:15:41 AM | ALPHA_LR=0.0003
07/24 12:15:41 AM | ALPHA_WEIGHT_DECAY=0.001
07/24 12:15:41 AM | BATCH_SIZE=64
07/24 12:15:41 AM | DATA_PATH=./data/
07/24 12:15:41 AM | DATASET=cifar100
07/24 12:15:41 AM | EPOCHS=50
07/24 12:15:41 AM | GPUS=[0]
07/24 12:15:41 AM | INIT_CHANNELS=16
07/24 12:15:41 AM | LAYERS=8
07/24 12:15:41 AM | NAME=cifar100-2e
07/24 12:15:41 AM | PATH=searchs/cifar100-2e
07/24 12:15:41 AM | PLOT_PATH=searchs/cifar100-2e/plots
07/24 12:15:41 AM | PRINT_FREQ=50
07/24 12:15:41 AM | SEED=2
07/24 12:15:41 AM | W_GRAD_CLIP=5.0
07/24 12:15:41 AM | W_LR=0.025
07/24 12:15:41 AM | W_LR_MIN=0.001
07/24 12:15:41 AM | W_MOMENTUM=0.9
07/24 12:15:41 AM | W_WEIGHT_DECAY=0.0003
07/24 12:15:41 AM | WORKERS=4
07/24 12:15:41 AM | 
07/24 12:15:41 AM | Logger is set - training start
07/24 12:28:09 AM | 
07/24 12:28:09 AM | Parameters:
07/24 12:28:09 AM | ALPHA_LR=0.0003
07/24 12:28:09 AM | ALPHA_WEIGHT_DECAY=0.001
07/24 12:28:09 AM | BATCH_SIZE=64
07/24 12:28:09 AM | DATA_PATH=./data/
07/24 12:28:09 AM | DATASET=cifar100
07/24 12:28:09 AM | EPOCHS=50
07/24 12:28:09 AM | GPUS=[0]
07/24 12:28:09 AM | INIT_CHANNELS=16
07/24 12:28:09 AM | LAYERS=8
07/24 12:28:09 AM | NAME=cifar100-2e
07/24 12:28:09 AM | PATH=searchs/cifar100-2e
07/24 12:28:09 AM | PLOT_PATH=searchs/cifar100-2e/plots
07/24 12:28:09 AM | PRINT_FREQ=50
07/24 12:28:09 AM | SEED=2
07/24 12:28:09 AM | W_GRAD_CLIP=5.0
07/24 12:28:09 AM | W_LR=0.025
07/24 12:28:09 AM | W_LR_MIN=0.001
07/24 12:28:09 AM | W_MOMENTUM=0.9
07/24 12:28:09 AM | W_WEIGHT_DECAY=0.0003
07/24 12:28:09 AM | WORKERS=4
07/24 12:28:09 AM | 
07/24 12:28:09 AM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/24 12:28:32 AM | Train: [ 1/50] Step 000/390 Loss 4.726 Prec@(1,5) (1.6%, 9.4%)
07/24 12:33:15 AM | Train: [ 1/50] Step 050/390 Loss 4.423 Prec@(1,5) (3.7%, 14.8%)
07/24 12:37:54 AM | Train: [ 1/50] Step 100/390 Loss 4.270 Prec@(1,5) (5.1%, 19.5%)
07/24 12:42:32 AM | Train: [ 1/50] Step 150/390 Loss 4.177 Prec@(1,5) (6.0%, 22.2%)
07/24 12:47:15 AM | Train: [ 1/50] Step 200/390 Loss 4.085 Prec@(1,5) (7.3%, 25.0%)
07/24 12:51:57 AM | Train: [ 1/50] Step 250/390 Loss 4.013 Prec@(1,5) (8.1%, 27.0%)
07/24 12:56:48 AM | Train: [ 1/50] Step 300/390 Loss 3.953 Prec@(1,5) (9.1%, 29.1%)
07/24 01:01:34 AM | Train: [ 1/50] Step 350/390 Loss 3.895 Prec@(1,5) (9.9%, 30.7%)
07/24 01:05:26 AM | Train: [ 1/50] Step 390/390 Loss 3.855 Prec@(1,5) (10.6%, 32.0%)
07/24 01:05:26 AM | Train: [ 1/50] Final Prec@1 10.5720%
07/24 01:05:27 AM | Valid: [ 1/50] Step 000/390 Loss 3.654 Prec@(1,5) (10.9%, 35.9%)
07/24 01:05:43 AM | Valid: [ 1/50] Step 050/390 Loss 3.509 Prec@(1,5) (16.0%, 43.2%)
07/24 01:06:00 AM | Valid: [ 1/50] Step 100/390 Loss 3.495 Prec@(1,5) (16.6%, 43.8%)
07/24 01:06:16 AM | Valid: [ 1/50] Step 150/390 Loss 3.481 Prec@(1,5) (16.7%, 43.8%)
07/24 01:06:32 AM | Valid: [ 1/50] Step 200/390 Loss 3.477 Prec@(1,5) (17.0%, 43.7%)
07/24 01:06:49 AM | Valid: [ 1/50] Step 250/390 Loss 3.476 Prec@(1,5) (17.0%, 43.7%)
07/24 01:07:05 AM | Valid: [ 1/50] Step 300/390 Loss 3.480 Prec@(1,5) (16.7%, 43.7%)
07/24 01:07:22 AM | Valid: [ 1/50] Step 350/390 Loss 3.486 Prec@(1,5) (16.5%, 43.5%)
07/24 01:07:35 AM | Valid: [ 1/50] Step 390/390 Loss 3.488 Prec@(1,5) (16.5%, 43.5%)
07/24 01:07:35 AM | Valid: [ 1/50] Final Prec@1 16.5320%
07/24 01:07:35 AM | genotype = Genotype(normal=[[('dil_conv_5x5', 1), ('max_pool_3x3', 0)], [('sep_conv_5x5', 1), ('sep_conv_3x3', 2)], [('dil_conv_5x5', 3), ('sep_conv_5x5', 0)], [('dil_conv_3x3', 1), ('dil_conv_5x5', 2)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('dil_conv_3x3', 1)], [('max_pool_3x3', 0), ('sep_conv_5x5', 2)], [('dil_conv_5x5', 3), ('dil_conv_5x5', 2)], [('max_pool_3x3', 0), ('dil_conv_5x5', 4)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1280, 0.1215, 0.1246, 0.1235, 0.1263, 0.1255, 0.1260, 0.1246],
        [0.1230, 0.1189, 0.1204, 0.1233, 0.1291, 0.1270, 0.1293, 0.1291]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1270, 0.1207, 0.1234, 0.1252, 0.1261, 0.1259, 0.1271, 0.1247],
        [0.1234, 0.1186, 0.1205, 0.1250, 0.1285, 0.1271, 0.1272, 0.1297],
        [0.1231, 0.1191, 0.1214, 0.1281, 0.1274, 0.1256, 0.1267, 0.1287]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1263, 0.1198, 0.1220, 0.1264, 0.1282, 0.1253, 0.1256, 0.1265],
        [0.1222, 0.1177, 0.1195, 0.1277, 0.1279, 0.1280, 0.1272, 0.1299],
        [0.1224, 0.1188, 0.1215, 0.1258, 0.1262, 0.1282, 0.1271, 0.1300],
        [0.1218, 0.1185, 0.1200, 0.1264, 0.1281, 0.1265, 0.1291, 0.1296]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1259, 0.1204, 0.1226, 0.1262, 0.1267, 0.1246, 0.1264, 0.1271],
        [0.1223, 0.1184, 0.1200, 0.1254, 0.1273, 0.1290, 0.1274, 0.1301],
        [0.1215, 0.1183, 0.1200, 0.1288, 0.1275, 0.1264, 0.1288, 0.1286],
        [0.1213, 0.1184, 0.1200, 0.1274, 0.1275, 0.1282, 0.1277, 0.1296],
        [0.1209, 0.1187, 0.1197, 0.1283, 0.1267, 0.1279, 0.1286, 0.1292]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1281, 0.1251, 0.1230, 0.1256, 0.1249, 0.1248, 0.1249, 0.1238],
        [0.1261, 0.1235, 0.1250, 0.1239, 0.1249, 0.1261, 0.1245, 0.1260]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1284, 0.1247, 0.1267, 0.1244, 0.1238, 0.1237, 0.1242, 0.1240],
        [0.1265, 0.1232, 0.1255, 0.1257, 0.1241, 0.1231, 0.1269, 0.1251],
        [0.1255, 0.1206, 0.1236, 0.1256, 0.1278, 0.1243, 0.1272, 0.1254]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1272, 0.1241, 0.1259, 0.1241, 0.1256, 0.1255, 0.1242, 0.1233],
        [0.1253, 0.1221, 0.1258, 0.1271, 0.1251, 0.1252, 0.1237, 0.1257],
        [0.1248, 0.1208, 0.1239, 0.1244, 0.1275, 0.1257, 0.1275, 0.1254],
        [0.1229, 0.1194, 0.1234, 0.1266, 0.1267, 0.1251, 0.1274, 0.1283]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1283, 0.1246, 0.1254, 0.1234, 0.1253, 0.1238, 0.1251, 0.1240],
        [0.1272, 0.1232, 0.1264, 0.1225, 0.1238, 0.1252, 0.1260, 0.1257],
        [0.1257, 0.1202, 0.1246, 0.1254, 0.1271, 0.1242, 0.1270, 0.1258],
        [0.1246, 0.1212, 0.1247, 0.1261, 0.1263, 0.1257, 0.1259, 0.1255],
        [0.1240, 0.1209, 0.1241, 0.1264, 0.1271, 0.1245, 0.1276, 0.1254]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/24 01:07:43 AM | Train: [ 2/50] Step 000/390 Loss 3.499 Prec@(1,5) (21.9%, 45.3%)
07/24 01:12:29 AM | Train: [ 2/50] Step 050/390 Loss 3.354 Prec@(1,5) (19.8%, 46.4%)
07/24 01:17:19 AM | Train: [ 2/50] Step 100/390 Loss 3.319 Prec@(1,5) (20.1%, 47.6%)
07/24 01:22:04 AM | Train: [ 2/50] Step 150/390 Loss 3.303 Prec@(1,5) (19.9%, 47.7%)
07/24 01:26:49 AM | Train: [ 2/50] Step 200/390 Loss 3.281 Prec@(1,5) (20.0%, 48.3%)
07/24 01:31:37 AM | Train: [ 2/50] Step 250/390 Loss 3.260 Prec@(1,5) (20.3%, 49.0%)
07/24 01:36:25 AM | Train: [ 2/50] Step 300/390 Loss 3.226 Prec@(1,5) (20.9%, 50.0%)
07/24 01:41:08 AM | Train: [ 2/50] Step 350/390 Loss 3.201 Prec@(1,5) (21.2%, 50.7%)
07/24 01:45:00 AM | Train: [ 2/50] Step 390/390 Loss 3.187 Prec@(1,5) (21.4%, 51.0%)
07/24 01:45:00 AM | Train: [ 2/50] Final Prec@1 21.4480%
07/24 01:45:01 AM | Valid: [ 2/50] Step 000/390 Loss 2.614 Prec@(1,5) (34.4%, 70.3%)
07/24 01:45:17 AM | Valid: [ 2/50] Step 050/390 Loss 2.982 Prec@(1,5) (24.4%, 56.9%)
07/24 01:45:33 AM | Valid: [ 2/50] Step 100/390 Loss 2.987 Prec@(1,5) (24.4%, 56.6%)
07/24 01:45:50 AM | Valid: [ 2/50] Step 150/390 Loss 3.003 Prec@(1,5) (24.3%, 56.2%)
07/24 01:46:06 AM | Valid: [ 2/50] Step 200/390 Loss 3.014 Prec@(1,5) (24.2%, 56.0%)
07/24 01:46:22 AM | Valid: [ 2/50] Step 250/390 Loss 3.028 Prec@(1,5) (23.9%, 55.7%)
07/24 01:46:38 AM | Valid: [ 2/50] Step 300/390 Loss 3.018 Prec@(1,5) (24.1%, 55.9%)
07/24 01:46:54 AM | Valid: [ 2/50] Step 350/390 Loss 3.020 Prec@(1,5) (24.1%, 55.7%)
07/24 01:47:08 AM | Valid: [ 2/50] Step 390/390 Loss 3.019 Prec@(1,5) (24.1%, 55.8%)
07/24 01:47:08 AM | Valid: [ 2/50] Final Prec@1 24.0840%
07/24 01:47:08 AM | genotype = Genotype(normal=[[('sep_conv_5x5', 1), ('max_pool_3x3', 0)], [('sep_conv_5x5', 1), ('max_pool_3x3', 0)], [('sep_conv_5x5', 3), ('dil_conv_3x3', 1)], [('dil_conv_3x3', 1), ('sep_conv_3x3', 2)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('sep_conv_5x5', 1)], [('max_pool_3x3', 0), ('dil_conv_5x5', 2)], [('max_pool_3x3', 0), ('dil_conv_5x5', 2)], [('max_pool_3x3', 0), ('dil_conv_5x5', 4)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1301, 0.1182, 0.1244, 0.1224, 0.1284, 0.1277, 0.1247, 0.1241],
        [0.1233, 0.1151, 0.1191, 0.1229, 0.1322, 0.1262, 0.1312, 0.1300]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1304, 0.1183, 0.1236, 0.1245, 0.1258, 0.1269, 0.1265, 0.1239],
        [0.1233, 0.1141, 0.1188, 0.1248, 0.1301, 0.1284, 0.1283, 0.1322],
        [0.1239, 0.1161, 0.1211, 0.1293, 0.1279, 0.1261, 0.1264, 0.1292]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1296, 0.1172, 0.1221, 0.1263, 0.1291, 0.1238, 0.1270, 0.1248],
        [0.1219, 0.1131, 0.1172, 0.1273, 0.1290, 0.1308, 0.1292, 0.1314],
        [0.1225, 0.1153, 0.1200, 0.1248, 0.1265, 0.1305, 0.1303, 0.1301],
        [0.1209, 0.1146, 0.1178, 0.1277, 0.1320, 0.1260, 0.1303, 0.1307]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1295, 0.1185, 0.1234, 0.1257, 0.1278, 0.1242, 0.1254, 0.1256],
        [0.1234, 0.1146, 0.1188, 0.1250, 0.1282, 0.1309, 0.1278, 0.1313],
        [0.1228, 0.1160, 0.1194, 0.1305, 0.1277, 0.1258, 0.1294, 0.1284],
        [0.1215, 0.1156, 0.1189, 0.1273, 0.1269, 0.1301, 0.1294, 0.1304],
        [0.1217, 0.1165, 0.1191, 0.1303, 0.1251, 0.1282, 0.1301, 0.1290]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1335, 0.1272, 0.1204, 0.1234, 0.1262, 0.1243, 0.1243, 0.1208],
        [0.1258, 0.1212, 0.1238, 0.1226, 0.1271, 0.1257, 0.1245, 0.1294]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1335, 0.1260, 0.1275, 0.1238, 0.1239, 0.1213, 0.1224, 0.1216],
        [0.1261, 0.1203, 0.1263, 0.1256, 0.1241, 0.1222, 0.1276, 0.1279],
        [0.1254, 0.1163, 0.1223, 0.1275, 0.1284, 0.1235, 0.1309, 0.1257]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1305, 0.1243, 0.1250, 0.1233, 0.1262, 0.1259, 0.1228, 0.1220],
        [0.1239, 0.1182, 0.1272, 0.1274, 0.1271, 0.1261, 0.1238, 0.1263],
        [0.1233, 0.1157, 0.1221, 0.1265, 0.1279, 0.1269, 0.1300, 0.1275],
        [0.1210, 0.1134, 0.1201, 0.1292, 0.1296, 0.1261, 0.1302, 0.1304]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1335, 0.1253, 0.1235, 0.1237, 0.1247, 0.1232, 0.1234, 0.1227],
        [0.1270, 0.1201, 0.1275, 0.1241, 0.1235, 0.1246, 0.1266, 0.1265],
        [0.1266, 0.1161, 0.1235, 0.1259, 0.1295, 0.1243, 0.1279, 0.1261],
        [0.1249, 0.1173, 0.1234, 0.1287, 0.1291, 0.1250, 0.1259, 0.1256],
        [0.1236, 0.1169, 0.1227, 0.1251, 0.1310, 0.1245, 0.1321, 0.1242]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/24 01:47:15 AM | Train: [ 3/50] Step 000/390 Loss 2.981 Prec@(1,5) (21.9%, 60.9%)
07/24 01:52:02 AM | Train: [ 3/50] Step 050/390 Loss 2.871 Prec@(1,5) (26.7%, 59.7%)
07/24 01:56:51 AM | Train: [ 3/50] Step 100/390 Loss 2.877 Prec@(1,5) (26.7%, 59.2%)
07/24 02:01:38 AM | Train: [ 3/50] Step 150/390 Loss 2.863 Prec@(1,5) (27.0%, 59.5%)
07/24 02:06:24 AM | Train: [ 3/50] Step 200/390 Loss 2.841 Prec@(1,5) (27.3%, 59.7%)
07/24 02:11:10 AM | Train: [ 3/50] Step 250/390 Loss 2.821 Prec@(1,5) (27.8%, 60.1%)
07/24 02:15:53 AM | Train: [ 3/50] Step 300/390 Loss 2.815 Prec@(1,5) (28.0%, 60.2%)
07/24 02:20:37 AM | Train: [ 3/50] Step 350/390 Loss 2.800 Prec@(1,5) (28.4%, 60.6%)
07/24 02:24:21 AM | Train: [ 3/50] Step 390/390 Loss 2.784 Prec@(1,5) (28.7%, 61.0%)
07/24 02:24:21 AM | Train: [ 3/50] Final Prec@1 28.6840%
07/24 02:24:22 AM | Valid: [ 3/50] Step 000/390 Loss 2.744 Prec@(1,5) (32.8%, 64.1%)
07/24 02:24:38 AM | Valid: [ 3/50] Step 050/390 Loss 2.776 Prec@(1,5) (29.7%, 61.9%)
07/24 02:24:55 AM | Valid: [ 3/50] Step 100/390 Loss 2.788 Prec@(1,5) (28.9%, 61.5%)
07/24 02:25:11 AM | Valid: [ 3/50] Step 150/390 Loss 2.782 Prec@(1,5) (28.8%, 61.6%)
07/24 02:25:28 AM | Valid: [ 3/50] Step 200/390 Loss 2.781 Prec@(1,5) (28.7%, 61.4%)
07/24 02:25:44 AM | Valid: [ 3/50] Step 250/390 Loss 2.781 Prec@(1,5) (28.7%, 61.3%)
07/24 02:26:01 AM | Valid: [ 3/50] Step 300/390 Loss 2.777 Prec@(1,5) (28.9%, 61.5%)
07/24 02:26:19 AM | Valid: [ 3/50] Step 350/390 Loss 2.773 Prec@(1,5) (29.2%, 61.6%)
07/24 02:26:32 AM | Valid: [ 3/50] Step 390/390 Loss 2.771 Prec@(1,5) (29.2%, 61.6%)
07/24 02:26:32 AM | Valid: [ 3/50] Final Prec@1 29.2200%
07/24 02:26:32 AM | genotype = Genotype(normal=[[('sep_conv_5x5', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('sep_conv_5x5', 1)], [('sep_conv_5x5', 3), ('dil_conv_3x3', 1)], [('dil_conv_3x3', 1), ('dil_conv_5x5', 4)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('sep_conv_5x5', 1)], [('max_pool_3x3', 0), ('dil_conv_5x5', 2)], [('max_pool_3x3', 0), ('sep_conv_3x3', 3)], [('max_pool_3x3', 0), ('dil_conv_5x5', 4)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1321, 0.1155, 0.1243, 0.1209, 0.1309, 0.1288, 0.1252, 0.1222],
        [0.1219, 0.1104, 0.1166, 0.1237, 0.1346, 0.1268, 0.1334, 0.1326]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1340, 0.1166, 0.1248, 0.1251, 0.1251, 0.1266, 0.1247, 0.1232],
        [0.1221, 0.1096, 0.1166, 0.1249, 0.1333, 0.1288, 0.1294, 0.1353],
        [0.1240, 0.1125, 0.1211, 0.1320, 0.1280, 0.1271, 0.1250, 0.1302]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1328, 0.1156, 0.1227, 0.1254, 0.1294, 0.1225, 0.1284, 0.1233],
        [0.1194, 0.1079, 0.1138, 0.1301, 0.1301, 0.1344, 0.1303, 0.1339],
        [0.1221, 0.1117, 0.1189, 0.1243, 0.1275, 0.1330, 0.1312, 0.1312],
        [0.1182, 0.1098, 0.1146, 0.1298, 0.1360, 0.1278, 0.1311, 0.1327]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1326, 0.1169, 0.1237, 0.1266, 0.1288, 0.1235, 0.1238, 0.1242],
        [0.1222, 0.1100, 0.1162, 0.1248, 0.1304, 0.1325, 0.1297, 0.1343],
        [0.1233, 0.1125, 0.1183, 0.1314, 0.1279, 0.1274, 0.1295, 0.1297],
        [0.1207, 0.1120, 0.1173, 0.1275, 0.1276, 0.1307, 0.1301, 0.1342],
        [0.1219, 0.1139, 0.1182, 0.1308, 0.1235, 0.1284, 0.1326, 0.1306]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1362, 0.1273, 0.1210, 0.1224, 0.1264, 0.1226, 0.1251, 0.1189],
        [0.1260, 0.1195, 0.1246, 0.1234, 0.1282, 0.1236, 0.1237, 0.1311]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1378, 0.1272, 0.1276, 0.1223, 0.1240, 0.1191, 0.1210, 0.1210],
        [0.1264, 0.1181, 0.1269, 0.1257, 0.1246, 0.1207, 0.1287, 0.1289],
        [0.1267, 0.1136, 0.1220, 0.1276, 0.1294, 0.1232, 0.1319, 0.1255]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1333, 0.1246, 0.1251, 0.1229, 0.1269, 0.1249, 0.1208, 0.1215],
        [0.1244, 0.1168, 0.1284, 0.1275, 0.1271, 0.1253, 0.1232, 0.1274],
        [0.1228, 0.1123, 0.1209, 0.1289, 0.1290, 0.1287, 0.1294, 0.1279],
        [0.1193, 0.1098, 0.1182, 0.1328, 0.1306, 0.1262, 0.1322, 0.1309]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1365, 0.1249, 0.1225, 0.1251, 0.1257, 0.1226, 0.1217, 0.1210],
        [0.1279, 0.1181, 0.1264, 0.1262, 0.1245, 0.1233, 0.1267, 0.1270],
        [0.1272, 0.1126, 0.1219, 0.1261, 0.1318, 0.1263, 0.1283, 0.1257],
        [0.1253, 0.1149, 0.1236, 0.1289, 0.1315, 0.1238, 0.1242, 0.1279],
        [0.1230, 0.1140, 0.1215, 0.1256, 0.1335, 0.1238, 0.1337, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/24 02:26:39 AM | Train: [ 4/50] Step 000/390 Loss 2.521 Prec@(1,5) (35.9%, 68.8%)
07/24 02:31:21 AM | Train: [ 4/50] Step 050/390 Loss 2.520 Prec@(1,5) (33.5%, 67.4%)
07/24 02:36:05 AM | Train: [ 4/50] Step 100/390 Loss 2.546 Prec@(1,5) (33.2%, 66.5%)
07/24 02:40:53 AM | Train: [ 4/50] Step 150/390 Loss 2.535 Prec@(1,5) (33.5%, 66.7%)
07/24 02:45:40 AM | Train: [ 4/50] Step 200/390 Loss 2.525 Prec@(1,5) (33.7%, 66.9%)
07/24 02:50:24 AM | Train: [ 4/50] Step 250/390 Loss 2.509 Prec@(1,5) (34.1%, 67.3%)
07/24 02:55:12 AM | Train: [ 4/50] Step 300/390 Loss 2.501 Prec@(1,5) (34.3%, 67.6%)
07/24 02:59:59 AM | Train: [ 4/50] Step 350/390 Loss 2.497 Prec@(1,5) (34.6%, 67.7%)
07/24 03:03:45 AM | Train: [ 4/50] Step 390/390 Loss 2.493 Prec@(1,5) (34.6%, 67.9%)
07/24 03:03:45 AM | Train: [ 4/50] Final Prec@1 34.5920%
07/24 03:03:46 AM | Valid: [ 4/50] Step 000/390 Loss 2.703 Prec@(1,5) (32.8%, 64.1%)
07/24 03:04:02 AM | Valid: [ 4/50] Step 050/390 Loss 2.605 Prec@(1,5) (34.1%, 65.1%)
07/24 03:04:19 AM | Valid: [ 4/50] Step 100/390 Loss 2.621 Prec@(1,5) (33.7%, 65.2%)
07/24 03:04:39 AM | Valid: [ 4/50] Step 150/390 Loss 2.638 Prec@(1,5) (33.2%, 64.9%)
07/24 03:04:56 AM | Valid: [ 4/50] Step 200/390 Loss 2.618 Prec@(1,5) (33.2%, 65.3%)
07/24 03:05:12 AM | Valid: [ 4/50] Step 250/390 Loss 2.621 Prec@(1,5) (33.2%, 65.3%)
07/24 03:05:28 AM | Valid: [ 4/50] Step 300/390 Loss 2.629 Prec@(1,5) (33.1%, 65.1%)
07/24 03:05:48 AM | Valid: [ 4/50] Step 350/390 Loss 2.627 Prec@(1,5) (33.1%, 65.1%)
07/24 03:06:05 AM | Valid: [ 4/50] Step 390/390 Loss 2.626 Prec@(1,5) (33.1%, 65.2%)
07/24 03:06:05 AM | Valid: [ 4/50] Final Prec@1 33.1200%
07/24 03:06:05 AM | genotype = Genotype(normal=[[('sep_conv_5x5', 1), ('sep_conv_5x5', 0)], [('max_pool_3x3', 0), ('sep_conv_5x5', 1)], [('sep_conv_5x5', 3), ('dil_conv_3x3', 1)], [('dil_conv_5x5', 4), ('dil_conv_3x3', 3)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('sep_conv_5x5', 1)], [('max_pool_3x3', 0), ('dil_conv_5x5', 2)], [('dil_conv_5x5', 3), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('dil_conv_5x5', 4)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1322, 0.1132, 0.1245, 0.1218, 0.1332, 0.1286, 0.1249, 0.1215],
        [0.1193, 0.1071, 0.1155, 0.1238, 0.1379, 0.1266, 0.1356, 0.1343]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1365, 0.1157, 0.1264, 0.1242, 0.1241, 0.1257, 0.1240, 0.1234],
        [0.1198, 0.1061, 0.1153, 0.1264, 0.1349, 0.1305, 0.1297, 0.1372],
        [0.1223, 0.1091, 0.1213, 0.1322, 0.1311, 0.1275, 0.1249, 0.1317]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1336, 0.1135, 0.1226, 0.1256, 0.1310, 0.1211, 0.1294, 0.1233],
        [0.1167, 0.1044, 0.1121, 0.1338, 0.1322, 0.1359, 0.1288, 0.1361],
        [0.1197, 0.1074, 0.1176, 0.1248, 0.1278, 0.1359, 0.1322, 0.1346],
        [0.1144, 0.1049, 0.1109, 0.1328, 0.1388, 0.1311, 0.1315, 0.1356]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1332, 0.1146, 0.1230, 0.1274, 0.1299, 0.1232, 0.1244, 0.1243],
        [0.1206, 0.1066, 0.1148, 0.1265, 0.1309, 0.1323, 0.1305, 0.1379],
        [0.1208, 0.1077, 0.1164, 0.1332, 0.1288, 0.1309, 0.1295, 0.1327],
        [0.1173, 0.1069, 0.1135, 0.1297, 0.1292, 0.1335, 0.1318, 0.1381],
        [0.1186, 0.1093, 0.1146, 0.1344, 0.1250, 0.1284, 0.1349, 0.1348]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1384, 0.1271, 0.1203, 0.1199, 0.1284, 0.1227, 0.1244, 0.1187],
        [0.1277, 0.1190, 0.1232, 0.1240, 0.1289, 0.1217, 0.1242, 0.1313]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1400, 0.1273, 0.1275, 0.1222, 0.1232, 0.1182, 0.1197, 0.1219],
        [0.1275, 0.1168, 0.1278, 0.1249, 0.1248, 0.1203, 0.1294, 0.1285],
        [0.1256, 0.1103, 0.1208, 0.1287, 0.1318, 0.1230, 0.1344, 0.1255]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1338, 0.1232, 0.1256, 0.1210, 0.1289, 0.1251, 0.1212, 0.1211],
        [0.1252, 0.1157, 0.1298, 0.1282, 0.1273, 0.1239, 0.1221, 0.1276],
        [0.1209, 0.1088, 0.1192, 0.1304, 0.1317, 0.1298, 0.1304, 0.1288],
        [0.1167, 0.1065, 0.1166, 0.1339, 0.1316, 0.1273, 0.1344, 0.1330]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1380, 0.1239, 0.1208, 0.1260, 0.1263, 0.1225, 0.1218, 0.1207],
        [0.1294, 0.1172, 0.1255, 0.1266, 0.1241, 0.1223, 0.1284, 0.1265],
        [0.1266, 0.1087, 0.1203, 0.1259, 0.1352, 0.1268, 0.1304, 0.1262],
        [0.1238, 0.1113, 0.1221, 0.1307, 0.1340, 0.1245, 0.1249, 0.1286],
        [0.1214, 0.1105, 0.1197, 0.1265, 0.1356, 0.1247, 0.1371, 0.1245]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/24 03:06:13 AM | Train: [ 5/50] Step 000/390 Loss 2.501 Prec@(1,5) (31.2%, 64.1%)
07/24 03:10:56 AM | Train: [ 5/50] Step 050/390 Loss 2.283 Prec@(1,5) (39.3%, 72.4%)
07/24 03:15:43 AM | Train: [ 5/50] Step 100/390 Loss 2.281 Prec@(1,5) (39.1%, 72.5%)
07/24 03:20:26 AM | Train: [ 5/50] Step 150/390 Loss 2.288 Prec@(1,5) (38.9%, 72.6%)
07/24 03:25:08 AM | Train: [ 5/50] Step 200/390 Loss 2.296 Prec@(1,5) (38.9%, 72.6%)
07/24 03:29:48 AM | Train: [ 5/50] Step 250/390 Loss 2.290 Prec@(1,5) (38.8%, 72.7%)
07/24 03:34:28 AM | Train: [ 5/50] Step 300/390 Loss 2.283 Prec@(1,5) (39.0%, 72.9%)
07/24 03:39:10 AM | Train: [ 5/50] Step 350/390 Loss 2.271 Prec@(1,5) (39.5%, 73.0%)
07/24 03:42:56 AM | Train: [ 5/50] Step 390/390 Loss 2.268 Prec@(1,5) (39.5%, 73.0%)
07/24 03:42:56 AM | Train: [ 5/50] Final Prec@1 39.5240%
07/24 03:42:57 AM | Valid: [ 5/50] Step 000/390 Loss 2.123 Prec@(1,5) (39.1%, 73.4%)
07/24 03:43:14 AM | Valid: [ 5/50] Step 050/390 Loss 2.429 Prec@(1,5) (37.3%, 69.4%)
07/24 03:43:30 AM | Valid: [ 5/50] Step 100/390 Loss 2.402 Prec@(1,5) (37.4%, 69.7%)
07/24 03:43:47 AM | Valid: [ 5/50] Step 150/390 Loss 2.404 Prec@(1,5) (37.7%, 69.7%)
07/24 03:44:03 AM | Valid: [ 5/50] Step 200/390 Loss 2.386 Prec@(1,5) (37.7%, 70.2%)
07/24 03:44:21 AM | Valid: [ 5/50] Step 250/390 Loss 2.383 Prec@(1,5) (38.1%, 70.1%)
07/24 03:44:37 AM | Valid: [ 5/50] Step 300/390 Loss 2.386 Prec@(1,5) (38.1%, 69.9%)
07/24 03:44:58 AM | Valid: [ 5/50] Step 350/390 Loss 2.395 Prec@(1,5) (37.7%, 69.8%)
07/24 03:45:14 AM | Valid: [ 5/50] Step 390/390 Loss 2.388 Prec@(1,5) (37.8%, 70.1%)
07/24 03:45:14 AM | Valid: [ 5/50] Final Prec@1 37.7600%
07/24 03:45:14 AM | genotype = Genotype(normal=[[('sep_conv_5x5', 1), ('sep_conv_5x5', 0)], [('sep_conv_5x5', 1), ('max_pool_3x3', 0)], [('sep_conv_5x5', 3), ('dil_conv_3x3', 2)], [('sep_conv_3x3', 4), ('sep_conv_3x3', 2)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('dil_conv_5x5', 2)], [('dil_conv_5x5', 3), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('sep_conv_5x5', 4)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1317, 0.1112, 0.1251, 0.1225, 0.1352, 0.1292, 0.1237, 0.1214],
        [0.1172, 0.1037, 0.1141, 0.1246, 0.1407, 0.1289, 0.1359, 0.1348]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1376, 0.1144, 0.1277, 0.1244, 0.1232, 0.1258, 0.1230, 0.1238],
        [0.1176, 0.1028, 0.1140, 0.1273, 0.1372, 0.1319, 0.1307, 0.1386],
        [0.1208, 0.1058, 0.1217, 0.1330, 0.1315, 0.1278, 0.1247, 0.1347]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1339, 0.1117, 0.1228, 0.1260, 0.1326, 0.1201, 0.1300, 0.1229],
        [0.1149, 0.1014, 0.1108, 0.1357, 0.1332, 0.1361, 0.1290, 0.1388],
        [0.1180, 0.1040, 0.1168, 0.1249, 0.1278, 0.1369, 0.1339, 0.1377],
        [0.1111, 0.1008, 0.1078, 0.1341, 0.1405, 0.1348, 0.1314, 0.1394]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1334, 0.1127, 0.1227, 0.1291, 0.1310, 0.1231, 0.1239, 0.1242],
        [0.1180, 0.1025, 0.1123, 0.1288, 0.1298, 0.1338, 0.1324, 0.1423],
        [0.1184, 0.1033, 0.1142, 0.1363, 0.1315, 0.1319, 0.1290, 0.1353],
        [0.1144, 0.1026, 0.1103, 0.1306, 0.1308, 0.1351, 0.1321, 0.1441],
        [0.1146, 0.1043, 0.1102, 0.1385, 0.1254, 0.1300, 0.1378, 0.1392]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1416, 0.1283, 0.1203, 0.1180, 0.1282, 0.1225, 0.1233, 0.1179],
        [0.1303, 0.1191, 0.1217, 0.1235, 0.1292, 0.1203, 0.1242, 0.1317]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1426, 0.1278, 0.1279, 0.1218, 0.1223, 0.1172, 0.1193, 0.1212],
        [0.1284, 0.1156, 0.1289, 0.1240, 0.1243, 0.1202, 0.1285, 0.1301],
        [0.1239, 0.1064, 0.1189, 0.1309, 0.1342, 0.1243, 0.1362, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1341, 0.1224, 0.1275, 0.1217, 0.1299, 0.1247, 0.1207, 0.1190],
        [0.1271, 0.1152, 0.1320, 0.1283, 0.1274, 0.1227, 0.1203, 0.1271],
        [0.1185, 0.1055, 0.1194, 0.1309, 0.1330, 0.1310, 0.1308, 0.1308],
        [0.1141, 0.1029, 0.1156, 0.1343, 0.1328, 0.1291, 0.1368, 0.1343]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1401, 0.1242, 0.1203, 0.1268, 0.1254, 0.1212, 0.1221, 0.1198],
        [0.1298, 0.1154, 0.1262, 0.1268, 0.1253, 0.1216, 0.1287, 0.1262],
        [0.1255, 0.1047, 0.1198, 0.1274, 0.1345, 0.1285, 0.1310, 0.1285],
        [0.1224, 0.1074, 0.1210, 0.1338, 0.1348, 0.1243, 0.1268, 0.1296],
        [0.1203, 0.1067, 0.1183, 0.1276, 0.1383, 0.1254, 0.1382, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/24 03:45:22 AM | Train: [ 6/50] Step 000/390 Loss 1.697 Prec@(1,5) (53.1%, 82.8%)
07/24 03:50:06 AM | Train: [ 6/50] Step 050/390 Loss 2.049 Prec@(1,5) (44.0%, 76.9%)
07/24 03:54:48 AM | Train: [ 6/50] Step 100/390 Loss 2.085 Prec@(1,5) (43.3%, 76.8%)
07/24 03:59:29 AM | Train: [ 6/50] Step 150/390 Loss 2.080 Prec@(1,5) (43.2%, 76.9%)
07/24 04:04:09 AM | Train: [ 6/50] Step 200/390 Loss 2.085 Prec@(1,5) (43.2%, 76.9%)
07/24 04:08:51 AM | Train: [ 6/50] Step 250/390 Loss 2.089 Prec@(1,5) (43.1%, 76.6%)
07/24 04:13:34 AM | Train: [ 6/50] Step 300/390 Loss 2.089 Prec@(1,5) (43.2%, 76.5%)
07/24 04:18:22 AM | Train: [ 6/50] Step 350/390 Loss 2.090 Prec@(1,5) (43.1%, 76.4%)
07/24 04:22:07 AM | Train: [ 6/50] Step 390/390 Loss 2.084 Prec@(1,5) (43.3%, 76.5%)
07/24 04:22:08 AM | Train: [ 6/50] Final Prec@1 43.3160%
07/24 04:22:08 AM | Valid: [ 6/50] Step 000/390 Loss 2.114 Prec@(1,5) (39.1%, 79.7%)
07/24 04:22:25 AM | Valid: [ 6/50] Step 050/390 Loss 2.179 Prec@(1,5) (42.0%, 74.1%)
07/24 04:22:44 AM | Valid: [ 6/50] Step 100/390 Loss 2.133 Prec@(1,5) (43.0%, 75.0%)
07/24 04:23:02 AM | Valid: [ 6/50] Step 150/390 Loss 2.145 Prec@(1,5) (42.5%, 74.8%)
07/24 04:23:18 AM | Valid: [ 6/50] Step 200/390 Loss 2.141 Prec@(1,5) (42.7%, 74.9%)
07/24 04:23:34 AM | Valid: [ 6/50] Step 250/390 Loss 2.146 Prec@(1,5) (42.9%, 74.8%)
07/24 04:23:51 AM | Valid: [ 6/50] Step 300/390 Loss 2.154 Prec@(1,5) (42.7%, 74.7%)
07/24 04:24:07 AM | Valid: [ 6/50] Step 350/390 Loss 2.148 Prec@(1,5) (42.9%, 74.8%)
07/24 04:24:20 AM | Valid: [ 6/50] Step 390/390 Loss 2.154 Prec@(1,5) (42.8%, 74.8%)
07/24 04:24:20 AM | Valid: [ 6/50] Final Prec@1 42.8440%
07/24 04:24:20 AM | genotype = Genotype(normal=[[('sep_conv_5x5', 1), ('sep_conv_5x5', 0)], [('sep_conv_5x5', 1), ('max_pool_3x3', 0)], [('sep_conv_5x5', 3), ('sep_conv_3x3', 1)], [('dil_conv_5x5', 4), ('sep_conv_3x3', 2)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 0), ('dil_conv_5x5', 2)], [('dil_conv_5x5', 3), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('dil_conv_5x5', 4)]], reduce_concat=range(2, 6))
####### ALPHA #######
# Alpha - normal
tensor([[0.1312, 0.1092, 0.1262, 0.1234, 0.1359, 0.1302, 0.1228, 0.1211],
        [0.1153, 0.1004, 0.1135, 0.1249, 0.1420, 0.1301, 0.1367, 0.1370]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1384, 0.1125, 0.1286, 0.1240, 0.1231, 0.1263, 0.1221, 0.1250],
        [0.1151, 0.0991, 0.1125, 0.1284, 0.1399, 0.1319, 0.1319, 0.1411],
        [0.1179, 0.1011, 0.1201, 0.1355, 0.1329, 0.1278, 0.1260, 0.1386]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1337, 0.1097, 0.1231, 0.1272, 0.1334, 0.1189, 0.1303, 0.1237],
        [0.1129, 0.0979, 0.1093, 0.1391, 0.1347, 0.1373, 0.1278, 0.1410],
        [0.1149, 0.0996, 0.1152, 0.1273, 0.1272, 0.1382, 0.1343, 0.1433],
        [0.1074, 0.0963, 0.1046, 0.1353, 0.1426, 0.1379, 0.1320, 0.1440]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1329, 0.1102, 0.1223, 0.1307, 0.1320, 0.1229, 0.1237, 0.1252],
        [0.1164, 0.0990, 0.1108, 0.1292, 0.1310, 0.1338, 0.1319, 0.1479],
        [0.1152, 0.0981, 0.1119, 0.1387, 0.1334, 0.1324, 0.1286, 0.1418],
        [0.1111, 0.0978, 0.1067, 0.1299, 0.1322, 0.1368, 0.1341, 0.1513],
        [0.1108, 0.0988, 0.1056, 0.1397, 0.1269, 0.1317, 0.1407, 0.1457]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1433, 0.1273, 0.1214, 0.1153, 0.1289, 0.1231, 0.1234, 0.1173],
        [0.1325, 0.1184, 0.1215, 0.1233, 0.1276, 0.1200, 0.1249, 0.1318]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1450, 0.1275, 0.1289, 0.1217, 0.1214, 0.1161, 0.1182, 0.1213],
        [0.1311, 0.1151, 0.1286, 0.1221, 0.1247, 0.1196, 0.1288, 0.1299],
        [0.1236, 0.1041, 0.1194, 0.1304, 0.1362, 0.1238, 0.1365, 0.1260]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1351, 0.1213, 0.1280, 0.1217, 0.1316, 0.1244, 0.1196, 0.1182],
        [0.1300, 0.1151, 0.1329, 0.1277, 0.1263, 0.1209, 0.1204, 0.1265],
        [0.1171, 0.1031, 0.1199, 0.1319, 0.1326, 0.1324, 0.1313, 0.1317],
        [0.1122, 0.0994, 0.1143, 0.1357, 0.1336, 0.1313, 0.1381, 0.1354]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1429, 0.1246, 0.1193, 0.1271, 0.1248, 0.1197, 0.1208, 0.1207],
        [0.1336, 0.1158, 0.1261, 0.1268, 0.1256, 0.1201, 0.1282, 0.1239],
        [0.1257, 0.1028, 0.1209, 0.1262, 0.1344, 0.1285, 0.1322, 0.1293],
        [0.1215, 0.1045, 0.1212, 0.1357, 0.1356, 0.1249, 0.1263, 0.1303],
        [0.1197, 0.1043, 0.1184, 0.1274, 0.1394, 0.1254, 0.1403, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/24 04:24:27 AM | Train: [ 7/50] Step 000/390 Loss 1.776 Prec@(1,5) (53.1%, 79.7%)
07/24 04:29:20 AM | Train: [ 7/50] Step 050/390 Loss 1.933 Prec@(1,5) (46.5%, 79.7%)
