<<<<<<< HEAD
07/26 07:18:55 AM | 
07/26 07:18:55 AM | Parameters:
07/26 07:18:55 AM | ALPHA_LR=0.0003
07/26 07:18:55 AM | ALPHA_WEIGHT_DECAY=0.001
07/26 07:18:55 AM | BATCH_SIZE=64
07/26 07:18:55 AM | DATA_PATH=./data/
07/26 07:18:55 AM | DATASET=cifar100
07/26 07:18:55 AM | EPOCHS=50
07/26 07:18:55 AM | GPUS=[0]
07/26 07:18:55 AM | INIT_CHANNELS=16
07/26 07:18:55 AM | LAYERS=8
07/26 07:18:55 AM | NAME=cifar100-2e2
07/26 07:18:55 AM | PATH=searchs/cifar100-2e2
07/26 07:18:55 AM | PLOT_PATH=searchs/cifar100-2e2/plots
07/26 07:18:55 AM | PRINT_FREQ=50
07/26 07:18:55 AM | SEED=2
07/26 07:18:55 AM | W_GRAD_CLIP=5.0
07/26 07:18:55 AM | W_LR=0.025
07/26 07:18:55 AM | W_LR_MIN=0.001
07/26 07:18:55 AM | W_MOMENTUM=0.9
07/26 07:18:55 AM | W_WEIGHT_DECAY=0.0003
07/26 07:18:55 AM | WORKERS=4
07/26 07:18:55 AM | 
07/26 07:18:55 AM | Logger is set - training start
=======
07/25 10:16:45 PM | 
07/25 10:16:45 PM | Parameters:
07/25 10:16:45 PM | ALPHA_LR=0.0003
07/25 10:16:45 PM | ALPHA_WEIGHT_DECAY=0.001
07/25 10:16:45 PM | BATCH_SIZE=64
07/25 10:16:45 PM | DATA_PATH=./data/
07/25 10:16:45 PM | DATASET=cifar100
07/25 10:16:45 PM | EPOCHS=50
07/25 10:16:45 PM | GPUS=[0]
07/25 10:16:45 PM | INIT_CHANNELS=16
07/25 10:16:45 PM | LAYERS=8
07/25 10:16:45 PM | NAME=cifar100-2e2
07/25 10:16:45 PM | PATH=searchs/cifar100-2e2
07/25 10:16:45 PM | PLOT_PATH=searchs/cifar100-2e2/plots
07/25 10:16:45 PM | PRINT_FREQ=50
07/25 10:16:45 PM | SEED=2
07/25 10:16:45 PM | W_GRAD_CLIP=5.0
07/25 10:16:45 PM | W_LR=0.025
07/25 10:16:45 PM | W_LR_MIN=0.001
07/25 10:16:45 PM | W_MOMENTUM=0.9
07/25 10:16:45 PM | W_WEIGHT_DECAY=0.0003
07/25 10:16:45 PM | WORKERS=4
07/25 10:16:45 PM | 
07/25 10:16:45 PM | Logger is set - training start
>>>>>>> c01054b557a41af8296d9b6ff7394bb5e3478530
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
<<<<<<< HEAD
07/26 07:19:11 AM | Train: [ 1/50] Step 000/390 Loss 4.726 Prec@(1,5) (1.6%, 9.4%)
=======
07/26 02:08:57 AM | 
07/26 02:08:57 AM | Parameters:
07/26 02:08:57 AM | ALPHA_LR=0.0003
07/26 02:08:57 AM | ALPHA_WEIGHT_DECAY=0.001
07/26 02:08:57 AM | BATCH_SIZE=64
07/26 02:08:57 AM | DATA_PATH=./data/
07/26 02:08:57 AM | DATASET=cifar100
07/26 02:08:57 AM | EPOCHS=50
07/26 02:08:57 AM | GPUS=[0]
07/26 02:08:57 AM | INIT_CHANNELS=16
07/26 02:08:57 AM | LAYERS=8
07/26 02:08:57 AM | NAME=cifar100-2e2
07/26 02:08:57 AM | PATH=searchs/cifar100-2e2
07/26 02:08:57 AM | PLOT_PATH=searchs/cifar100-2e2/plots
07/26 02:08:57 AM | PRINT_FREQ=50
07/26 02:08:57 AM | SEED=2
07/26 02:08:57 AM | W_GRAD_CLIP=5.0
07/26 02:08:57 AM | W_LR=0.025
07/26 02:08:57 AM | W_LR_MIN=0.001
07/26 02:08:57 AM | W_MOMENTUM=0.9
07/26 02:08:57 AM | W_WEIGHT_DECAY=0.0003
07/26 02:08:57 AM | WORKERS=4
07/26 02:08:57 AM | 
07/26 02:08:57 AM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/26 02:09:25 AM | Train: [ 1/50] Step 000/390 Loss 4.706 Prec@(1,5) (3.1%, 10.9%)
07/26 02:12:23 AM | 
07/26 02:12:23 AM | Parameters:
07/26 02:12:23 AM | ALPHA_LR=0.0003
07/26 02:12:23 AM | ALPHA_WEIGHT_DECAY=0.001
07/26 02:12:23 AM | BATCH_SIZE=64
07/26 02:12:23 AM | DATA_PATH=./data/
07/26 02:12:23 AM | DATASET=cifar100
07/26 02:12:23 AM | EPOCHS=50
07/26 02:12:23 AM | GPUS=[0]
07/26 02:12:23 AM | INIT_CHANNELS=16
07/26 02:12:23 AM | LAYERS=8
07/26 02:12:23 AM | NAME=cifar100-2e2
07/26 02:12:23 AM | PATH=searchs/cifar100-2e2
07/26 02:12:23 AM | PLOT_PATH=searchs/cifar100-2e2/plots
07/26 02:12:23 AM | PRINT_FREQ=50
07/26 02:12:23 AM | SEED=2
07/26 02:12:23 AM | W_GRAD_CLIP=5.0
07/26 02:12:23 AM | W_LR=0.025
07/26 02:12:23 AM | W_LR_MIN=0.001
07/26 02:12:23 AM | W_MOMENTUM=0.9
07/26 02:12:23 AM | W_WEIGHT_DECAY=0.0003
07/26 02:12:23 AM | WORKERS=4
07/26 02:12:23 AM | 
07/26 02:12:23 AM | Logger is set - training start
07/26 02:12:49 AM | 
07/26 02:12:49 AM | Parameters:
07/26 02:12:49 AM | ALPHA_LR=0.0003
07/26 02:12:49 AM | ALPHA_WEIGHT_DECAY=0.001
07/26 02:12:49 AM | BATCH_SIZE=64
07/26 02:12:49 AM | DATA_PATH=./data/
07/26 02:12:49 AM | DATASET=cifar100
07/26 02:12:49 AM | EPOCHS=50
07/26 02:12:49 AM | GPUS=[0]
07/26 02:12:49 AM | INIT_CHANNELS=16
07/26 02:12:49 AM | LAYERS=8
07/26 02:12:49 AM | NAME=cifar100-2e2
07/26 02:12:49 AM | PATH=searchs/cifar100-2e2
07/26 02:12:49 AM | PLOT_PATH=searchs/cifar100-2e2/plots
07/26 02:12:49 AM | PRINT_FREQ=50
07/26 02:12:49 AM | SEED=2
07/26 02:12:49 AM | W_GRAD_CLIP=5.0
07/26 02:12:49 AM | W_LR=0.025
07/26 02:12:49 AM | W_LR_MIN=0.001
07/26 02:12:49 AM | W_MOMENTUM=0.9
07/26 02:12:49 AM | W_WEIGHT_DECAY=0.0003
07/26 02:12:49 AM | WORKERS=4
07/26 02:12:49 AM | 
07/26 02:12:49 AM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/26 02:13:08 AM | Train: [ 1/50] Step 000/390 Loss 4.706 Prec@(1,5) (3.1%, 10.9%)
07/26 02:27:03 AM | 
07/26 02:27:04 AM | Parameters:
07/26 02:27:04 AM | ALPHA_LR=0.0003
07/26 02:27:04 AM | ALPHA_WEIGHT_DECAY=0.001
07/26 02:27:04 AM | BATCH_SIZE=64
07/26 02:27:04 AM | DATA_PATH=./data/
07/26 02:27:04 AM | DATASET=cifar100
07/26 02:27:04 AM | EPOCHS=50
07/26 02:27:04 AM | GPUS=[0]
07/26 02:27:04 AM | INIT_CHANNELS=16
07/26 02:27:04 AM | LAYERS=8
07/26 02:27:04 AM | NAME=cifar100-2e2
07/26 02:27:04 AM | PATH=searchs/cifar100-2e2
07/26 02:27:04 AM | PLOT_PATH=searchs/cifar100-2e2/plots
07/26 02:27:04 AM | PRINT_FREQ=50
07/26 02:27:04 AM | SEED=2
07/26 02:27:04 AM | W_GRAD_CLIP=5.0
07/26 02:27:04 AM | W_LR=0.025
07/26 02:27:04 AM | W_LR_MIN=0.001
07/26 02:27:04 AM | W_MOMENTUM=0.9
07/26 02:27:04 AM | W_WEIGHT_DECAY=0.0003
07/26 02:27:04 AM | WORKERS=4
07/26 02:27:04 AM | 
07/26 02:27:04 AM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/26 02:27:21 AM | Train: [ 1/50] Step 000/390 Loss 4.706 Prec@(1,5) (3.1%, 10.9%)
07/26 02:31:32 AM | Train: [ 1/50] Step 050/390 Loss 4.450 Prec@(1,5) (3.9%, 15.5%)
07/26 02:35:02 AM | 
07/26 02:35:03 AM | Parameters:
07/26 02:35:04 AM | ALPHA_LR=0.0003
07/26 02:35:04 AM | ALPHA_WEIGHT_DECAY=0.001
07/26 02:35:05 AM | BATCH_SIZE=64
07/26 02:35:06 AM | DATA_PATH=./data/
07/26 02:35:07 AM | DATASET=cifar100
07/26 02:35:07 AM | EPOCHS=50
07/26 02:35:08 AM | GPUS=[0]
07/26 02:35:09 AM | INIT_CHANNELS=16
07/26 02:35:09 AM | LAYERS=8
07/26 02:35:10 AM | NAME=cifar100-2e2
07/26 02:35:11 AM | PATH=searchs/cifar100-2e2
07/26 02:35:11 AM | PLOT_PATH=searchs/cifar100-2e2/plots
07/26 02:35:12 AM | PRINT_FREQ=50
07/26 02:35:12 AM | SEED=2
07/26 02:35:13 AM | W_GRAD_CLIP=5.0
07/26 02:35:14 AM | W_LR=0.025
07/26 02:35:15 AM | W_LR_MIN=0.001
07/26 02:35:16 AM | W_MOMENTUM=0.9
07/26 02:35:17 AM | W_WEIGHT_DECAY=0.0003
07/26 02:35:18 AM | WORKERS=4
07/26 02:35:19 AM | 
07/26 02:35:19 AM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/26 02:35:56 AM | Train: [ 1/50] Step 000/390 Loss 4.706 Prec@(1,5) (3.1%, 10.9%)
07/26 02:38:36 AM | Train: [ 1/50] Step 050/390 Loss 4.442 Prec@(1,5) (4.1%, 16.2%)
07/26 02:41:19 AM | Train: [ 1/50] Step 100/390 Loss 4.254 Prec@(1,5) (5.8%, 20.7%)
07/26 02:44:04 AM | Train: [ 1/50] Step 150/390 Loss 4.162 Prec@(1,5) (6.7%, 23.4%)
07/26 02:46:48 AM | Train: [ 1/50] Step 200/390 Loss 4.077 Prec@(1,5) (7.6%, 25.7%)
07/26 02:49:32 AM | Train: [ 1/50] Step 250/390 Loss 4.002 Prec@(1,5) (8.6%, 27.9%)
07/26 02:52:14 AM | Train: [ 1/50] Step 300/390 Loss 3.941 Prec@(1,5) (9.4%, 29.7%)
07/26 02:54:53 AM | Train: [ 1/50] Step 350/390 Loss 3.886 Prec@(1,5) (10.3%, 31.3%)
07/26 02:57:00 AM | Train: [ 1/50] Step 390/390 Loss 3.841 Prec@(1,5) (10.9%, 32.6%)
07/26 02:57:01 AM | Train: [ 1/50] Final Prec@1 10.8840%
07/26 02:57:01 AM | Valid: [ 1/50] Step 000/390 Loss 3.379 Prec@(1,5) (21.9%, 43.8%)
07/26 02:57:09 AM | Valid: [ 1/50] Step 050/390 Loss 3.382 Prec@(1,5) (17.0%, 46.4%)
07/26 02:57:16 AM | Valid: [ 1/50] Step 100/390 Loss 3.381 Prec@(1,5) (17.8%, 46.0%)
07/26 02:57:24 AM | Valid: [ 1/50] Step 150/390 Loss 3.367 Prec@(1,5) (18.2%, 46.5%)
07/26 02:57:32 AM | Valid: [ 1/50] Step 200/390 Loss 3.362 Prec@(1,5) (18.3%, 46.6%)
07/26 02:57:40 AM | Valid: [ 1/50] Step 250/390 Loss 3.372 Prec@(1,5) (18.0%, 46.3%)
07/26 02:57:48 AM | Valid: [ 1/50] Step 300/390 Loss 3.373 Prec@(1,5) (18.2%, 46.4%)
07/26 02:57:55 AM | Valid: [ 1/50] Step 350/390 Loss 3.376 Prec@(1,5) (18.0%, 46.2%)
07/26 02:58:02 AM | Valid: [ 1/50] Step 390/390 Loss 3.377 Prec@(1,5) (18.1%, 46.1%)
07/26 02:58:02 AM | Valid: [ 1/50] Final Prec@1 18.0680%
07/26 02:58:02 AM | genotype = Genotype(normal=[[('max_pool_3x3', 0), ('sep_conv_5x5', 1)], [('max_pool_3x3', 0), ('sep_conv_3x3', 1)], [('dil_conv_3x3', 1), ('max_pool_3x3', 0)], [('dil_conv_5x5', 4), ('max_pool_3x3', 0)]], normal_concat=range(2, 6), reduce=[[('sep_conv_5x5', 0), ('sep_conv_5x5', 1)], [('sep_conv_3x3', 1), ('dil_conv_3x3', 2)], [('sep_conv_5x5', 2), ('max_pool_3x3', 0)], [('dil_conv_5x5', 3), ('dil_conv_5x5', 2)]], reduce_concat=range(2, 6))
07/26 02:58:47 AM | 
07/26 02:58:47 AM | Parameters:
07/26 02:58:47 AM | ALPHA_LR=0.0003
07/26 02:58:47 AM | ALPHA_WEIGHT_DECAY=0.001
07/26 02:58:47 AM | BATCH_SIZE=64
07/26 02:58:47 AM | DATA_PATH=./data/
07/26 02:58:47 AM | DATASET=cifar100
07/26 02:58:47 AM | EPOCHS=50
07/26 02:58:47 AM | GPUS=[0]
07/26 02:58:47 AM | INIT_CHANNELS=16
07/26 02:58:47 AM | LAYERS=8
07/26 02:58:47 AM | NAME=cifar100-2e2
07/26 02:58:47 AM | PATH=searchs/cifar100-2e2
07/26 02:58:47 AM | PLOT_PATH=searchs/cifar100-2e2/plots
07/26 02:58:47 AM | PRINT_FREQ=50
07/26 02:58:47 AM | SEED=2
07/26 02:58:47 AM | W_GRAD_CLIP=5.0
07/26 02:58:47 AM | W_LR=0.025
07/26 02:58:47 AM | W_LR_MIN=0.001
07/26 02:58:47 AM | W_MOMENTUM=0.9
07/26 02:58:47 AM | W_WEIGHT_DECAY=0.0003
07/26 02:58:47 AM | WORKERS=4
07/26 02:58:47 AM | 
07/26 02:58:47 AM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/26 02:59:10 AM | Train: [ 1/50] Step 000/390 Loss 4.706 Prec@(1,5) (3.1%, 10.9%)
07/26 03:03:50 AM | Train: [ 1/50] Step 050/390 Loss 4.454 Prec@(1,5) (3.6%, 14.7%)
07/26 03:07:56 AM | Train: [ 1/50] Step 100/390 Loss 4.273 Prec@(1,5) (5.3%, 19.4%)
07/26 03:12:23 AM | Train: [ 1/50] Step 150/390 Loss 4.173 Prec@(1,5) (6.5%, 22.6%)
07/26 03:16:29 AM | Train: [ 1/50] Step 200/390 Loss 4.090 Prec@(1,5) (7.3%, 25.2%)
07/26 03:20:51 AM | Train: [ 1/50] Step 250/390 Loss 4.022 Prec@(1,5) (8.3%, 27.1%)
07/26 03:24:58 AM | Train: [ 1/50] Step 300/390 Loss 3.964 Prec@(1,5) (9.0%, 28.9%)
07/26 03:29:20 AM | Train: [ 1/50] Step 350/390 Loss 3.909 Prec@(1,5) (9.7%, 30.6%)
07/26 03:32:44 AM | Train: [ 1/50] Step 390/390 Loss 3.863 Prec@(1,5) (10.3%, 31.9%)
07/26 03:32:45 AM | Train: [ 1/50] Final Prec@1 10.2600%
07/26 03:32:45 AM | Valid: [ 1/50] Step 000/390 Loss 3.260 Prec@(1,5) (17.2%, 43.8%)
07/26 03:33:00 AM | Valid: [ 1/50] Step 050/390 Loss 3.415 Prec@(1,5) (16.2%, 44.9%)
07/26 03:33:14 AM | Valid: [ 1/50] Step 100/390 Loss 3.407 Prec@(1,5) (16.9%, 45.4%)
07/26 03:33:27 AM | Valid: [ 1/50] Step 150/390 Loss 3.392 Prec@(1,5) (17.2%, 45.8%)
07/26 03:33:42 AM | Valid: [ 1/50] Step 200/390 Loss 3.388 Prec@(1,5) (17.3%, 45.9%)
07/26 03:33:57 AM | Valid: [ 1/50] Step 250/390 Loss 3.399 Prec@(1,5) (17.2%, 45.7%)
07/26 03:34:13 AM | Valid: [ 1/50] Step 300/390 Loss 3.400 Prec@(1,5) (17.2%, 45.7%)
07/26 03:34:27 AM | Valid: [ 1/50] Step 350/390 Loss 3.403 Prec@(1,5) (17.0%, 45.5%)
07/26 03:34:39 AM | Valid: [ 1/50] Step 390/390 Loss 3.404 Prec@(1,5) (17.1%, 45.4%)
07/26 03:34:40 AM | Valid: [ 1/50] Final Prec@1 17.0840%
07/26 03:34:40 AM | genotype = Genotype(normal=[[('max_pool_3x3', 0), ('sep_conv_5x5', 1)], [('max_pool_3x3', 0), ('sep_conv_3x3', 1)], [('max_pool_3x3', 0), ('sep_conv_5x5', 2)], [('dil_conv_5x5', 1), ('max_pool_3x3', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('sep_conv_5x5', 1)], [('dil_conv_3x3', 2), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 0), ('sep_conv_3x3', 2)], [('dil_conv_5x5', 3), ('dil_conv_3x3', 4)]], reduce_concat=range(2, 6))
07/26 03:35:40 AM | 
07/26 03:35:40 AM | Parameters:
07/26 03:35:40 AM | ALPHA_LR=0.0003
07/26 03:35:40 AM | ALPHA_WEIGHT_DECAY=0.001
07/26 03:35:40 AM | BATCH_SIZE=64
07/26 03:35:40 AM | DATA_PATH=./data/
07/26 03:35:40 AM | DATASET=cifar100
07/26 03:35:40 AM | EPOCHS=50
07/26 03:35:40 AM | GPUS=[0]
07/26 03:35:40 AM | INIT_CHANNELS=16
07/26 03:35:40 AM | LAYERS=8
07/26 03:35:40 AM | NAME=cifar100-2e2
07/26 03:35:40 AM | PATH=searchs/cifar100-2e2
07/26 03:35:40 AM | PLOT_PATH=searchs/cifar100-2e2/plots
07/26 03:35:40 AM | PRINT_FREQ=50
07/26 03:35:40 AM | SEED=2
07/26 03:35:40 AM | W_GRAD_CLIP=5.0
07/26 03:35:40 AM | W_LR=0.025
07/26 03:35:40 AM | W_LR_MIN=0.001
07/26 03:35:40 AM | W_MOMENTUM=0.9
07/26 03:35:40 AM | W_WEIGHT_DECAY=0.0003
07/26 03:35:40 AM | WORKERS=4
07/26 03:35:40 AM | 
07/26 03:35:40 AM | Logger is set - training start
####### ALPHA #######
# Alpha - normal
tensor([[0.1249, 0.1252, 0.1249, 0.1249, 0.1249, 0.1252, 0.1249, 0.1250],
        [0.1249, 0.1251, 0.1250, 0.1250, 0.1252, 0.1249, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1249, 0.1253, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1249],
        [0.1251, 0.1247, 0.1249, 0.1253, 0.1248, 0.1249, 0.1251, 0.1253],
        [0.1250, 0.1250, 0.1249, 0.1251, 0.1252, 0.1250, 0.1251, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249, 0.1251],
        [0.1250, 0.1249, 0.1251, 0.1249, 0.1249, 0.1253, 0.1252, 0.1248],
        [0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1250, 0.1251, 0.1250],
        [0.1250, 0.1251, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1248]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249, 0.1252],
        [0.1250, 0.1252, 0.1252, 0.1248, 0.1250, 0.1249, 0.1248, 0.1251],
        [0.1251, 0.1251, 0.1250, 0.1251, 0.1248, 0.1250, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1250, 0.1253, 0.1251, 0.1251, 0.1247, 0.1249],
        [0.1251, 0.1251, 0.1252, 0.1249, 0.1249, 0.1251, 0.1250, 0.1249]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)

# Alpha - reduce
tensor([[0.1248, 0.1249, 0.1249, 0.1251, 0.1250, 0.1250, 0.1251, 0.1251],
        [0.1250, 0.1248, 0.1249, 0.1252, 0.1249, 0.1250, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1249, 0.1252, 0.1249, 0.1250, 0.1250, 0.1249, 0.1250],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1249, 0.1250, 0.1251],
        [0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1252]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1251, 0.1247, 0.1249, 0.1252, 0.1252, 0.1249, 0.1251, 0.1249],
        [0.1250, 0.1250, 0.1250, 0.1251, 0.1253, 0.1249, 0.1249, 0.1248],
        [0.1250, 0.1252, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249],
        [0.1250, 0.1251, 0.1251, 0.1249, 0.1248, 0.1251, 0.1250, 0.1250]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[0.1250, 0.1249, 0.1251, 0.1251, 0.1251, 0.1249, 0.1250, 0.1250],
        [0.1252, 0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249],
        [0.1249, 0.1250, 0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1252],
        [0.1250, 0.1249, 0.1250, 0.1251, 0.1250, 0.1251, 0.1250, 0.1249],
        [0.1249, 0.1251, 0.1250, 0.1251, 0.1248, 0.1251, 0.1249, 0.1251]],
       device='cuda:0', grad_fn=<SoftmaxBackward>)
#####################
07/26 03:35:59 AM | Train: [ 1/50] Step 000/390 Loss 4.706 Prec@(1,5) (3.1%, 10.9%)
07/26 03:39:56 AM | Train: [ 1/50] Step 050/390 Loss 4.449 Prec@(1,5) (3.8%, 15.4%)
07/26 03:43:36 AM | Train: [ 1/50] Step 100/390 Loss 4.259 Prec@(1,5) (5.6%, 20.5%)
07/26 03:47:19 AM | Train: [ 1/50] Step 150/390 Loss 4.169 Prec@(1,5) (6.5%, 23.2%)
>>>>>>> c01054b557a41af8296d9b6ff7394bb5e3478530
